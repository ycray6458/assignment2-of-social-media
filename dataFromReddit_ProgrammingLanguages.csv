Title,ID,Time,Body,TopComment
"Concept Art: what might python look like in Japanese, without any English characters?",g9iu8x,2020-04-28 15:40:07,,I'm curious: Do Japanese use hiragana characters as variables when they're doing math?
DreamBerd is a perfect programming language,13ztlms,2023-06-04 07:29:40,https://github.com/TodePond/DreamBerd,"> You can make classes, but you can only ever make one instance of them. This shouldn't affect how most object-oriented programmers work.

This is a work of art. I died a little inside. 10/10."
ChatGPT and related posts are now banned,1252ekz,2023-03-29 05:08:52,"[Recently we asked for feedback](https://www.reddit.com/r/ProgrammingLanguages/comments/11ta0eh/whats_your_opinion_on_chatgpt_related_posts/) about banning ChatGPT related posts. The feedback we got was that while on rare occasion such content may be relevant, most of the time it's undesired.

Based on this feedback we've set up AutoModerator to remove posts that cover ChatGPT and related content. This currently works using a simple keyword list, but we may adjust this in the future. We'll keep an eye on AutoModerator in the coming days to make sure it doesn't accidentally remove valid posts, but given that 99% of posts mentioning these keywords are garbage, we don't anticipate any problems. 

We may make exceptions if posts are from active members of the community _and_ the content is directly relevant to programming language design, but this will require the author to notify us through modmail so we're actually aware of the post.",Appreciated
I've developed a new programming language,gnmq6k,2020-05-21 08:36:26,,"Mathematicians have been using the same letter in different fonts to mean different things for years, so there's certainly precedent for this."
Introducing the Cat esoteric programming language,xqwbo0,2022-09-29 10:53:02,"It's often very hard for programmers to get started with a new language. How often have we seen verbose boiler plate just like this?

        public class HelloWorld {
            public static void main(String[] args) {
                System.out.println(""Hello World"");
            }
        }

That's just too much for a new programmer to grasp. Wouldn't you rather have the programming language handle all of the boilerplate for you? Now there is an elegant and simple solution.

Introducing the Cat programming language. Cat source files use the `.kitty` extension. Here is the source code for the `Hello.kitty` example:

        Hello World!

Doesn't that look much better? Simple, and super easy to understand!

To run the above program, use the Cat compiler and interpreter from the Linux or UNIX shell:

        cat Hello.kitty

Version 1 is already included in most major Linux and UNIX distributions. Copyright 2022 by Larry Ellison. All rights reserved.",I expected a category theory oriented programming language to be honnest.
I analyzed Wikipedia documents of Programming Languages to visualize a paradigm relationship between them: and this is the result,fvx72d,2020-04-06 19:35:44,,"I suggest using physics-based clustering: every pair of vertices repel with a force proportional to the inverse square root of the distance; every pair of *similar* vertices attract with a force proportional to the similarity. Now place the vertices randomly and use a simple physics simulation to calculate velocities and positions and once the system stabilizes, you get nice clusters."
Been thinking about writing a custom layer over HTML (left compiles into right). What are your thoughts on this syntax?,ioon55,2020-09-08 14:29:53,,Great idea. Would be even greater if you could use `div.menu` for `div [class=menu]` and `div#id` for `div [id=id]` and you didn't have to write empty parentheses. Maybe `.a` could be a shorthand for `div [class=a]`.
The keyword used to declare functions in various programming languages (Source: https://twitter.com/code_report/status/1325472952750665728),ngf382,2021-05-20 03:44:09,,"""auto"" for C++ ?  
Am I missing something ?  
I would say that the presence of C++ here is irrelevant, since there is no dedicated keyword to declare functions."
The Rust I Wanted Had No Future - Graydon Hoare,141qm6g,2023-06-06 04:24:18,,"This is one of the cooler, more surprising, write ups from a popular programming language designer."
The Clickbait Headline Programming Language,12c57rm,2023-04-05 10:20:09,,Please like and subscribe.
The Periodic Table of Programming Languages,hterx6,2020-07-18 18:57:20,,"It's a cool chart but you might want to fix some spelling errors:
 * COBAL -> COBOL
 * SNOWBOL -> SNOBOL
 * Haskel -> Haskell"
"I wrote my own programming language for my interactive fiction game's story. On launch, it compiles and parses the code into Chapter, Scene, and Choice classes. Syntax highlighting courtesy of Notepad++",oxnywq,2021-08-04 16:08:46,,"Cool project and a useful application of a DSL, but only a little can be gleaned from a screenshot. Got a link to the repo?"
this somehow fits this sub,okppox,2021-07-15 17:58:44,,"If you write the Next Big Thing in a dying language then, well, I don't use the word hero lightly."
"If you want a .lang domain ending for your website, it's time to let Registrars know.",wgxfm4,2022-08-05 22:50:21,"# The idea

Currently, there is a pattern of appending `[-]lang` to websites related to \`languages\`. A few examples are `rust-lang.org` or `ponylang.io` and it is probably simply because we lack a `.lang` domain ending.

I posted on [*r/ICANN*](https://www.reddit.com/r/ICANN/comments/w66pxo/make_a_gtlds_for_languages/) about it.

I honestly didn't know how these things worked. It happens to be really slow and costly (hundred thousands of dollars) to register a new generic top-level domain (gTLD). I don't want to start a new business that I can't afford in order to simply have a .lang website.

Today I learned that my hope shouldn't be completely vanished, as I can actually let registrars know about my interest in new domain endings. I, myself alone, would not achieve anything following this path, though.

This is a call for the community, **the community** of users interested in having a `.lang` website, to come together and let registrars know about our interest in this domain ending.

If there is a strong enough movement, then, *hopefully*, it may happen and we *may* have a `.lang` ending for the next round.

## Who benefits from this

Us! If you want a website for your constructed language, for your programming language, for your language school, etc. then you benefit from having this gTLD available.

## TLDR

Would you like to have a website called `website.lang` instead of `website-lang.org`, `website.org`, or similar? Then you can join this little ""*movement*"" and let some Registrars know about it! You can use the how-to guides below.

# How-to:

* Google Domains: Follow [this link](https://domains.google/tld/?utm_medium=support&utm_source=helpcenter&utm_campaign=tld_req&utm_content=#tld). Fill the input boxes with *your* data and set `Desired domain ending (TLD)*` to `.lang`. Accept Google's Terms and Conditions and submit.

# Current websites/organizations that may benefit from this

- [awklang.org](http://awklang.org/)
- [ciao-lang.org](https://ciao-lang.org/)
- [crystal-lang.org](https://crystal-lang.org/)
- [dlang.org](https://dlang.org/)
- [elm-lang.org](https://elm-lang.org/)
- [erlang.org](https://www.erlang.org/)
- [forthlang.org](https://www.forthlang.org/)
- [fortran-lang.org](https://fortran-lang.org/)
- [genielang.com](https://www.genielang.com/)
- [golang.com](https://go.dev/)¹
- [gren-lang.org](https://gren-lang.org/)
- [groovy-lang.org](http://www.groovy-lang.org/)
- [hacklang.org](https://hacklang.org/)
- [iolanguage.org](https://iolanguage.org/)
- [julialang.org](https://julialang.org/)
- [kotlinlang.org](https://kotlinlang.org/)
- [lisp-lang.org](https://lisp-lang.org/)
- [nim-lang.org](https://nim-lang.org/)
- [ponylang.io](https://www.ponylang.io/)
- [racket-lag.org](https://racket-lang.org/)
- [red-lang.org](https://www.red-lang.org/)
- [roc-lang.org](https://www.roc-lang.org/)
- [ruby-lang.org](https://www.ruby-lang.org/)
- [rust-lang.org](https://www.rust-lang.org/)
- [sas-lang.com](https://sass-lang.com/)
- [scala-lang.org](https://scala-lang.org/)
- [typescriptlang.org](https://www.typescriptlang.org/)
- [vlang.io](https://vlang.io/)
- [ziglang.org](https://ziglang.org/)
- and many more!

¹ Currently go.dev, but golang.com is still active.   

# Final words

* If you participated in this little movement, then thank you very much!
* I will cross-post this post on those subreddits that I think it may be of interest based on [Reddit Cross-posting best practices](https://reddit.zendesk.com/hc/en-us/articles/4835584113684-What-is-Crossposting-), trying to maximally respect the subreddit's rules and users.
* If you know about other Registrars that are willing to listen for community petitions, then, don't hesitate and let me know. I will update this post as soon as I possibly can.


I hope that you have a great day!","Interesting idea. Perhaps some conlangers would be interested in this, too."
Beyond Opinionated: Announcing The First Actually Bigoted Language,u2lrbo,2022-04-13 16:41:36,"I have decided to suspend work on my previous project `Charm` because I now realize that implementing a merely *opinionated* scripting language is not enough. I am now turning my attention to a project tentatively called `Malevolence` which will have essentially the same syntax and semantics but a completely different set of psychiatric problems.

Its error messages will be designed not only to reprove but to humiliate the user. This will of course be done on a sliding scale, someone who introduced say one syntax error in a hundred lines will merely be chided, whereas repeat offenders will be questioned as to their sanity, human ancestry, and the chastity of their parents.

But it is of course style and not the mere functioning or non-functioning of the code that is most important. For this reason, while the `Malevolence` parser inspects your code for clarity and structure, an advanced AI routine will search your computer for your email details and the names of your near kin and loved ones. Realistic death-threats will be issued unless a sufficiently high quality is met. You may be terrified, but your code will be *beautifully* formatted.

If you have any suggestions on how my users might be further cowed into submission, my gratitude will not actually extend to acknowledgement but I'll still steal your ideas. What can I say? I've given up on trying to be nice.","Suggestion: when Malevolence detects an error, it shouldn't print a message mentioning the code in question.  Instead it should print a message listing some other piece of code and ask ""why couldn't all your code be like this?"""
The WORST features of every language you can think of.,jn3n4f,2020-11-03 12:13:01,"I’m making a programming language featuring my favorite features but I thought to myself “what is everyone’s least favorite parts about different languages?”. So here I am to ask. Least favorite paradigm? Syntax styles (for many things: loops, function definitions, variable declaration, etc.)? If there’s a feature of a language that you really don’t like, let me know and I’ll add it in. I’l write an interpreter for it if anyone else is interested in this idea.

Edit 1: So far we are going to include unnecessary header files and enforce unnecessary namespaces. Personally I will also add unnecessarily verbose type names, such as having to spell out integer, and I might make it all caps just to make it more painful.

Edit 2: I have decided white space will have significance in the language, but it will make the syntax look horrible. All variables will be case-insensitive and global.

Edit 3: I have chosen a name for this language. PAIN.

Edit 4: I don’t believe I will use UTF-16 for source files (sorry), but I might use ascii drawing characters as operators. What do you all think?

Edit 5: I’m going to make some variables “artificially private”. This means that they can only be directly accessed inside of their scope, but do remember that all variables are global, so you can’t give another variable that variable’s name.

Edit 6: Debug messages will be put on the same line and I’ll just let text wrap take care of going to then next line for me.

Edit 7: A [GitHub](www.github.com/Co0perator/PAIN) is now open. Contribute if you dare to.

Edit 8: The link doesn’t seem to be working (for me at least Idk about you all) so I’m putting it here in plain text.

www.github.com/Co0perator/PAIN

Edit 9: I have decided that PAIN is an acronym for what this monster I have created is

Pure AIDS In a Nutshell","MatLab's `varargout`/`nargout` takes the cake for me.

Functions returning multiple values? Great! 

Functions returning a dynamic number of values which may change between invocations? Uh...

Functions that check at runtime how many variables you are binding to their invocation and do (sometimes very) different things based on that number? Oh. Oh no."
I made an ancient Hebrew programming language to help programmers speak to God,whilk6,2022-08-06 15:40:09,"Hello everyone! I am a secondary school student who has recently observed that many languages sometimes profess themselves as ""God's programming language"" (e.g. Lisp, C, and other inane functional ones). This appalls me in ways that are ineffable; even the irreligious among us know what the good book says about worshiping false gods. Technically, I suppose, this is not worshiping false gods, but rather people indulging in false prophets. Nevertheless, it is still immensely painful for me to see people mired in the defilements of the programming world in this manner.

To ameliorate the wickedness I see in the constructs used by most programmers, I created [Genesis](https://github.com/elonlit/Genesis), an interpreted Turing-complete Paleo-Hebrew programming language based on a procedural paradigm. There are no objects because object worship is explicitly forbidden in the Bible. There are also no Hindu-Arabic numerals (sinful). Instead, you define all variables using Gematria (Jewish isopsephy). I should warn you: the interpreter is extraordinarily enigmatic (and probably buggy), but that is simply the price you have to pay for salvation.

Please let me know your thoughts about this endeavor. If you would like to give advice or make a pull request to make this language even holier, I am eager for it. To recap the discussion I posted on the GitHub repository, some future features might include:

* An integrated calculus/bioinformatics library.
* A command-line tool.
* Removing all numbers.
* No garbage collection.
   * ""Nothing is covered up that will not be revealed, or hidden that will not be known. Therefore whatever you have said in the dark shall be heard in the light, and what you have whispered in private rooms shall be proclaimed on the housetops."" (Luke 12:3)
* No I/O.
* Introducing more ambiguity.","Finally, competition to Holy C"
10 Most(ly dead) Influential Programming Languages • Hillel Wayne,fp7sko,2020-03-26 16:13:13,,"""Smalltalk wasn’t the only casualty of the “Javapocalypse”: Java also marginalized Eiffel, Ada95, and pretty much everything else in the OOP world. The interesting question isn’t “Why did Smalltalk die”, it’s “Why did C++ survive”. I think it’s because C++ had better C interop so was easier to extend into legacy systems.""

This is something I strongly disagree with. Java may have ""purged"" many of these languages because of their comparable use cases: ""ease of use"", no memory management, ""cross platform"".

C++ ""survived"" because it was a different use case. It wasn't supposed to be these things. It promised OOP with fine grained memory control, no compromise on speed. C++ was made with the intent to build low-level systems, Java with the intent to build user-level programs"
"""Folders"" is a programming language where programs are encoded as hierarchies of folders in the Windows operating system.",gr1sk0,2020-05-27 01:46:34,,"> Source code for the Folders compiler can be found on github.

I was ready for this to be a self-compiling compiler, a github repo with just a bunch of folders in it."
I built a 2D grid-based esoteric language with a Visualizer! Here is a Hello World!,f3ekao,2020-02-14 02:40:50,,"Looks a bit like [befunge](https://en.m.wikipedia.org/wiki/Befunge).

I should see how the two stack up..."
Made a simple assembler for my CPU emulator,pf715g,2021-08-31 22:31:36,,"I recently made a CPU emulator to learn more about how the machine functions on a low level (inspired heavily by Ben Eater's awesome YT channel). But I got sick of hard coding instructions in it so I created a simple assembler to make writing code for it easier. Next step is writing a ""higher"" level language that compiles to this.

Here's the source code: https://github.com/dibsonthis/CPU-Emulator"
See how my new tool visualizes Python code - and shows bugs in the code base.,lhqlk9,2021-02-12 02:07:20,,"I am sorry if my post doesn't sound like an innovation to you, but would like you to take a look at our projects as it evolved out of a research project! I thought people in this subreddit might be interested :) Oh and yes! Anyone can use it!

The repository I used is: [https://metabob.com/gh/galt2x/sherlock](https://metabob.com/gh/galt2x/sherlock?utm_source=Reddit%20Post&utm_medium=Reddit&utm_campaign=Reddit%20(r%2FProgrammingLanguages)%202-11-21)

The program works best on Google Chrome, If you would like to check out the website, I linked it [here](https://www.metabob.com/?utm_source=Reddit%20Post&utm_medium=Reddit&utm_campaign=Reddit%20(r%2FProgrammingLanguages)%202-11-21)."
YOU WON'T BELIEVE what it looks like to have an IDE for the TABLOID programming language!,12h2epz,2023-04-10 09:14:40,,oh my god I love this
pLam - for anyone exploring λ-calculus,a0yyfc,2018-11-28 04:57:40,,Cool! I've wanted to have a pure lambda calculus environment to play around with and see how much it could do.
Flow - a little language I've been working on,xw189u,2022-10-05 13:02:23,,"Just FYI, Facebook has [a JS dialect called Flow](https://flow.org/). There was a point in time that Flow and TypeScript were duking it out, but clearly TS has won. I *think* Flow is still used inside Facebook, but I don't think it has really caught on in the larger developer ecosystem."
I had a T-shirt made!,d69kff,2019-09-19 13:30:27,,Interpret | Compile? :)
[Hobby Project] I wrote a lexer generator that takes in token definitions (pictured) based on patterns and outputs a lexer. This makes it so much easier to create new languages or modify an existing language's syntax.,lpqoxl,2021-02-22 23:23:10,,"Missign link to repository, anyway good work !!!

I made a similar, Lex alternative, program as my CS final project.

I use ""set"" instead of ""container"", and also use ""token"" like ""Lex"", but the ""transformation"" section it's the one I don't get how it works, although sounds interesting.

In my app., simple tokens always use quotes, I notest you didn't.

Are the ""transform"" and ""pattern"" sections the rules for detect complex tokens ?

Cheers."
Guido van Rossum joins Microsoft,jsyfnm,2020-11-13 01:06:56,,"I decided that retirement was boring and have joined the Developer Division at Microsoft. To do what? Too many options to say! But it’ll make using Python better for sure (and not just on Windows :-). There’s lots of open source here. Watch this space.

***

posted by [@gvanrossum](https://twitter.com/gvanrossum)

^[(Github)](https://github.com/username) ^| ^[(What's new)](https://github.com/username)"
Glide - data transformation language (documentation in comments),ybjtax,2022-10-23 22:57:37,,It seems like in this language the pipe operator will be use used very often. Why not make it an easier to type operator. -> is harder to type than >> or | or even just .
Research programming language with compile-time memory management,gfgn0r,2020-05-08 06:23:26,"[https://github.com/doctorn/micro-mitten](https://github.com/doctorn/micro-mitten)

I've been working on implementing the compile-time approach to memory management described in this thesis ([https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.pdf](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-908.pdf)) for some time now - some of the performance results look promising! (Although some less so...) I think it would be great to see this taken further and built into a more complete functional language.","Thanks for sharing! The original work is a bit dense, so having an implementation to browse side-by-side along with a written description in dissertation form is great, nicely done! Looking forward to digging into it."
What Operators Do You WISH Programming Languages Had? [Discussion],ya87l1,2022-10-22 07:37:02,"Most programming languages have a fairly small set of *symbolic operators* (excluding reassignment)—Python at 19, Lua at 14, Java at 17. Low-level languages like C++ and Rust are higher (at 29 and 28 respectively), some scripting languages like Perl are also high (37), and array-oriented languages like APL (and its offshoots) are above the rest (47). But on the whole, it seems most languages are operator-scarce and keyword-heavy. Keywords and built-in functions often fulfill the gaps operators do not, while many languages opt for libraries for functionalities that should be *native*. This results in multiline, keyword-ridden programs that can be hard to parse/maintain for the programmer. I would dare say most languages feature too little abstraction at base (although this may be by design).

Moreover I've found that *some* languages feature useful operators that aren't present in most other languages. I have described some of them down below:

**Python (// + & | \^ @)**

Floor divide (//) is quite useful, like when you need to determine how many minutes have passed based on the number of seconds (mins = secs // 60). Meanwhile Python overloads (+ & | \^) as list extension, set intersection, set union, and set symmetric union respectively. Numpy uses (@) for matrix multiplication, which is convenient though a bit odd-looking.

**JavaScript (++ -- ?: ?? .? =>)**

Not exactly rare– JavaScript has the classic trappings of C-inspired languages like the incrementors (++ --) and the ternary operator (?:). Along with C#, JavaScript features the null coalescing operator (??) which returns the first value if not null, the second if null. Meanwhile, a single question mark (?) can be used for nullable property access / optional chaining. Lastly, JS has an arrow operator (=>) which enables shorter inline function syntax.

**Lua (# \^)**

Using a unary number symbol (#) for length feels like the obvious choice. And since Lua's a newer language, they opted for caret (\^) for exponentiation over double times (\*\*).

**Perl (<=> =\~)**

Perl features a signum/spaceship operator (<=>) which returns (-1,0,1) depending on whether the value is less, equal, or greater than (2 <=> 5 == -1). This is especially useful for bookeeping and versioning. Having regex built into the language, Perl's bind operator (=\~) checks whether a string matches a regex pattern.

**Haskell (<> <\*> <$> >>= >=> :: $ .)**

There's much to explain with Haskell, as it's quite unique. What I find most interesting are these three: the double colon (::) which checks/assigns type signatures, the dollar ($) which enables you to chain operations without parentheses, and the dot (.) which is function composition.

**Julia (' \\ .+ <:  :  ===)**

Julia has what appears to be a tranpose operator (') but this is *actually* for complex conjugate (so close!). There is left divide (\\) which conveniently solves linear algebra equations where multiplicative order matters (Ax = b  becomes  x = A\\b). The dot (.) is the broadcasting operator which makes certain operations elementwise (\[1,2,3\] .+ \[3,4,5\] == \[4,6,8\]). The subtype operator (<:) checks whether a type is a subtype or a class is a subclass (Dog <: Animal). Julia has ranges built into the syntax, so colon (:) creates an inclusive range (1:5 == \[1,2,3,4,5\]). Lastly, the triple equals (===) checks object identity, and is semantic sugar for Python's ""is"".

**APL ( ∘.×  +/   +\\  ! )**

APL features reductions (+/) and scans (+\\) as core operations. For a given list A = \[1,2,3,4\], you could write +/A == 1+2+3+4 == 10 to perform a sum reduction. The beauty of this is it can apply to any operator, so you can do a product, for all (reduce on AND), there exists/any (reduce on OR), all equals and many more! There's also the inner and outer product (A+.×B  A∘.×B)—the first gets the matrix product of A and B (by multiplying then summing result elementwise), and second gets a cartesian multiplication of each element of A to each of B (in Python: \[a\*b for a in A for b in B\]). APL has a built-in operator for factorial and n-choose-k (!) based on whether it's unary or binary. APL has many more fantastic operators but it would be too much to list here. Have a look for yourself! [https://en.wikipedia.org/wiki/APL\_syntax\_and\_symbols](https://en.wikipedia.org/wiki/APL_syntax_and_symbols)

**Others (:=: \~> |>)**

Icon has an exchange operator (:=:) which obviates the need for a temp variable (a :=: b akin to Python's (a,b) = (b,a)). Scala has the category type operator (\~>) which specifies what each type maps to/morphism ((f: Mapping\[B, C\]) === (f: B \~> C)). Lastly there's the infamous pipe operator (|>) popular for chaining methods together in functional languages like Elixir. R has the same concept denoted with (%>%).

It would be nice to have a language that featured many of these *all at the same time*. Of course, tradeoffs are necessary when devising a language; not everyone can be happy. But methinks we're failing as language designers.

By no means comprehensive, the link below collates the operators of many languages all into the same place, and makes a great reference guide:

[https://rosettacode.org/wiki/Operator\_precedence](https://rosettacode.org/wiki/Operator_precedence)

**Operators I wish were available:**

1. Root/Square Root
2. Reversal (as opposed to Python's \[::-1\])
3. Divisible (instead of n % m == 0)
4. Appending/List Operators (instead of methods)
5. Lambda/Mapping/Filters (as alternatives to list comprehension)
6. Reduction/Scans (for sums, etc. like APL)
7. Length (like Lua's #)
8. Dot Product and/or Matrix Multiplication (like @)
9. String-specific operators (concatentation, split, etc.)
10. Function definition operator (instead of fun/function keywords)
11. Element of/Subset of (like ∈  and  ⊆)
12. Function Composition (like math: (f  ∘ g)(x))

**What are your favorite operators in languages or operators you wish were included?**","I don't have any suggestion that you haven't mentioned, but I wanted to thank you for the extremely comprehensive list, and the interesting question. This is exactly the kind of post I like seeing in this subreddit."
"Announcing Hush, a modern shell scripting language",ubwizf,2022-04-26 05:51:56,"Hush is a new shell scripting language that aims to enable developers to write robust shell scripts. It provides support for complex data structures and common programming paradigms, without giving up on ergonomic shell capabilities.

Official guide: [https://hush-shell.github.io/](https://hush-shell.github.io/)  
Repository: [https://github.com/hush-shell/hush](https://github.com/hush-shell/hush)","I’ve written a lot of POSIX compliant shell script and a little bit of bash.  And a lot of Python for work.
To me first impressions of this make it seem more like a traditional scripting language like Perl or Python.  I had a brief read of the guide and you mention that you focused on robustness and that it may give up some flexibility, for me this is the wrong way around, I like shell scripting languages because they are more flexible for the most part than traditional scripting languages like Python.  However, with all that said I’m intrigued and have a question, what are the advantages of me using this over Python, Perl or maybe Julia?  I’m comparing it to traditional scripting languages because the syntax is different enough that I would have to learn a new language and so what would be the advantage of using Hush over one of those?"
Bob Nystrom's Crafting Interpreters book is available on Amazon. Been waiting to grab a printed copy of this. Yay!,ou3h9z,2021-07-30 02:40:39,https://www.amazon.com/dp/0990582930/ref=cm_sw_r_cp_apa_glt_fabc_67MWY723CW71J54M9GTJ,Nice! I really enjoy his style of writing and his drawings. I hope he tackles another fun subject!
CS 6120: Advanced Compilers: The Self-Guided Online Course,kb4szf,2020-12-11 23:25:40,,CS 5120 doesn't seem to have a self-study equivalent. :(
Zig: File for Divorce from LLVM,14mno1w,2023-06-30 10:18:42,,"I wish them well, but I think they drastically underestimate the difficulty of their mission."
"DitLang: Write functions in any other language! Follow up to ""KirbyLang"" post from 6 months ago",oh3sx7,2021-07-10 04:19:52,,You probably spend a lot of effort for this. I still have doubts. Programming languages are not only about syntax. The biggest difference between programming languages comes from the semantic. You seem to concentrate on dynamic languages. Your example is about some generic number type. But languages implement such a generic type in different ways. Some use floats while others use rationals or big-integers. What about compiled languages. What about different string representations. There are many open questions.
NSA urges orgs to use memory-safe programming languages,ys89gv,2022-11-11 19:53:42,,Government agency? Memory safe language? Sounds like a job for Java.
Making JS Garbage Collection 30% faster by differential calculus,yegnk7,2022-10-27 10:56:19,,"also, MARISA KIRISAME?"
RustScript: A simple functional based programming language with as much relation to Rust as JavaScript has to Java,n58lkd,2021-05-05 13:48:32,,"Hahaha, it’s not even written in Rust. Have you checked out [rustscript](https://github.com/faern/rustscript)? It generates a cargo project from one-off Rust files and lets you treat (actual) Rust as a scripting language. I definitely love your Haskell and Python influence, it looks fun to use!"
Dennis Ritchie's first C compiler on GitHub,m3e98d,2021-03-12 17:41:02,,"First *self-hosted* C compiler, I think. Small but worthwhile distinction."
LLVM Tutorial,kjuh0b,2020-12-25 14:25:11,,"This is golden, and saved to my stash, thank you!"
"I’m learning Python, but my favorite resource is my dads Basic textbook from the 70s.",lqv6em,2021-02-24 06:41:16,,"As mentioned in the comments, this doesn't really belong here. However, we only noticed this just now, so I'll leave it up this one time."
Worst Design Decisions You've Ever Seen,uhtxqi,2022-05-04 08:15:36,"Here in r/ProgrammingLanguages, we all bandy about what features we *wish* were in programming languages — arbitrarily-sized floating-point numbers, automatic function currying, database support, comma-less lists, matrix support, pattern-matching... the list goes on. But language design comes down to *bad* design decisions as much as it does *good* ones. What (potentially fatal) features have you observed in programming languages that exhibited horrible, unintuitive, or clunky design decisions?","I work on Dart. The original unsound optional type system was such a mistake that we took the step of replacing it in 2.0 with a different static type system and did an enormous migration of all existing Dart code.

The language was designed with the best of intentions:

* Appeal to fans of dynamic typing by letting them not worry about types if they don't want to.
* Appeal to fans of static types by letting them write types.
* Work well for small scripts and throwaway code by not bothering with types.
* Scale up to larger applications by incrementally adding types and giving you the code navigation features you want based on that.

It was supposed to give you the best of both worlds with dynamic and static types. It ended up being more like the lowest common denominator of both. :(

* Since the language was designed for running from source like a scripting language, it didn't do any real type inference. That meant untyped code was *dynamically* typed. So people who liked static types were forced to annotate *even more* than they had to in other fully typed languages that did inference for local variables.

* In order to work for users who didn't want to worry about types at all, `dynamic` was treated as a top type. That meant, you could pass a `List<dynamic>` to a function expecting a `List<int>`. Of course, there was no guarantee that the list actually only contained ints, so even fully annotated code wasn't reliably safe.

* This made the type system unsound, so compilers couldn't rely on the types even in annotated code in order to generate smaller, faster code.

* Since the type system wasn't *statically* sound, a ""checked mode"" was added that would validate type annotations at runtime. But that meant that the type annotations had to be kept around in memory. And since they were around, they participated in things like runtime type checks. You could do `foo is Fn` where `Fn` is some specific function type and `foo` is a function. That expression would evaluate to `true` or `false` based on the parameter type annotations on that function, so Dart was never really optionally typed and the types could never actually be discarded.

* But checked mode wasn't the default since it was much slower. So the normal way to run Dart code looked completely bonkers to users expecting a typical typed language:

        main() {
          int x = ""not an int"";
          bool b = ""not a bool either"";
          List<int> list = x + b;
          print(list);
        }

    This program when run in normal mode would print ""not an intnot a bool either"" and complete without error.

* Since the language tried not to use static types for semantics, highly desired features like extension methods that hung off the static types were simply off the table.

It was a good attempt to make optional typing work and balance a lot of tricky trade-offs, but it just didn't hang together. People who didn't want static types at all had little reason to discard their JavaScript code and rewrite everything in Dart. People who did want static types wanted them to actually be sound, inferred, and used for compiler optimizations. It was like a unisex T-shirt that didn't fit anyone well.

Some people *really* liked the original Dart 1.0 type system, but it was a small set of users. Dart 1.0 was certainly a much *simpler* language. But most users took one look and walked away.

Users are much happier now with the new type system, but it was a hard path to get there."
Introducing the future of Scheme...take your S-expressions to the next level with Scheme 2-D!,ntr4l5,2021-06-07 01:42:03,,"Finally, a good and elegant language I can use"
Unpopular Opinions?,jd30p7,2020-10-18 04:57:17,"I know this is kind of a low-effort post, but I think it could be fun. What's an unpopular opinion about programming language design that you hold? Mine is that I hate that every langauges uses `*` and `&` for pointer/dereference and reference. I would much rather just have keywords `ptr`, `ref`, and `deref`.

Edit: I am seeing some absolutely *rancid* takes in these comments I am so proud of you all","I think the ""weirdness budget"" concept is overrated.  In particular, there is far too much blind emulation of C/C++.

In fact, I think there is occasionally something to be said for deliberately choosing unfamiliar syntax:

* To prevent confusion.  E.g., it can be a bad idea to use familiar syntax to represent unfamiliar semantics.
* As a context cue.  E.g., Python looks different from C++-alikes, so it is harder to confuse your Python reflexes with your C++-alike reflexes."
Introducing the Beef Programming Language,elbt5u,2020-01-07 21:51:09,"Beef is an open source performance-oriented compiled programming language which has been built hand-in-hand with its IDE environment. The syntax and many semantics are most directly derived from C#, while attempting to retain the C ideals of bare-metal explicitness and lack of runtime surprises, with some ""modern"" niceties inspired by languages such as Rust, Swift, and Go. See the [Language Guide](https://www.beeflang.org/docs/language-guide/) for more details.

Beef's primary [design goal](https://www.beeflang.org/docs/foreward/) is to provide a fluid and pleasurable development experience for high-performance real-time applications such as video games, with low-level features that make it suitable for engine development, combined with high-level ergonomics suitable for game code development.

Beef allows for safely mixing different optimization levels on a per-type or per-method level, allowing for performance-critical code to be executed at maximum speed without affecting debuggability of the rest of the application.

Memory management in Beef is manual, and includes first-class support for custom allocators. Care has been taken to reduce the burden of manual memory management with language ergonomics and runtime safeties – Beef can detect memory leaks in real-time, and offers guaranteed protection against use-after-free and double-deletion errors. As with most safety features in Beef, these memory safeties can be turned off in release builds for maximum performance.

The Beef IDE supports productivity features such as autocomplete, fixits, reformatting, refactoring tools, type inspection, runtime code compilation (hot code swapping), and a built-in profiler. The IDE's general-purpose debugger is capable of debugging native applications written in any language, and is intended to be a fully-featured standalone debugger even for pure C/C++ developers who want an alternative to Visual Studio debugging.

Binaries and documentation are available on [beeflang.org](https://www.beeflang.org). Source is available on [GitHub](https://github.com/beefytech/Beef/).",Looks like an enormous amount of work. Impressive. Still trying to figure out how the GUI is implemented (custom implementation based on SDL2?) and what third party dependencies (besides LLVM) there are.
What are the current hot topics in type theory and static analysis?,13c53kf,2023-05-09 05:16:06,"I’m taking a compilers course right now and really enjoying it, so I’m trying to potentially get involved in PL research. I’ve read some papers from the lab in my college and I’ve found the type theory and static analysis-related papers to be interesting and I’m wondering what are some topics or trends that current research is focusing on.","From order of “least bleeding-edge but still kind of new” to “there isn’t a mainstream language with this yet” based on my opinion:

[**Coroutines**](https://en.wikipedia.org/wiki/Coroutine), async/await and general multicore support in the type system. Most languages by now either have some variant of async / await (JavaScript, Kotlin, Swift, Rust) or super-lightweight threads (Go, Elixir, Java via Project Loom), or they just have Monads which supersede coroutines entirely (Haskell, Scala). It's at the point where [some say a language isn't suitable for production if it doesn't have good multicore support](https://news.ycombinator.com/item?id=35852321#unv_35855074) (also see Rust speeding through getting async/await *even though they already had `Send + Sync`*, and looking to add async traits). Even Python and C++ have coroutines now, and of course there is a [coroutine library for C which uses macros and low-level magic](https://github.com/hnes/libaco).

[**Gradual typing**](https://en.wikipedia.org/wiki/Gradual_typing). This is what TypeScript is: you have some typed values and untyped values, and want to verify and possibly optimize the typed code while permitting the untyped code; thus, the user is able to take an existing untyped codebase and ""gradually"" add types. Besides JavaScript/TypeScript, you'll find gradual typing on pretty much any other popular untyped language including [Python](https://docs.python.org/3/library/typing.html), [Lua](https://github.com/andremm/typedlua), and [Racket](https://docs.racket-lang.org/ts-guide/). Mojo is also going to have gradual typing.

[**Flow typing**](https://en.wikipedia.org/wiki/Flow-sensitive_typing). This is where, when you have a condition like `if (foo instanceof Bar)`, the compiler knows that within the `if` block `foo` is an instance of `Bar` and allows you to treat it as such. TypeScript, Kotlin, and Swift all have this. Most functional languages like Rust, Haskell, and OCaml probably will never have it because explicit pattern matching achieves essentially the same thing, it's more useful for languages like TypeScript where you already have code checking for subtypes via `if`.

[**Linear/affine (substructural) types**](https://en.wikipedia.org/wiki/Substructural_type_system). Rust has them and is doing great, Haskell has implemented them as a “new experimental”-kind of thing. Other languages are looking to incorporate some variant or alternative to Rust’s borrowing rules: [Mojo (the hyped-up “AI” language) includes them](https://docs.modular.com/mojo/programming-manual.html#argument-passing-control-and-memory-ownership), and [Val-lang](https://www.val-lang.dev/) has mutable value semantics which are similar.

First-class **distributed and multicore computing**. [Swift has first-class “actors” and “distributed” methods](https://www.swift.org/blog/distributed-actors/). [Unison](https://www.unison-lang.org/), Erlang, and Elixir are built with distributed being one of the #1 concerns. Though first-class is not super common and I don't really expect it to be because usually libraries are enough (e.g. Scala has [Akka](https://akka.io/) and is used WIDELY for distributed); whereas something like linear types and typed effects, you can't emulate in a library.

[**Effect systems**](https://en.wikipedia.org/wiki/Effect_system) and [**Algebraic effects**](https://ncatlab.org/nlab/show/algebraic+effect). ocaml has *just* released a stripped-down effect system. People are also working on Effect systems for Haskell ([eff](https://github.com/hasura/eff), [fused-effects](https://hackage.haskell.org/package/fused-effects), [effet](https://hackage.haskell.org/package/effet)). Koka is a language built with effects first and foremost and it’s rapidly gaining popularity. Unison also has effects. Verse, the hot new language for Unreal/Fortnite “for the metaverse”, has limited effects and massive parallazability (which is related to the multicore section)

[**Formal methods**](https://en.wikipedia.org/wiki/Formal_methods). This is *not* in most general-purpose programming languages and probably never will be (maybe we'll see formal methods to verify unsafe code in Rust...) because it's a ton of boilerplate (you have to help the compiler type-check your code) and also extremely complicated. However, formal methods is very important for proving code secure, such as [sel4](https://sel4.systems/) (microkernel formally verified to not have bugs or be exploitable) which has [just received the ACM Software Systems Award 3 days ago](https://awards.acm.org/software-system).

-	Most of the proof assistants out there: [Lean](https://leanprover.github.io/), [Coq](https://coq.inria.fr/), [Dafny](https://dafny.org/), [Isabelle](https://isabelle.in.tum.de/), [F*](https://www.fstar-lang.org/), [Idris 2](https://github.com/idris-lang/Idris2), and [Agda](https://github.com/agda/agda). And the main concepts are [dependent types](https://en.wikipedia.org/wiki/Dependent_type), [Homotopy Type Theory AKA HoTT](https://en.wikipedia.org/wiki/Homotopy_type_theory), and [Category Theory](https://en.wikipedia.org/wiki/Category_theory). **Warning:** HoTT and Category Theory are *really dense*, you’re going to really need to research them.

**Secure computing**. This includes [**Fully Homomorphic Encryption AKA FHE**](https://github.com/google/fully-homomorphic-encryption), of which there is a [language/compiler which just got released](https://www.zama.ai/post/zama-concrete-fully-homomorphic-encryption-compiler) and [Google’s older FHE compiler](https://jeremykun.com/2023/02/13/googles-fully-homomorphic-encryption-compiler-a-primer/). FHE is probably more “compiler” than “type system”, e.g. Google’s compiler works on C++. Also [**Security Type Systems**](https://en.wikipedia.org/wiki/Security_type_system) which include  [Oblivious data structures](https://en.wikipedia.org/wiki/Oblivious_data_structure) and [Oblivious ADTs](https://www.cs.purdue.edu/homes/bendy/OADT/oadt.pdf).

-	I haven't seen these actually used much though, most people just use smart security practices (e.g. don't store passwords in plaintext, don't send confidential information from server to client), and secure hardware. And I don't even think there's a real working compiler for Secure Type Systems, and concrete and Google's compiler haven't been used in any projects I'm aware of, so this is still a very early topic.

Lastly, look at anything in recent and upcoming like ICFP [2022](https://icfp22.sigplan.org/)/[2023](https://icfp23.sigplan.org/), PLDI, POPL, SPLASH, etc. These papers have the real bleeding-edge stuff, most of which are super dense proof-of-concepts, and it may literally take decades for these ideas to go into a real-world production compiler. But unlike the above, you will be able to learn concepts from them which have not been done before, and who knows, maybe one of them will be the next big thing."
My new type system caught a bug in my own standard library that would have ruined someone's day at runtime,zhk74l,2022-12-10 13:40:45,"That's it. I just wanted to share that. I'm having a proud dad moment (the type system being my child).

I was hesitant to spend time building a proper type system originally, but I'm so glad I decided to do it. Having a typing stage in the pipeline has made my language ([Glide](https://github.com/dibsonthis/Glide)) feel so much closer to a real language than the toy language I've always seen it as.

It's also given me the ability to hugely simplify my virtual machine, since the type system handles a bunch of errors I was checking at runtime.

Next step is an optimization layer in the pipeline to make it go fast.","Edit: Can't seem to edit the post on my phone. Amidst my excitement I missed to share the bug:

A function inside my standard library was referencing an object that didn't exist, and on top of that it was also reassigning another object variable to a very different object type, which if returned would have been pretty annoying to debug.

Not that crazy, but still. You'd expect the standard lib to not have such annoying errors."
Comparing algebraic data types: Rust and Datatype99,nc1o18,2021-05-14 14:02:56,,"I've made a simple comparison of Rust and [Datatype99], a Rust-inspired pure C algebraic data types library. Of course, Datatype99 is more limited: it doesn't allow nested patterns, `if` predicates, and other advanced stuff.

I hope you'll find it interesting. This comparison can also help Rust developers to get used to [Datatype99] quicker, and vice versa.

[Datatype99]: https://github.com/Hirrolot/datatype99"
I'm impressed with Raku,hnk0wy,2020-07-09 00:19:45,"Sorry if this kind of post doesn't belong here.

A professor at my uni has recommended Raku (formerly Perl 6) to me as an interesting language with a bunch of cool design choices. I'm a programming language enthusiast and a hobby designer, so obviously, I got interested.

Perl has a bad rap of being unreadable, messy, and so on. So I was kinda expecting the same from Raku, but boy was I mistaken.

Now a disclaimer, I'm only a week or two into learning it and yes, there is some learning curve. But I'm very impressed. The language is clean, consistent, and most of all: extremely practical. There is a function for everything and the code you write is usually very concise, yet quite readable. Grammars are a true OP feature for a hobby language designer like me. The language is also very disciplined, for example, arguments to functions are immutable by default, including arrays and stuff.

It is kind of unfortunate that so few people use it, however, that could change considering the language was fully released only 4 years ago and renamed to Raku just 1 year ago.

But even if nobody used it, it would still probably be the most practical language for hobby language designers that I have encountered yet.

Thanks for reading, I just wanted to share.","> Sorry if this kind of post doesn't belong here.

No, it's all good. We're friendly. :)

> A professor at my uni has recommended Raku

Yeah, PL enthusiast hobbyists paying attention to academics who like good design and practical stuff may turn out to be a tiny niche it gets some play in in the next few years. Several folk who shaped its design are CS folk who are also great PL teachers. I think it shows.

> Perl has a bad rap of being unreadable, messy, and so on. So I was kinda expecting the same from Raku, but boy was I mistaken.

Larry has described it as ""My mom told me to clean up after myself if I make a mess.""

> I'm only a week or two into learning it and yes, there is some learning curve.

It's a rich language. Countless moons ago I wrote a bunch of assembler for my $dayjob. It's relatively simple to learn. Then a never ending journey of learning the ins and outs of the practical consequences of this or that way of writing code, or using a particular platform, or type, or library function, etc. etc. Raku actually cleans a lot of that up, but that means it's much larger, and it's still a never ending journey, and it's got its own bunch of bugs, and traps, and on an on.

Fortunately most of the language and compiler is written in the language, so as you learn it, you empower yourself to better understand and improve both it and the compiler.

> But I'm very impressed. The language is clean, consistent, and most of all: extremely practical. There is a function for everything and the code you write is usually very concise, yet quite readable.

That's the intent. It doesn't always pan out that way, and a lot of folk insist it's all a pile of crap. (Typically because it ""must be"" because it's associated with the word Perl, which isn't a pile of crap either, but whatchya gonna do? Imo Raku still isn't mature enough yet to actively push much -- the time will come but not yet imo.)

> Grammars are a true OP feature

I love me some grammar. [One my SOs](https://stackoverflow.com/questions/45172113/extracting-from-bib-file-with-raku-previously-aka-perl-6/45181464#45181464).

""OP feature""?

> The language is also very disciplined, for example, arguments to functions are immutable by default, including arrays and stuff.

I agree, but it's more nuanced than some readers (and perhaps you) will think based on black-and-white thinking.

The notion that array arguments are immutable by default is both true and not true, and can be made entirely true or entirely untrue relatively easily.

> It is kind of unfortunate that so few people use it, however, that could change considering the language was fully released only 4 years ago and renamed to Raku just 1 year ago.

If it's vital to you that the number of people using it changes, well, I think that's unknown, and imo unlikely to change in the next few years beyond the steady slow growth that seems to be happening. I think a few pieces need to fall into place to start shifting that trajectory.

> But even if nobody used it, it would still probably be the most practical language for hobby language designers that I have encountered yet.

Right. And it has some Aces up its sleeve. When the time is right, Rakuns will start playing them.

> Thanks for reading, I just wanted to share.

Sure. And welcome. :)"
Have you heard about Seed7,n0nii7,2021-04-29 04:05:45,"Hello, I am Thomas Mertes. I have created a programming language based on my diploma and doctoral theses. I've been working on it since 1989 and released it after several rewrites in 2005 under the name Seed7. Since then, I improve it on a regular basis. Seed7 follows several [design principles](http://seed7.sourceforge.net/faq.htm#design_principles). The [Homepage](http://seed7.sourceforge.net/) contains more information about Seed7.

Seed7 has an [interpreter](http://seed7.sourceforge.net/faq.htm#interpreter) and a [compiler](http://seed7.sourceforge.net/faq.htm#compile), which compiles to machine code (via a C compiler as back-end). Beyond that, Seed7 provides [run-time libraries](http://seed7.sourceforge.net/libraries/index.htm) which cover many areas. The run-time libraries are essential for the [portability](http://seed7.sourceforge.net/faq.htm#portable) of Seed7 programs.

I consider libraries written in Seed7 a better approach than libraries that use an FFI to access external (binary) libraries. In the spirit of open source, you can look at the implementations of [TLS](http://seed7.sourceforge.net/libraries/tls.htm), [AES](http://seed7.sourceforge.net/libraries/aes.htm), [LZW](http://seed7.sourceforge.net/libraries/lzw.htm), [LZMA](http://seed7.sourceforge.net/libraries/lzma.htm), [XZ](http://seed7.sourceforge.net/libraries/xz.htm), [ZSTD](http://seed7.sourceforge.net/libraries/zstd.htm), [INFLATE](http://seed7.sourceforge.net/libraries/inflate.htm), [TAR](http://seed7.sourceforge.net/libraries/tar.htm), [AR](http://seed7.sourceforge.net/libraries/ar.htm), [CPIO](http://seed7.sourceforge.net/libraries/cpio.htm), [FTP](http://seed7.sourceforge.net/libraries/ftp.htm), [ZIP](http://seed7.sourceforge.net/libraries/zip.htm), [RPM](http://seed7.sourceforge.net/libraries/rpm.htm), [BMP](http://seed7.sourceforge.net/libraries/bmp.htm), [PNG](http://seed7.sourceforge.net/libraries/png.htm), [GIF](http://seed7.sourceforge.net/libraries/gif.htm), [JPEG](http://seed7.sourceforge.net/libraries/jpeg.htm) and more. You might know what I mean if you ever searched for the source code of a corresponding C library and tried to understand it. Many people see libraries as a black box. I see black boxes as good concept, but I also like the opportunity to open a black box and see how it works. With Seed7 you can do that.

To demonstrate the possibilities of Seed7, I programmed the Unix utilities [tar](http://seed7.sourceforge.net/scrshots/tar7.htm), [ftp](http://seed7.sourceforge.net/scrshots/ftp7.htm) and [make](http://seed7.sourceforge.net/scrshots/make7.htm) with it. I also implemented a [ftp server](http://seed7.sourceforge.net/scrshots/ftpserv.htm), an [http(s) server](http://seed7.sourceforge.net/scrshots/comanche.htm) and a [BASIC interpreter](http://seed7.sourceforge.net/scrshots/bas7.htm) in Seed7. Various other Seed7 programs can be found [here](http://seed7.sourceforge.net/scrshots/index.htm).

Please tell me what you think about Seed7 and its [Homepage](http://seed7.sourceforge.net/).

Support for Seed7 is always welcome.

Regards

Thomas Mertes",1989!!! Since 1989!!!! Thats a whole new level of determination
I wrote a new programming language that compiles to SQL,kuh899,2021-01-11 00:14:14,"Hi everyone,

I’ve spent the last year working on a new interpreted, relational language, that I call Preql. It compiles to SQL at runtime (similar to how Julia does it). I'm hoping it can be to SQL the same thing that C was to Assembly: A high-level abstraction that makes work more efficient, and lets your code be more safe and expressive, without getting too much in your way.

I wrote it in Python, with heavy use of dataclasses and multiple-dispatch (which I implemented using function decorators), and Lark as the parser.

This is still a very young project, with a lot of missing features, but I believe it is already useful, and can be used to do real work.

I’m looking forward to hearing your thoughts, ideas, and even criticisms :)

Preql on Github: https://github.com/erezsh/Preql

Tutorial for the language: https://preql.readthedocs.io/en/latest/tutorial.html","The project looks really cool! What are some of the things you don't like about SQL that you were hoping to improve with this project?

One thing that stands out to me right away is that the syntax is very imperative, Vs SQLs declarative, which personally I think is a big value driver behind SQL."
Are people too obsessed with manual memory management?,110gitm,2023-02-12 22:07:36,"I've always been interested in language implementation and lately I've been reading about data locality, memory fragmentation, JIT optimizations and I'm convinced that, for most business and server applications, choosing a language with a ""compact""/""copying"" garbage collector and a JIT runtime (eg. C# + CLR, Java/Kotlin/Scala/Clojure + JVM, Erlang/Elixir + BEAM, JS/TS + V8) is the best choice when it comes to language/implementation combo.

If I got it right, when you have a program with a complex state flow and make many heap allocations throughout its execution, its memory tends to get fragmented and there are two problems with that:

First, it's bad for the execution speed, because the processor relies on data being close to each other for caching. So a fragmented heap leads to more cache misses and worse performance.

Second, in memory-restricted environments, it reduces the uptime the program can run for without needing a reboot. The reason for that is that fragmentation causes objects to occupy memory in such an uneven and unpredictable manner that it eventually reaches a point where it becomes difficult to find sufficient contiguous memory to allocate large objects. When that point is reached, most systems crash with some variation of the ""Out-of-memory"" error (even though there might be plenty of memory available, though not contiguous).

A “mark-sweep-compact”/“copying” garbage collector, such as those found in the languages/runtimes I cited previously, solves both of those problems by continuously analyzing the object tree of the program and compacting it when there's too much free space between the objects at the cost of consistent CPU and memory tradeoffs. This greatly reduces heap fragmentation, which, in turn, enables the program to run indefinitely and faster thanks to better caching.

Finally, there are many cases where JIT outperforms AOT compilation for certain targets. At first, I thought it hard to believe there could be anything as performant as static-linked native code for execution. But JIT compilers, after they've done their initial warm-up and profiling throughout the program execution, can do some crazy optimizations that are only possible with information collected at runtime. 

Static native code running on bare metal has some tricks too when it comes to optimizations at runtime, like branch prediction at CPU level, but JIT code is on another level.

JIT interpreters can not only optimize code based on branch prediction, but they can entirely drop branches when they are unreachable! They can also reuse generic functions for many different types without having to keep different versions of them in memory. Finally, they can also inline functions at runtime without increasing the on-disk size of object files (which is good for network transfers too).

In conclusion, I think people put too much faith that they can write better memory management code than the ones that make the garbage collectors in current usage. And, for most apps with long execution times (like business and server), JIT can greatly outperform AOT.

It makes me confused to see manual memory + AOT languages like Rust getting so popular outside of embedded/IOT/systems programming, especially for desktop apps, where strong-typed + compact-GC + JIT languages clearly outshine.

What are your thoughts on that?

EDIT: This discussion might have been better titled “why are people so obsessed with unmanaged code?” since I'm making a point not only for copying garbage collectors but also for JIT compilers, but I think I got my point across...","There are a few areas in which GC is not at all appropriate. And arguably in power constrained environments (e.g. in terms of battery), GC is at a disadvantage because it requires substantially more silicon to be powered, e.g. on the order of twice as much DRAM -- which requires cyclic electrical refresh. In some papers and simulations, GC appears to use fewer total CPU cycles to do its job (a benefit of doing memory management in large batches), but in most of the real world implementations that people actually use today, GC-based solutions are still more CPU intensive than non-GC solutions. Similarly, the state of the art in GC is pauseless, yet most programmers still are using older GC implementations with their ""stop the world"" behavior.

So the academic work on GC has been hugely successful. The work by Sun/Oracle on Java GC implementations (nearly a dozen production GC implementations by this point), Azul on their own custom CPU and hardware and subsequently on a fully software Java GC implementation, Microsoft on the CLR GC, and Google on V8 GC -- it's all been a huge success technology-wise. In ten years, when everyone is finally updated to use the latest stuff, the term ""GC pause"" will be an ancient memory.

Apple with ""ARC"" reference counting implementations (first Objective C, and now Swift) and Rust with its enforced ownership model are each a huge step up from C. And C++ hasn't been sitting still, either; memory management in modern C++ is (arguably?) much closer to Rust than it is to C. I can't even remember the last time that I used the `delete` keyword.

Which is to say that no one has been sitting still. Heck, even COBOL is object oriented now 🤷‍♂️

>_If I got it right, when you have a program with a complex state flow and make many heap allocations throughout its execution, its memory tends to get fragmented and there are two problems with that: First, it's bad for the execution speed [..], Second, in memory-restricted environments, it reduces the uptime the program can run for without needing a reboot [..] most systems crash with some variation of the ""Out-of-memory"" error_

Neither of these is a real concern in any modern implementation. Memory fragmentation is handled quite easily by virtual memory managers with 64 bits (probably 40 or 48 in reality) of fake address space to play around with, and large amounts of flash storage to page to/from. Also, `malloc()` doesn't allocate directly from the OS, so malloc() doesn't itself cause memory fragmentation that would impact other programs. And while fragmentation does impact performance (in a small way) by making the CPU L_n_ caches less effective, that is a minor detail in a hugely complex system of other-things-that-could-be-better.

>_why are people so obsessed with unmanaged code?_

Three fairly obvious reasons come to mind:

1. Some people actually work in environments and with code bases that require lower level memory management. This is somewhere on the order of 2-5% of developers. Not a huge percentage, but a solid chunk of the industry.

2. Some people have old habits, and old habits die hard. If you learned ancient languages like C or C++, or God forbid, any of the hundreds of languages that almost no one remembers the name of now, then you learned to manage memory, because you had no choice. So, with apologies to the Simpsons, ""old man yells at cloud"" is a real thing. A lot of old dogs don't want to learn new tricks.

3. A lot of developers, particularly young developers, imagine that languages like C++ represent ""leveling up"" in the game of development. These developers spend their days toiling in cesspools of Python and Javascript, converting XML to JSON to SQL to TOML to XML to SQL to protobuf to JSON, while simultaneously wrestling with Kubernetes Amazon Docker React Chef, and imagine that ""real programmers"" are having non-stop sex parties while slinging around manual memory management code and probably some cool inline assembly that's so fast that the CPU clocks actually run backwards. In a sense, this is ""manual memory management as a fetish"", and I don't kink shame, so more power to the new generation in exploring whatever makes them segfault the biggest.

It would seem that you've encountered #3."
An Accessible Introduction to Type Theory and Implementing a Type Checker,ss3w6n,2022-02-14 14:27:12,,"> If you try to run `5 / ""Hello""`, it won’t actually run the code, JS/Python will see `""Hello` has type `string` and will throw a runtime error instead of executing it.

I wish JS would throw a runtime error. Instead, because it’s JS, it coerces `""Hello""` into a number (`NaN`), so `5 / ""Hello""` evaluates to `NaN`."
Introducing Neb: A parser with the nebulous purpose of reading mathematical syntax (https://github.com/JohnDTill/Neb),lbbr69,2021-02-03 09:07:51,,"GitHub: [https://github.com/JohnDTill/Neb](https://github.com/JohnDTill/Neb)

Neb is the result of work to parse mathematical notation. It provides support for parsing Unicode and expressions typeset in the MathBran format, as in [YAWYSIWYGEE](https://github.com/JohnDTill/YAWYSIWYGEE). See the GitHub page for sample images.

This library stops at creating an AST; there are a world of possibilities beyond that such as creating a matrix manipulation language, computer algebra system, checking for valid equations, etcetera. I made a few simple interpreters but never arrived at anything spectacular.

The parser is presented as-is. I had hoped to eventually use this as part of a research program at WKU, but personal issues dictate a reduction in work output, so I want to publish this in case anyone is working similar problems. I don't have plans for extensive work on this, but am happy to answer any questions."
Lessons learned over the years.,kro7li,2021-01-06 21:49:58,"I've been working on a language with a buddy of mine for several years now, and I want to share some of the things I've learned that I think are important:

First, parsing theory is nowhere near as important as you think it is. It's a super cool subject, and learning about it is exciting, so I absolutely understand why it's so easy to become obsessed with the details of parsing, but after working on this project for so long I realized that it's not what makes designing a language interesting or hard, nor is it what makes a language useful. It's just a thing that you do because you need the input source in a form that's easy to analyze and manipulate. Don't navel gaze about parsing too much.

Second, hand written parsers are better than generated parsers. You'll have direct control over how your parser and your AST work, which means you can mostly avoid doing CST->AST conversions. If you need to do extra analysis during parsing, for example, to provide better error reporting, it's simpler to modify code that you wrote and that you understand than it is to deal with the inhumane output of a parser generator. Unless you're doing something bizarre you probably won't need more than recursive descent with some cycle detection to prevent left recursion.

Third, bad syntax is OK in the beginning. Don't bikeshed on syntax before you've even used your language in a practical setting. Of course you'll want to put enough thought into your syntax that you can write a parser that can capture all of the language features you want to implement, but past that point it's not a big deal. You can't understand a problem until you've solved it at least once, so there's every chance that you'll need to modify your syntax repeatedly as you work on your language anyway. After you've built your language, and you understand how it works, you can go back and revise your syntax to something better. For example, we decided we didn't like dealing with explicit template parameters being ambiguous with the `<` and `>` operators, so we switched to curly braces instead.

Fourth, don't do more work to make your language less capable. Pay attention to how your compiler works, and look for cases where you can get something interesting for free. As a trivial example, `2r0000_001a` is a valid binary literal in our language that's equal to 12. This is because we convert strings to values by multiplying each digit by a power of the radix, and preventing this behavior is harder than supporting it. We've stumbled across lots of things like this over the lifetime of our project, and because we're not strictly bound to a standard we can do whatever we want. Sometimes we find that being lenient in this way causes problems, so we go back to limit some behavior of the language, but we never start from that perspective.

Fifth, programming language design is an incredibly under explored field. It's easy to just follow the pack, but if you do that you will only build a toy language because the pack leaders already exist. Look at everything that annoys you about the languages you use, and imagine what you would like to be able to do instead. Perhaps you've even found something about your own language that annoys you. How can you accomplish what you want to be able to do? Related to the last point, is there any simple restriction in your language that you can relax to solve your problem? This is the crux of design, and the more you invest into it, the more you'll get out of your language. An example from our language is that we wanted users to be able to define their own operators with any combination of symbols they liked, but this means parsing expressions is much more difficult because you can't just look up each symbol's precedence. Additionally, if you allow users to define their own precedence levels, and different overloads of an operator have different precedence, then there can be multiple correct parses of an expression, and a user wouldn't be able to reliably guess how an expression parses. Our solution was to use a nearly flat precedence scheme so expressions read like Polish Notation, but with infix operators. To handle assignment operators nicely we decided that any operator that ended in `=` that wasn't `>=`, `<=`, `==`, or `!=` would have lower precedence than everything else. It sounds odd, but it works really well in practice.

tl;dr: relax and have fun with your language, and for best results implement things yourself when you can","I agree with the majority of what you've said; I just wanted to add -

I'm looking to have an IDE along with the language, this has shaped my parser to some degree.

1. Its necessary to produce a CST, but parsing is better on a AST. You can embed the AST into the CST very simply. Whitespace is tagged onto the right hand token (As i always have an EOF node). Basically three points to a string rather than two (ws\_start, middle, txt\_end). Storing pointers to the AST nodes, and a children list for all nodes - in nonterminal nodes, etc.
2. 99% of the time, input text is invalid. Because.. well, you're still typing it! This means error nodes and recovery is quite important. More important than a batch based compiler.
3. My current recommendations are - For a batch based compiler, make a RDP as you've said. I am looking to move to an LR parser - As this will make the stack explicit - and means I could do parsing on text deltas - Recovery from a giving parsing point. This isn't possible in LL IMHO.

Anyway, good summary. Keep coding, Kind regards,

Mike"
Vale,hplj2i,2020-07-12 08:57:34,"After eight years, Vale just hit its first major milestone: we ran our first real program! It's a basic [terminal roguelike game](https://github.com/ValeLang/Vale/blob/master/Midas/test/tests/roguelike.vale) (you can walk around and bump into enemies to defeat them) but under the hood it's using the full spectrum of language features: interfaces, generics, polymorphic lambdas, ownership, destructors, universal function call syntax, infix calling, tuples, arrays, underscore params, externs, destructuring, and even const generics. With this program and our other tests, we can finally say that Vale's approach works!

We'll be introducing Vale to the world over the next few weeks, but I wanted to start the conversation here. We couldn't have gotten this far without all the brilliant folks here in r/programminglanguages and in the discord, and we want to hear your thoughts, questions, criticisms, and ideas on where we should go next!

More can be found on [https://vale.dev/](https://vale.dev/), but here's a reddit-post-sized explanation of what Vale is and where it's going:

Vale's main goal is to be as fast as C++, but much easier and safer, without sacrificing aliasing freedom. It does this by using ""[constraint references](https://vale.dev/ref/references)"", which behave differently depending on the compilation mode:

* [Normal Mode](https://vale.dev/ref/references), for development and testing, will halt the program when we try to free an object that any constraint ref is pointing at.
* [Fast Mode](https://vale.dev/ref/references) compiles constraint refs to raw pointers for performance on par with C++. This will be very useful for games (where performance is top priority) or sandboxed targets such as WASM.
* [Resilient Mode](https://vale.dev/ref/references) (in v0.2) will compile constraint refs to weak refs, and only halt when we dereference a dangling pointer (like a faster [ASan](https://en.wikipedia.org/wiki/AddressSanitizer)). This will be useful for programs that want zero unsafety.

Vale v0.2 will almost completely eliminate Normal Mode and Resilient Mode's overhead with:

* Compile-time [""region"" borrow checking](https://vale.dev/ref/regions), where one place can borrow a region as mutable, or multiple places can borrow it as immutable for zero-cost safe references. It's like Rust but region-based, or Verona but with immutable borrowing.
* Pure functions, where a function opens a new region for itself and immutably [borrows the region outside](https://vale.dev/ref/regions), making all references into outside memory zero-cost.
* [""Bump calling""](https://vale.dev/ref/regions), where a pure function's region uses a bump allocator instead of malloc/free.

Between these approaches, we get performance and memory safety *and* mutable aliasing. We suspect that in practice, Vale programs could incur even less overhead than Rust's usual workarounds (Rc, or Vec + generational indices), and with easy bump calling, could even outperform C++ in certain circumstances.

We hope that Vale will show the world that speed and safety can be easy.

Vale explicitly does not support shared mutable ownership (C++'s shared\_ptr, Rust's Rc, Swift's strong references), though it does allow shared ownership of [immutable objects](https://vale.dev/ref/structs). One would think that a language needs shared mutables, but we've found that single ownership and constraint references largely obviate the need. In fact, taking out shared ownership opened a lot of doors for us.

In a few days we'll post to various sites (here, r/cpp, r/programming, HN, etc.) about how our approach enabled us to take RAII further than ever before, with multiple destructors, destructor params and return values, and un-droppable owning references. After that, we might post about constraint refs' potential for cross-compilation, or how RC + regions could drastically outperform garbage collection. We're interested in your thoughts, reply below or swing by our [discord](https://discord.gg/SNB8yGH)!

PS. Fun fact, eight years ago Vale was originally called vlang, but [http://vlang.org](http://vlang.org) (since taken down) and more recently [http://vlang.io](http://vlang.io) already have that name, so we called it GelLLVM (in honor of Gel which first introduced constraint refs in 2007) and recently settled on the name Vale.","Congrats on finally getting this ready!

I'd love to try making a 3D engine in this and learn Vulkan at the same time.  Is there a way to interface with Vulkan and other potentially useful existing libraries?"
C Isn't A Programming Language Anymore - Faultlore,tg55ow,2022-03-17 16:09:47,,"I'm not a C programmer (long time .NET jabroni) but have been playing with a toy transpiler to teach myself more about low level programming and how VMs/runtimes are working under the hood, and the unspecific type sizes has definitely been driving me a little crazy. The pretending-to-be-C state of FFI across language boundaries has bugged me for pretty much as long as I can remember, but I also think this is moreso the fault of languages for not having a good way to describe opaque types and how to deal with them, and having little to no sense of ABI stability. I was expecting the author to bring up the WASM interface proposals since this is one of the first major steps forward for FFI boundaries that I'm personally aware of. I know as an early programmer I often scratched my head as to why we never normalized something other than the C ABI, and I feel that way more than ever 15 years later.

I'm a bit surprised to see people call out the article as unproductive. These things seem obvious or necessary to people entrenched in PL design or low level programming, but these are valid complaints a lot of us have to deal with even in app development. I can't say I agree with the authors conclusion but I certainly felt validated by the shared horror of the imprecise type system none of us can reliably parse definitions from. If  we have a lingua franca, these things should be on our minds in a world with seemingly never ending platforms and runtimes. I guess I don't see what there is to be done, but surely it's a discussion worth having?"
Microfeatures I'd like to see in more languages,104lxrd,2023-01-06 13:32:30,,The automatic lift for collections is indeed a cool one. I can see this being useful pretty much everywhere. Specially considering how we still write code as if machines were single threaded.
What are some cool/wierd features of a programming language you know?,mpiscj,2021-04-13 01:29:32,I'm asking this question out of curiosity and will to widen my horizons.,"Several different features from the ""pure functional"" world:

- [Koka](https://koka-lang.github.io/koka/doc/index.html): effect systems
- [Haskell](https://en.wikibooks.org/wiki/Haskell/GADT): GADTs (""generalized algebraic data types""; which are an alternative to ""path-dependent types"")
- [Idris](https://idris2.readthedocs.io/en/latest/app/linear.html): Linear Types + Dependent Types"
What's your favorite programming language? Why?,fc274s,2020-03-02 06:51:22,What's your favorite programming language? Why?,"Forth, because of its incredible power under extreme resource constraints.

It's amazing how little infrastructure it takes to get an extensible compiler and interactive environment running."
"My bachelors project. A compiler, assembler and a CPU that the code finally runs on.",1460agz,2023-06-10 21:27:45,"[https://github.com/CoolBassist/Final-year-project](https://github.com/CoolBassist/Final-year-project)  


In this project I designed and simulated a CPU which is based on the SAP-1, with a custom ISA. A compiler which takes my high level language called JWhile, which has features such as branches, for/while loops, arrays. And an assembler which takes the assembly and turns it into machine code for the CPU.

I have also added a CPU emulator, so you dont need to download the software that I used to create the ""real"" CPU.

Any feedback/comments would be greatly appreciated :)",I love stuff like this.  Good work!
Parser generators vs. handwritten parsers: surveying major language implementations in 2021,p8vvcs,2021-08-22 01:46:35,,"```Although parser generators are still used in major language implementations, maybe it's time for universities to start teaching handwritten parsing?```

A university that only teaches you how to use specific tools, rather than the theory behind the tools, is not worth attending."
if … then … else had to be invented,kc9fy9,2020-12-13 19:59:11,,"If you’re not interested in reading 5 pages to get to the main point, here it is:

> I think a carefully-chosen German word was probably translated as an archaic English word and then never revisited. Unfortunately we do not have the original German text to consult."
Writing a Simple Garbage Collector in C,12g5ekt,2023-04-09 09:56:20,,"Of all things *this* is the place that *finally* explains how to get memory from the kernel without going through someone else's alligator! (Oh sure I could have hit `man sbrk` but I did and it was inscrutable. Also it was many years ago, like '93 or something, so all is forgiven.) 

Good show, lads!"
What are the worst features you've tried in your programming language?,yhm4vx,2022-10-31 02:33:52,"A few years ago I had a talk with a professor on ways to shape the development of physics. A problem we identified is that everyone celebrates their successes, without going too much into the details of why the failures encountered along the way didn't work. So the mistakes don't get shared, and every research group tries the same failed ideas at least once.

In the PL world, we have sort of a celebration of very very bad (or simply very weird) programming ideas in esolangs. That being said, most esolangs are jokes, and nobody would think of non-ironically implementing a brainfuck-like interface in their language. I'm more interested in things that appeared to be good ideas but end up backfiring or not being a right fit.

So to the question, does anyone in here have a feature you've tried that ended up not working out?","After completing my low-level cs course, I decided it would be great to do my own version of C. I thought that since C devs encounter a lot of memory leak issues, why not make a C compiler that properly inserts the right free() given a malloc? I mean, it must just be like fixing an unbalanced sequence of parentheses right? After a couple of weeks of working on it, it was much harder than I thought and put the project on hold. It wasn't until I talked about with my compiler professor and after taking my computability course that I realized that I was essentially trying to solve a variant of the halting problem. 

I still think if I had two more weeks to do it, I coulda done it /s"
Learn Assembly by Writing Entirely Too Many Brainfuck Compilers in Rust,jsf9cr,2020-11-12 03:49:34,,"Hey, thanks for submitting my article to this subreddit! I thought about submitting it here myself a couple days ago but wasn't sure if it was ""on topic"" enough for this subreddit. I'm glad to see that it got a good response and people are enjoying it."
Let's collect relatively new research programming languages in this thread,yvuvuv,2022-11-15 19:49:00,"There is probably a substantial number of lesser known academic programming languages with interesting and enlightening features, but discovering them is not easy without scouring the literature for mentions of these new languages. So I propose we list the languages we know of thus helping each other with this discoverability issue. The requirement is that the language in question should have at least one published paper associated with it.","- [Cogent](https://trustworthy.systems/projects/TS/cogent.pml), late 2010s, a language with linear types for verification. The idea is that you write functional-looking code that is easy to verify using the functional semantics, but with an efficient compilation strategy enabled by linear types to get realistic system programs.

- [Granule](https://granule-project.github.io/), early 2020s, a language designed around ""graded monads"" and linear types. ""Other examples include capturing fine-grained information about side effects, data use, privacy levels, cost, and permissions via various kinds of (co)effect types captures via graded modal types.""

- [Eff](https://www.eff-lang.org/), 2010s, the language that introduced effect handlers

- [Futhark](https://futhark-lang.org/), late 2010s: SML-inspired functional programming for the GPU, executed very well. You need to revisit functional programming idioms and genericity features to understand those that can be efficiently mapped to a GPU, building on decades of work on data-parallel programming with a pragmatic focus of working well on today's machines. The [blog](https://futhark-lang.org/blog.html) is a great read. Actively used for research.

- [Hazel](https://hazel.org/), a ""live"" functional programming language focusing on typed holes and structured editing. Actively used for research.

- [Jasmin](https://github.com/jasmin-lang/jasmin/wiki), late 2010s, a language designed to be lower-level than C and provide good low-level control for cryptographic code. Basically a new take on ""C as a high-level assembly language"", with formal semantics etc. I suspect that this design space is rather close to ""a good language to use as a compiler backend"", but I think this would require changes to Jasmin and no one is working on that as far as I know. Actively used for research.

- [Koka](https://github.com/koka-lang/koka), already cited in this thread, early 2010s. Koka's first claim to fame was a usable effect system (at a time where, basically, effect systems were not usable in practice; in fact few languages have managed to do as well as Koka since). Now working on cool implementation strategies for functional languages as well. Actively used for research, and by a small community of programmers.

- [Mezzo](https://protz.github.io/mezzo/), designed in the 2010s, an ML-family languages with linear or rather separation-logic types and interesting ergonomic choices. One of the most usable ""let's use linear types in practice"" languages that is not Rust. (Rust was in development at the same time, so Mezzo was not inspired by it.)

- [Rosette](https://emina.github.io/rosette/), late 2010s, a language (embedded in Racket) that aims to gracefully combine usual programming and SMT solvers -- ""solver-aided programming"". Actively used for research.

- [Pony](https://www.ponylang.io/), 2010s, an efficient actor-based concurrent language (think: lower-level Erlang for systems programming). Memory/resource ownship and usage are controlled by ""reference capabilities"" (uniquely-owned, immutable, mutable but not sendable across actors). Actively used by a small community of programmers.

- [Pyret](https://www.pyret.org/index.html), late 2010s, a programming language design for teaching. There are very few languages designed by people who are both programming-language researchers and programming educators for the purpose of teaching, and it's worth checking out. Actively used for teaching and research.

- [Syndicate](https://syndicate-lang.org/), late 2010s, an interesting new take on concurrent programming, a sort of cool hybrid of actor-style message-passing and tuple-space fact-publishing model. Currently the basis of an experiment on [Structuring the system layer](https://syndicate-lang.org/projects/2021/system-layer/): ""Could dataspaces be a suitable system layer foundation, perhaps replacing software like systemd and D-Bus?""

- [Zélus](https://zelus.di.ens.fr/), late 2010s, a synchronous language (think Lustre / Lucid Synchrone) with continuous-time programming / ordinary differential equations. Actively used for research."
"Nulls really do infect everything, don't they?",w6fdpv,2022-07-24 06:01:33,"We all know about Tony Hoare and his admitted [**""Billion Dollar Mistake""**](https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/):

> Tony Hoare introduced Null references in ALGOL W back in 1965 *""simply because it was so easy to implement""*, says Mr. Hoare. He talks about that decision considering it *""my billion-dollar mistake""*.

But i'm not here looking at it not just *null pointer exceptions*,   
but how they really can infect a language,   
and make the right thing almost impossible to do things correctly the first time.

Leading to more lost time, and money: contributing to the ongoing *Billion Dollar Mistake*.

It Started With a Warning
---------------------

I've been handed some 18 year old Java code. And after not having had used Java in 19 years myself, and bringing it into a modern IDE, i ask the IDE for as many:

- hints
- warnings
- linter checks

as i can find. And i found a simple one:

> Comparing Strings using == or != 
> 
> Checks for usages of == or != operator for comparing Strings. String comparisons should generally be done using the equals() method.

Where the code was basically:

    firstName == """"

and the hint (and auto-fix magic) was suggesting it be:

    firstName.equals("""")

or alternatively, to avoid accidental assignment):

    """".equals(firstName)

In C# that would be a strange request
-------------------------

Now, coming from C# (and other languages) that **know** how to check string **content** for equality:

- when you use the *equality operator* (**` == `**)
- the compiler will translate that to `Object.Equals`

And it all [works like you, a human, would expect:](https://dotnetfiddle.net/p5zMrc)

    string firstName = getFirstName();
       		
- `firstName == """"`: False
- `"""" ==  firstName`: False
- `"""".Equals(firstName)`: False

And a lot of people in C#, and Java, will ***insist*** that you must **never** use:

    firstName == """"

and **always** convert it to:

    firstName.Equals("""")

*or possibly*:

    firstName.Length == 0

Tony Hoare has entered the chat
================

Except the problem with blindly converting:

    firstName == """"

into 

    firstName.Equals("""")

is that you've just introduced a **NullPointerException**. 

If `firstName` *happens* to be `null`:

- `firstName == """"`: False
- `"""" ==  firstName`: False
- `"""".Equals(firstName)`: False
- `firstName.Length == 0`: Object reference not set to an instance of an object.
- `firstName.Equals("""")`: Object reference not set to an instance of an object.

So, in **C#** at least, you are better off using the **equality operator** (` == `) for comparing Strings:

- it does what you want
- it doesn't suffer from possible NullPointerExceptions

And trying to 2nd guess the language just causes grief.

But the `null` really is a time-bomb in everyone's code. And you can approach it with the best intentions, but still get caught up in these subtleties.

Back in Java
===========

So when i saw a hint in the IDE saying:

- convert `firstName == """"`
- to `firstName.equals("""")`

i was kinda concerned, *""What happens if `firstName` is null? Does the compiler insert special detection of that case?""*

No, no it doesn't.

In fact Java it doesn't insert special null-handling code (unlike C#) in the case of:

    firstName == """"

This means that in Java its just **hard** to write safe code that does:

    firstName == """"

But because of the `null` landmine, it's very hard to compare two strings successfully.  

(Not even including the fact that Java's *equality operator* always checks for reference equality - not actual *string* equality.)

I'm sure Java has a helper function somewhere:

    StringHelper.equals(firstName, """")

But this isn't about that.

This isn't C# vs Java
------------------------------

It just really hit me today how **hard** it is to write correct code when `null` is allowed to exist in the language.  You'll find 5 different variations of string comparison on Stackoverflow. And unless you **happen** to pick the right one it's going to crash on you.

Leading to more lost time, and money: contributing to the ongoing *Billion Dollar Mistake*.

Just wanted to say that out loud to someone - my wire really doesn't care :)

Addendum
------------

It's interesting to me that (almost) nobody has caught that all the methods i posted above to compare strings are wrong. I intentionally left out the **1** correct way, to help prove a point.

Spelunking through this old code, i can see the evolution of learning all the gotchas.

- Some of them are (in hindsight) poor decisions on the language designers. But i'm going to give them a pass, it was the early to mid 1990s. We learned a lot in the subsequent 5 years
- and some of them are gotchas because `null` is allowed to exist

**Real Example Code 1** 

    if (request.getAttribute(""billionDollarMistake"") == """") { ... }

It's a gotcha because it's checking **reference equality** verses two strings being the same. Language design helping to cause bugs.

**Real Example Code 2**

The developer learned that the *equality operator* (**==**) checks for **reference** equality rather than equality. In the Java language you're **supposed** to call `.equals` if you want to check if two things are equal. No problem:

    if (request.getAttribute(""billionDollarMistake"").equals("""") { ... }

Except its a gotcha because the value **billionDollarMistake** might not be in the request. We're expecting it to be there, and barreling ahead with a **NullPointerException**. 

**Real Example Code 3**

So we do the C-style, *hack-our-way-around-poor-language-design*, and adopt a code convention that prevents a NPE when comparing to the empty string


    if ("""".equals(request.getAttribute(""billionDollarMistake"")) { ... }

**Real Example Code 4**

But that wasn't the only way i saw it fixed:

    if ((request.getAttribute(""billionDollarMistake"") == null) || (request.getAttribute(""billionDollarMistake"").equals("""")) { ... }

Now we're quite clear about how we expect the world to work:

    """" is considered empty
    null is considered empty
    therefore  null == """"

It's what we expect, because we don't care about `null`. We don't want `null`. 

Like in Python, passing a special ""nothing"" value (i.e. ""**None**"") to a compare operation returns what you expect:

> a `null` takes on it's *""default value""* when it's asked to be compared

In other words:

- **Boolean**: `None == false`  true
- **Number**: `None == 0` true
- **String**: `None == """"` true
 
Your values **can** be null, but they're still not-null - in the sense that you can get still a value out of them.","The problem isn't `null` itself. The concept of `null` (or `nil` or whatever) is well understood and reasonable.

The problem is the broken type system that states: ""The null type is the sub type of every reference type."" That allows `null` to be hiding inside of any variable / field / etc. that isn't explicitly a primitive type, and so the developer (in theory) needs to always check to make sure that each reference is not `null`.

Crazy. But easy to solve."
What tiny thing annoys you about some programming languages?,in3d8r,2020-09-05 23:48:56,"I want to know what not to do. I'm not talking major language design decisions, but smaller trivial things. For example for me, in Python, it's the use of `id`, `open`, `set`, etc as built-in names that I can't (well, shouldn't) clobber.","A few off the top of my head:

* Whitespace sensitivity in otherwise non-whitespace sensitive languages creeps me out. In a C macro, `foo (bar)` and `foo(bar)` mean different things. Likewise in Ruby.

* C just got the precedence of the bitwise operators wrong. They should bind tighter than the logical ones.

* PHP got the precedence of `?:` wrong compared to every other language that has that syntax.

* Having to put a space between `> >` for nested templates in older versions of C++ because the lexer got confused and treated it like a right shift. (Using angle brackets in general for templates and generics is annoying. ASCII needs more bracket characters.)

* Needing a `;` after `end` in Pacsal. It's consistent and keeps the grammar simpler, which I get, but it just looks ugly and feels redundant.

* Function type and pointer syntax in C is a disaster. Declaration reflects use was a mistake.

* Java went overboard with the length of some of its keywords. `implements`, `extends`, `protected`, etc. At least in C++, you only need to use an access modifier once and it applies to an entire section of declarations.

* Hoisting in JavaScript. Ick.

* 1-based indexing in Lua. I sort of get why they did it, but it's just painful to anyone coming from any other language."
I wrote my first interpreter!,ykd21p,2022-11-03 02:34:05,"Hi! Hope you guys are doing great!  

I've been part of this subreddit for a while now (I haven't posted anything until now, but I do read most of the posts on the sub and most of the comments on such posts) and after a lot of inspiration and good ideas gathered from multiple places, I was able to write my first tree walk interpreter for a superset of the Lox programming language.

Initially the whole project started as a read through of Crafting Interpreters and Compilers, but after a while I decided to add additional features (that I consider cool and useful), in order to keep on learning how the different parts on an interpreter fit together and how to represent certain language constructs on my own. It may not be the most efficient or cool implementation, but it definitely was a good starting point.

I decided to name my superset L# (it's written on C# and it's a Lox superset. How original, right?), it's in a super alpha stage but again, I think it is a good starting point. I want to thank all of you, since your comments on certain questions were pretty useful when I had a blurry idea on mind and needed some guidance to materialize it.

You can take a look at the [GitHub](https://github.com/jmj0502/LSharp) repo if you want. Any comments will be well appreciated!

Have an awesome day!",Congrats!
Lambda Calculus in 400 Bytes,t3d7kx,2022-02-28 19:11:01,,"> ... for more industrial scale applications a 520 byte version is provided too, that overcomes many of those limitations, although it requires writing code at an even higher difficulty setting.

This is exciting! My boss wants to know if an LTS version of this VM is planned?? He thinks we could use this as a back-end for our online Deep Learning platform..."
"Cyber is a new language for fast, efficient, and concurrent scripting",10mid0r,2023-01-27 19:11:32,,"Looks excellent, I'd like to give this a try. One thing I'll note on first look: I believe making 0s and empty strings 'falsey' is generally a mistake. It makes it ambiguous to determine if you really meant 0/"""" or 'no value'. 

And there isn't really much need for it in dynamic languages, where it's easy to mix `false` or `none` values in with your integers/strings if you really need to denote 'no value'.

Anyway, that's a bit of a nitpick, on the whole this looks like a very impressive project, well done!"
"How JIT Compilers are Implemented and Fast: Julia, Pypy, LuaJIT, Graal and More",hlut4h,2020-07-06 05:40:42,,Great read!
"Crafting ""Crafting Interpreters""",fvnfok,2020-04-06 07:06:15,,Congratulations /u/munificent on your tremendously valuable and inspiring resource. Keep looking for the fun and the light; it is out there even amidst the gloom and despair.
Garbage Collection · Crafting Interpreters,e41idc,2019-12-01 03:02:03,,"I've been waiting eagerly for this chapter for a while now. Very excited to read it! Seems like an incredible amount of work he does for free, so I'm definitely going to buy the book once it gets a physical release."
[meme] Isn't it?,jy7rd1,2020-11-21 17:00:33,,Let's not make low-effort /r/programmerhumor style posts a thing. There are better subreddits for such posts.
Building the fastest Lua interpreter.. automatically!,z2i4dp,2022-11-23 14:58:40,,Discussion on Hackernews: https://news.ycombinator.com/item?id=33711583
Lawvere - a categorical programming language with effects,lls7q9,2021-02-17 19:15:00,,What a cool idea! :D
Carbon's most exciting feature is its calling convention,wb6x1e,2022-07-29 22:55:52,,what’s the size threshold for passing by value
How to implement dependent types in 80 lines of code,11bo39o,2023-02-25 23:49:15,,"Interesting idea. The code is small indeed, but I think we need a bit more infrastructure to get a viable solution. The problem here is that evaluation is essentially the HOAS-flavored version of normalization by iterated substitution, so it's very inefficient. If you try to compute larger Church numerals (probably around 1k-10k) you'll see slowdowns.

(Note: `f n` is fine instead of `eval (f n)` [here](https://gist.github.com/Hirrolot/27e6b02a051df333811a23b97c375196#file-coc-ml-L28), because `f` already evaluates its result by the definition of `eval` on lambdas. However, this change also doesn't make the solution efficient.)

The embedded programs in the current example only create syntax but compute no redexes. That implies that HOAS binders only perform substitution and the normalizer has to repeatedly traverse and substitute terms. 

Alternatively, we could use the ""semantic"" version of application in embedded code which performs beta reduction. But that would not allow us to print or remember unreduced terms.

[Here's a version](https://gist.github.com/AndrasKovacs/7f81ac652052829809611236b018442e) which is both efficient and remembers all unreduced terms. It prints million-sized Church numerals instantly when compiled with `-O`. The idea is to compute both unreduced and fully reduced values in embedded programs at the same time. It is important to use laziness (I use Haskell) for the fully reduced values, because clearly we don't want to always normalize every embedded program."
Zig-style generics are not well-suited for most languages,xzfdix,2022-10-09 15:23:32,,"The unconstrained generic style (templates) has one big advantage, flexibility. It has two big disadvantages, documentation and error surfacing.

Since there are no constraints, the library writer can do whatever with the type and if the user happens to supply the right type then everything will be good. It is not limited by what the language's generics currently support.

Since there are no constraints, automatic documentation generators have no way to discover themselves any extra information about the generic type than what the author explicitly wrote down in doc comments.

Since there are no constraints, compilers have no way to communicate whether the error is in the user code (supplied the wrong type) or the library code (used operations that the documentation does not communicate as needed).

I have been thinking, then, what if we reach somewhere in the middle? Something with almost complete flexibility while still being almost easily documentable and letting the compiler show errors in correct location.

So here's what I had in mind. A 2 step generic definition.

```
fn generic_function<T, U>(foo: T, bar: SomeType<U>) -> OtherType<T, U>
where {
  std::ops::Add(T, U): U;
  T.field1: i32;
  T.field2: impl Debug;
  AnotherType<U>::a_method(): T;
  CompletelyArbitraryCompTimeFunction(SomeType<T>);
  // ...more constraints
} {
  // function implementation which can add T and U to get a U
  // and can access the i32 field1 on T and be able to get a
  // debug representation of field2 on T and other things

  // CompletelyArbitraryCompTimeFunction will have access to complete
  // reflection style capabilities since it runs at compile time and
  // is not even meant to be available at runtime
  // These type of opaque functions panic if constraints are not satisfied
  // and can directly specify valid marker traits and all that
}
```"
Implement a non-trivial hashing algorithm in your language.,m5sfzb,2021-03-16 03:16:18,"On a whim I implemented SHA-3 in our language last month. I wasn't intending to do anything with it except say I'd made it, but it's actually turned out to be pretty valuable as a way to detect compiler bugs. Statistically it's incredibly unlikely that a compiler bug that alters the hashing algorithm will change it in such a way that you'll still get the same output hash for a given input. Shove random values into your hashing algorithm and compare the output to a verified implementation, and you basically won't need to worry about false positives.

The caveat here is that you shouldn't take this as proof that your compiler can compile anything correctly. It's strong inductive evidence that it can compile your hashing algorithm correctly, but that's it. This is a tool for alerting you to problems, not formal verification.","Oh, this sounds like a great idea! I'll give it a try after work!"
Flix | The Flix Programming Language,occing,2021-07-02 23:31:28,,"Wow, this looks quite interesting."
I built a Lisp!,nsktgu,2021-06-05 09:43:30,"So last month, I literally did not even know what Lisp was.

A month later, I'd built my own programming language (from scratch in Go), a Lisp dialect inspired by Scheme and Clojure

I also documented my entire journey so you can see the entire process from noob -> little less of a noob

Try it out 👉 [lispy.amirbolous.com](http://lispy.amirbolous.com/)

Well-documented source: [https://github.com/amirgamil/lispy](https://github.com/amirgamil/lispy)

Journal/Blog post: [https://amirbolous.com/posts/pl](https://amirbolous.com/posts/pl)","Sorry, only because it's literally on the front page of your site, but it's 'brief' not 'breif'"
Swift type checking is undecidable,i367sn,2020-08-04 05:38:22,,"Hypothesis: Any sufficiently popular statically-typed language eventually grows enough type system features that type checking becomes undecidable and the language designers don't realize it until *after* said features ship.

Corollary: Having an undecidable type system is apparently not as dangerous as most type theorists believe."
A proposed Stack Exchange site for programming language development is close to entering beta!,12ke1o8,2023-04-13 14:00:16,,**Good _news_ everyone!**
Lessons from Writing a Compiler,w0biir,2022-07-16 16:06:33,,"W.R.T parsing, I've stopped prototyping languages syntax-first. I basically now start with AST-like structures and generate code / do interpretation from them. After getting the evaluation right, I build the scanner and parser.

You might have to change your data structures around a bit as you iterate, but having the semantics in place keeps you grounded. Rapid changes to your syntax don't feel as heavy, because most of the time you're targeting the same structures underneath."
Comparing Golang and Interface99,tglrw8,2022-03-18 06:11:38,,"Inspired by [my previous post](https://www.reddit.com/r/ProgrammingLanguages/comments/q40om0/comparing_interfaces_rust_and_interface99/), this comparison presents how stuff can be done with Golang and [Interface99](https://github.com/Hirrolot/interface99), my library featuring interfaces with dynamic dispatch for pure C99.

There are also some differences. Golang, for example, can [resolve interface methods at run-time](https://research.swtch.com/interfaces), whereas Interface99 constructs virtual tables statically. Interface99 allows [default implementations](https://github.com/Hirrolot/interface99/blob/master/examples/default_impl.c); Golang doesn't. And, of course, Interface99 mandates placing `impl(MyIface, MyType)`, whereas Golang uses a.k.a. duck typing for interfaces (interface implementations are indistinguishable from ordinary methods). Also, when you would use embedding in Golang, such as this:

    type ReadWriter interface {
        Reader
        Writer
    }

In Interface99, the above code translates to this:

    #define ReadWriter_IFACE
    #define ReadWriter_EXTENDS (Reader, Writer)
    interface(ReadWriter);

This is basically interface inheritance you might have seen in Rust.

I hope you'll find this comparison cute and interesting. Personally, I use [Interface99](https://github.com/Hirrolot/interface99) at [OpenIPC](https://github.com/OpenIPC) for some time and find it extremely useful in my C code."
"A Tale of Yak Shaving: Accidentally Making a Language, for an Engine, for a Game",twcrj9,2022-04-05 04:53:48,,"To my great delight (as a hobbyist) and great shame (as a software engineer), the level of yak-shaving is actually a bit more than implied in the article: I started working on my game back in 2012, and started the language back in 2013. I later posted a video from that time, at [https://www.youtube.com/watch?v=xp2b4mJspxI&ab\_channel=EvanOvadia](https://www.youtube.com/watch?v=xp2b4mJspxI&ab_channel=EvanOvadia)

Now here we are, almost a *decade* later, working on a language that's doing things nobody's ever seen before, I'd say that's a pretty good outcome!"
Zig: Statement Regarding the Zen Programming Language,itazke,2020-09-15 23:26:59,,"There seems to be some drama around Zig.

A summary:

* One of the open source contributors got banned
* That contributor then started a company, created a Zig fork called Zen, and is selling it in Japan
* The company has since hired two other big contributors to Zig, but are supposedly no longer with the company though they are restricted by a do not compete clause
* The Zen source code seems to be identical to Zig, except superficial changes
* The company has written some weird claims against Zig and its creator on the company website
* They filed for a trademark on ZIG in Japan
* Now Zig has released a public announcement"
Is there an operating systems that is a runtime of a programming language?,xyrah5,2022-10-08 20:14:34,"I mean, is there a computing environment in which everything is an application of a single programming language and the ""shell"" of this OS is the language itself?

Something like Emacs and ELisp but Emacs has parts written in C and runs on another operating system (can not be booted independently)

Is this the description of ""Lisp Machines""? Any other examples?

I wonder if it's necessary to have an operating system on a device...","This describes Smalltalk pretty well. [https://squeak.org/](https://squeak.org/) is the most popular current Smalltalk out there. It's not run baremetal these days, but the early Smalltalk systems were bare metal (Xerox, Tektronix).  


At a totally different level of abstraction,  bare metal Forth systems circa 1980, were essentially OS + programming environments combined."
In Defense of Programming Languages,oi00yu,2021-07-11 15:26:24,,"> On the contrary, I think we are still in the infancy of programming language design.

I think this is the foundation of the argument, really.

The truth of the matter is that programming languages are not even 100 years old yet. We've been refining the materials we use to build houses for millennia and _still_ making progress, it's the height of arrogance to expect that within a mere century we've achieved the pinnacle of evolution with regard to programming languages.

> New programming languages are too complicated!
>
> That's the way of the world.

I disagree.

First of all, I disagree that _new_ programming languages are the only ones that are complicated. C++ is perhaps the most complicated programming language out there, where even its experts (and creators) must unite and discuss together when particularly gnarly examples are brought up to divine what the specification says about it. And C++ was born in 1983, close to 40 years ago, though still 30 years after Lisp.

Secondly, I think that part of the issue with the complexity of programming languages is the lack of orthogonality and the lack of regularity:

 1. The lack of orthogonality between features leads to having to specify feature interactions in detail. The less orthogonality, the more interactions requiring specifications, and the most complex the language grew. That's how C++ got where it's at.
 2. The lack of regularity in the language means that each feature has to be remembered in a specific context. An example is languages distinguishing between statements and expressions, distinguishing between compile-time and run-time execution (and typically reducing the usable feature-set at compile-time), ...

And I think those 2 issues are _specifically_ due to programming languages being in their infancy. As programming languages evolve, I expect that we will get better at keeping features more orthogonal, and keeping the languages more regular, leading to an overall _decrease_ of complexity.

I also feel that are 2 other important points to mention with regard to complexity:

 1. Inherent domain complexity: Rust's ownership/borrowing is relatively complex, for example, however this mostly stems from inherent complexity in low-level memory management in the first place.
 2. Unfamiliarity with a (new) concept leads to a _perception_ of complexity of the language, even if the concept itself is in fact simple.

So, I disagree that complexity is inherent there, and that languages will necessarily grow more and more complex."
Query-based compiler architectures,hfs53y,2020-06-26 03:12:55,,"For the past several years I’ve been convinced of the value of incremental, query-based, build system–style compilation with a language server + program database model, and the superiority over traditional linear batch pipelines. It’s been immensely satisfying to see people get on board with the idea, and push the techniques for it farther forward than I could ever have done on my own—especially major industrial projects like Roslyn, but also many of my fellow PL enthusiasts here and on Twitter.

It’s also very validating that we arrived at the same general structure for Sixten and the latest implementation of Kitten, even based on the same papers, since it tells me that other people whose work I respect and admire have come to similar philosophies about what constitutes good compiler architecture. :)"
New LISP on the block 😎,142pydn,2023-06-07 03:00:05,"Hello all, I am excited to share with you a project I have been working on: [Relish](https://gitlab.com/whom/relish)!

Relish  is a homegrown LISP I have written from scratch using only safe Rust  (with the exception of libc calls in POSIX job control libraries). It  offers a full REPL with multiline editing and autocomplete. Relish  implements most features one would expect from a LISP (while, let,  lambda, quote/eval, def, if, etc...) as well as a fully interactive job  control shell! Included in the shell features are first class forms for  piped commands, command short circuiting, and IO redirection. It also  comes with a whole library of data manipulation functions for strings,  numbers, boolean values, and more.

Relish  offers a simple and easy to work with environment that lets the user  manipulate stored procedures and data as well as jobs and environment  variables. I originally wrote it because I was deeply dissatisfied with  Bash/Zsh and the like. It turns out being able to work with a homoiconic  language for your shell is super powerful. I can make self programming  routines that generate shell commands and bindings without individually  aliasing things or writing redundant boilerplate code. Relish also comes  with functions for writing and viewing documentation from the REPL.

After  dogfooding it for a few months I think Relish has reached a state where  its interface is more or less stable. I have a roadmap sketched out in  the Readme and the beginnings of some release CI (as well as something  like 125 unit tests). I also have a lot of examples of Relish in use in  both the CI and in the snippets directory. My goal is to create an  environment that is easy and natural to use that helps introduce more  people to programming their own tools and projects.

I  hope at least one other person thinks this is cool. I think Relish has a  lot of value and that having more perspectives and people willing to  experiment with this code would be super useful!

You (could possibly) wish your shell config looked this cool: [(My shell config)](https://pastebin.com/tRZPx9ff)

Relish called in CI: [(Tests for optional features written in Relish)](https://gitlab.com/whom/relish/-/blob/main/snippets/userlib-tests.rls)

Homoiconicity put to work for shell use: [(Shell command binding generator)](https://gitlab.com/whom/relish/-/blob/main/snippets/genbind.rls)

(Docs are linked to in the Readme)","Sounds interesting. Would like to try it out but sadly Im in Iran, gitlab blocks all Iranian (and Syrian and Cuban and north Korean and a bunch of other countries) IPs, and I dont have a functioning vpn at the moment

Oh well..

Just FYI, github doesn't block IPs, though it does restrict people like me from using anything except the basic version. So if part of your intention is to have it accessible to others you might want to host it on a site where everyone can access it easily.

Its easy to mirror a gitlab repo on github. I made a private copy so I can access it.

BTW trying to do `(call ""/DIR/TO/SNIPPETS/userlib.rsl"")` fails with:

<call script>: binary called, unimplemented.
Please refactor forms and try again...

I tried to run your relishrc file and had some trouble with loading new code."
Some language design lessons learned,12alw4w,2023-04-03 22:18:43,,"A good post.  Some commentary on individual points:

> Lexing, parsing and codegen are all well covered by textbooks. But how to model types and do semantic analysis can only be found in by studying compilers.

While it is true that lexing and parsing are probably where textbooks tell you almost everything you need to know, there are also textbooks that do a good job explaining type checking (of which semantic analysis is often a subpart).  While it is true the specifics will invariably depend on the language, most languages are going to have some kind of top-down lexical scope (and it's at least a good starting point).  The free book [Basics of Compiler Design](http://hjemmesider.diku.dk/~torbenm/Basics/) does a decent job explaining how to manage symbol tables and such for doing this.

> Don’t take advice from other language designers

I think this is much too aggressive (although the specific subpoints are not too bad).  More experienced language designers will be better at spotting contradictions in your design (e.g. type system features that are in conflict), which may save you a lot of work.

> “Better syntax” is subjective and never a selling point.

I don't think this is correct.  There are languages whose selling point is ""better syntax"" broadly construed, and this is *absolutely legitimate*.  The essay [Notation as a Tool of Thought](https://www.jsoftware.com/papers/tot.htm) is the classic explanation of this view.  It is true that *trivial* syntactical niceties don't matter too much, but I'm convinced that e.g. Ruby grew in popularity around 2005 because it allows a ""natural language-style"" syntax for programming (which Rails took full advantage of).  The elaboration of this point in TFA does mention that it's specifically warning against languages that are mere ""reskins"", but I want to make sure the title isn't taken too seriously.

(And of course, reskinning an existing language is probably a good way to *learn*.)

> There will always be people who hate your language no matter what.

You can definitely learn this without creating your own language.  You just have to read basically anything on the programmer-populated parts of the Internet."
"[humour/satire] Just came across the ""OK?"" language, thought people here might appreciate it .""OK?s mission is to do away with the needless complexity of today's programming languages and let you focus on what matters"".",y7ufsb,2022-10-19 13:57:37,,"OK? ingeniously pin-pointed majority of all pain-points allover a software developer's body, and successfully displaced every one of them to another location, like the pain in-the-butt to some where in-the-head, and some on-the-eye to in-the-wrist. So much relief that you can feel the pains in freshly new ways!"
LAMBDA: The ultimate Excel worksheet function,l5g4gd,2021-01-26 23:22:33,,"Re: Simon Peyton Jones

""He did it. That crazy son of a bitch. He did it."""
Let's talk about interesting language features.,rbp0vn,2021-12-08 19:10:38,"Personally, multiple return values and coroutines are ones that I feel like I don't often need, but miss them greatly when I do.

This could also serve as a bit of a survey on what features successful programming languages usually have.",The number one feature whose absence makes me want to curse is pattern matching.
"loxcraft: a compiler, language server, and online playground for the Lox programming language",132ximy,2023-04-29 23:12:54,,"[loxcraft](https://github.com/ajeetdsouza/loxcraft) started off as yet another implementation of Crafting Interpreters, but I decided to take it up a notch and build:

* syntax highlighting, via tree-sitter
* an online playground, via WebAssembly
* IDE integration, via Language Server Protocol
* an REPL
* really good error messages

Also, it's really fast! Most implementations of Lox turn out to be significantly slower than the one in C, but this one comes really close. Suggestions for how to improve performance further would be highly appreciated!"
Assembly interpreter inside of TypeScript's type system,yww51r,2022-11-16 23:17:23,,"TypeScript's type system is crazy advanced. We can even build entire programming languages on the type level. I built a whole Assembly interpreter in type annotations and it's cursed lol.

It's a bit long but I hope you like it :) All feedback is appreciated!

Oh and by the way, if you like the article, consider subscribing to my newsletter :) https://judehunter.dev/blog  
No pressure though."
Journal of Functional Programming moving to Open Access,qttgej,2021-11-15 00:31:39,,"(JFP is a well-established journal on language design and functional programming, similar in scope to the ICFP conference.)

Before, JFP offered a per-submission ""Open Access"" option at around 3K€ per paper. This is well above the accepted standards of the field, and essentially no one took it.

Now the publisher, Cambridge University Press (CUP) (which is supposedly a non-profit?) has decided to lower the publishing charges / APCs to around 1.5K€, but make the journal ""Open Access"" and therefore force all authors to pay it. But why exactly should anyone pay 1.5K€ for a mostly volunteer-run organization? (Is CUP charging more than its operating costs, or is its organization inefficient?)

The editors of the journal are then going through a complex dance of finding ways to avoid charging authors directly for APCs. Some research institutions are going to pay large registration fees to CUP, that will cover their members' ACPs. The editors seem to hope to get the rest paid by industrial sponsors interested in functional programming. (Facebook ?)

Personally, as a public-funded researcher, I prefer to submit work to an Open Access journal that charges a fair price / is supported by public institutions, instead of a private publisher getting sponsorship money from private companies to avoid asking authors directly to pay excessive APCs.

I will stick to LMCS or other Diamond Open Access journals, not JFP."
Carbon has well documented design rationales,w3juhj,2022-07-20 19:21:03,"You've probably all seen carbon lang by now: https://github.com/carbon-language/carbon-lang

I've been spending the last week browsing the language documentation, they've got incredibly well documented rationale, you might want to take inspiration in.

* Goals and more importantly non-goals: https://github.com/carbon-language/carbon-lang/blob/trunk/docs/project/goals.md
* Design principles: https://github.com/carbon-language/carbon-lang/blob/trunk/docs/project/principles/README.md
* Language design (although mostly incomplete): https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/README.md
* Every proposal for every feature: https://github.com/carbon-language/carbon-lang/blob/trunk/proposals/README.md","I do find their “if you can, please use any other fucking language” section a bit amusing"
Historical programming-language groups disappearing from Google,hzsm76,2020-07-29 10:42:52,,So tired of Google.
"From Turbo Pascal to Delphi to C# to TypeScript, an interview with PL legend Anders Hejlsberg",y1ryzt,2022-10-12 10:43:36,,"What a great video! This guy is definitely one of the heroes of programming languages as far as I’m concerned and the Context Free channel, as usual, produced a great interview with him. Thanks for posting!"
“Don’t listen to language designers”,11hil82,2023-03-04 07:26:12,"I realized that my most important lesson I learned, and the advice I’d like to pass on to other language designers is simply this:

_Don’t take advice from other language designers_

Nowhere else have I encountered as much bad advice as the ones language designers give to other language designers.

The typical advice I am talking about would go like this: “I did X and it’s great” or: “X is the worst thing you could do*.

Unfortunately *in practice* it turns out language designers (a) think in the context of their particular language and also (b) too often draw conclusions from their narrow experiences in the middle or even beginning of their language design and compiler construction.

While talking to other language designers is very helpful, just keep in mind to that what applies to one language might be really bad advice for another.","Someone told me I shouldn’t take advice from other language designers, so I didn’t read your post."
Thoughts on infectious systems: async/await and pure,vofiyv,2022-07-01 02:38:47,"It occurred to me recently why I like the `pure` keyword, and don't really like async/await as much. I'll explain it in terms of types of ""infectiousness"".

In async/await, if we add the `async` keyword to a function, all of its callers must also be marked `async`. Then, all of its callers must be marked `async` as well, and so on. `async` is **upwardly infectious**, because it spreads to **those who call you**.

(I'm not considering blocking as a workaround for this, as it can grind the entire system to a halt, which often defeats the purpose of async/await.)

Pure functions can only call pure functions. If we make a function `pure`, then any functions it calls must also be `pure`. Any functions they call must then also be `pure` and so on. D has a system like this. `pure` is **downwardly infectious**, because it spreads to **those you call**.

Here's the big difference:

 * You can always call a `pure` function.
 * You can't always call an `async` function.

To illustrate the latter:

 * Sometimes you can't mark the caller function `async`, e.g. because it implements a third party interface that itself is not `async`.
 * If the interface is in your control, you can change it, but you end up spreading the `async` ""infection"" to *all* users of those interfaces, and you'll likely eventually run into *another* interface, which you don't control.

Some other examples of upwardly infectious mechanisms:

* Rust's &mut, which requires all callers have zero other references.
* Java's `throw Exception` because one should rarely catch the base class Exception, it should propagate to the top.

I would say that we should often avoid upwardly infectious systems, to avoid the aforementioned problems.

Would love any thoughts!

Edit: See [u/MrJohz's reply below](https://www.reddit.com/r/ProgrammingLanguages/comments/vofiyv/comment/ied4gaw/?utm_source=reddit&utm_medium=web2x&context=3) for a very cool observation that we might be able to change upwardly infectious designs to downwardly infectious ones and vice versa in a language's design!","I think this is a really insightful point, but I think your argumentation is missing something. You're describing purity from the perspective of a language where the default is impurity - if you translate your idea to, say, Haskell, you'll find that the interesting functions aren't the pure ones, they're the impure ones - the ones that actually do something. If you analyse purity through the lens of impurity (that's an odd sentence), you'll find that it really is upwardly infectious, just like `async`.

I *think* it is always possible to convert an upwardly infectious colour system into a downwardly infectious one, and vice versa. Which then leads to the question: if it's always possible to switch between upwardly and downwardly infectious colours, why do we not always only use the downwardly infectious variant? And I think the answer to that is that the upwardly infectious version is always (or at least, almost always) the more useful or powerful version.

For example, with purity, in a language where impurity is the default, purity isn't necessarily all that interesting. It's very easy to write simple pure functions, but that's possible with or without an explicit `pure` annotation. There might be optimisation advantages, but most of the time, you aren't getting much out of the system unless you explicitly work on pushing more and more of your code into `pure`-land. And at a certain point, you've pushed all (or almost all) of your code into pure functions, at which point you're now back to an upwardly infectious system.

On the other hand, a language where purity is the default gives you significantly more guarantees about your code, at the cost of an upwardly infectious system from the start.

This kind of raises the question of whether languages exist with some sort of `sync` function modifier - essentially a downwardly infectious synchronicity guarantee. I think an answer could be any language with threads and locks. When I call code within a locked region, I can't call code that expects other code to be running simultaneously (this would create a deadlock), but if I add locking to a function, this doesn't affect its signature.

So to sum up:

* Every downwardly infectious system (probably) has an inverse upwardly infectious system.
* The upwardly infectious system is (probably) always the more powerful of these two, because it provides more guarantees about code execution.
* One might expect that a language providing a downwardly infectious system will find either that the downwardly infectious parts are not used (because they cause too much trouble or have few practical benefits), *or* that users will attempt to write as much code as possible within the infecting colour, thereby converting it into essentially an upwardly infecting system."
Compiling to Assembly from Scratch: book released!,j1wsvh,2020-09-29 18:39:51,,"I’d be quite curious about a book about compiling to assembly from MIT scratch . :) the playful cover of your book would be a good fit for it too!

In any case, congratulations on publishing a useful book! I hope you will inspire the next generation of language implementors!"
“How do I read type systems notation” overview by Alexis King,15qcea4,2023-08-14 06:31:23,,"Thanks! I've skipped over these weird symbols for far too long. Now they finally make sense. Mostly, that is. 

The output context seems relevant, but it lacks examples. Don't I need, like, some operators like add or remove for that to make sense? What are common notations for those?"
Why Lexing and Parsing Should Be Separate,gdt3xd,2020-05-05 15:18:59,,"I just read over this nice post from 8 days ago, and realized that it actually gave data on what I suggested, so I added it to this wiki page.

https://old.reddit.com/r/ProgrammingLanguages/comments/g8f2j5/speeding_up_the_sixty_compiler/

The basic argument is ""do the easy thing with the fast algorithm, and the hard thing with the slow algorithm"", and that appears to have shown up in practice.

----

I also added a quote from a retrospective on ShellCheck, another project that uses parser combinators in Haskell (via the parsec library).  They also ran into performance issues.  FWIW this is the only frontend I've ever used ""for real"" that uses parser combinators."
Book recommendations after reading “crafting interpreters”,x5soly,2022-09-05 01:34:04,"Hello, I finished the book crafting interpreters by Robert Nystrom. The book has helped me alot and felt like an amazing introduction to the field of language design and implementation. 

My question however is: what next to read?
I know of the dragon book and have read the first couple of chapters. But maybe there are better alternatives. Also, after crafting interpreters, i have a basic understanding of interpreted language design. However, I have the urge to study compiler design. 

So are there any books you would recommend me for my level of knowledge?","From what I remember, the book ""Engineering a Compiler"" goes over pretty much every part of a compiler (and you can probably skip the lexer/parser chapters since they'll probably duplicate much of ""Crafting Interpreters"").  There are some books specific to type theory if you want to learn more about that as well, ""Types and Programming Languages"", and the ""advanced"" sequel.  If you're looking for something more like ""Crafting an Interpreter"", there may be other books (I think ""Modern Compiler Implementation in Java"" or ""in C"" or ""in ML"" by Appel).  I wasn't a huge fan of the Appel book in Java when I first read it though, but I can't remember why."
Passerine – extensible functional scripting language – v0.8.0 released,k97g8d,2020-12-09 00:34:03,"I'm excited to share an early preview of a novel programming language I've been developing for the past year or so. Passerine is an *functional scripting language*, blending the rapid iteration of languages like Python with the concise correctness of languages like Ocaml, Rust, and Scheme. If you'd like to  learn more, read the Overview section of the [README](https://github.com/vrtbl/passerine#an-overview).

It's still a  ways away from being fully complete, but this release marks the introduction of Passerine's macro system. Like the order of songbirds it  was named after, Passerine sings to more than just one tune – this new  hygenic macro system makes it easy to extend the language itself –  allowing you to bend the langauge to your needs, rather than bending  your needs to the language!

Here's a quick overview of Passerine:

**Functions**  
Functions are defined with an arrow (`->`). They can close over their enclosing scope and be partially applied. Here's a function:

    -- comment
    add = a b -> a + b

Here are some function calls:

    -- standard
    fish apple banana
    -- parens for grouping
    outer (inner argument)
    -- functions can be composed
    data |> first |> second

A block is a group of expressions, evaluated one after another. It takes on the value of the last expression:

    -- value of block is ""Hello, Passerine!""
    {
        hello = ""Hello, ""
        hello + ""Passerine!""
    }

**Macros**  
Passerine has a hygienic macro system, which allows the language to be extended. Here's a simple (convoluted) example:

    -- define a macro
    syntax this 'swap that {
        tmp = this
        this = that
        that = tmp
    }
    
    tmp = ""Banana!""
    a = false
    b = true
    
    -- use the macro we defined
    a swap b
    -- tmp is still ""Banana!""

There's a lot I didn't cover, like concurrency (fibers), error handling, pattern matching, etc. Be sure to check out the repo! Comments, thoughts, and suggestions are appreciated :)

This submission links to the GitHub Repo, but there's also [a website](https://www.passerine.io) if you'd like to look at that.",The syntax is a bit strange for my taste but the language seems to be amazingly powerful and well designed. I like what you did with the fibers. Well done. The fact that you are a high school student is comforting to know that the next generation is equally capable or even better and also a little bit terrifying. You'll surpass us in a few years (or already did:)).
"I revamped my tree walking interpreter into a bytecode VM, and now I'm much happier with the performance",1612opy,2023-08-25 23:38:35,,"Last time I shared some [disappointing results](https://www.reddit.com/r/ProgrammingLanguages/comments/15dx5jq/feeling_disappointed_with_my_work_i_found_out/) with my software renderer written in my language Vortex. Back then, Vortex was an AST interpreter. Since then, I've taken the time to read and re-read Crafting Interpreters and the implementation of clox to wrap my head around what a decent bytecode interpreter looks like.

Finally, after a lot of refactoring and rewriting, Vortex has become a stack based VM. As you can see from the video, I'm running the same code as before, but this time it's a lot faster.

Some downsides include the stripping away (for now) of the type system, but I really just wanted to get the bare minimum working so I could compare performance.

For reference, all the math needed to render this *damn* teapot was done in the language itself, including matrix multiplication. Next step is to hook up GLM or similar and hopefully that makes things run smoothly.

Notice also that the program now starts instantly. In the last demo, just the process of parsing the OBJ file stalled the program. Now it's pretty smooth."
Penrose: From Mathematical Notation to Beautiful Diagrams,gv8dz4,2020-06-02 22:32:57,,Amazing.
I made a programming language!,tjw4wi,2022-03-22 13:55:10,"Hello, after some time lurking in this subreddit. I decided to make my own programming language! It's called [Hazure](https://github.com/azur1s/hazure) (a spinoff of my name, *azur*), syntax is inspired by OCaml and it transpile to Typescript!

Here are some examples:

example/io/hello.hz:

    fun main: void = do
        @write(""Hello, World!""); -- an intrinsic (hardcoded function) starts with `@`
    end;

example/69.hz

    fun add2 (lhs: int) (rhs: int): int = do
        return lhs + rhs;
    end;
    
    fun main: void = do
        let result: int = add2(34, 35);
        @write(result);
        if result == 69 then
            @write(""\nbig cool"");
        else
            @write(""\nnot cool"");
        end;
    end;

example/factorial.hz:

    fun factorial (n: int): int = do
        case n of
            | 0 -> return 1;
            | else return n * factorial(n - 1);
        end;
    end;
    
    fun main: void = do
        factorial(5)
        |> @write(_); -- pipe operators!
    end;

If you are a bit unsure about the syntax, I've included [SYNTAX.md](https://github.com/azur1s/hazure/blob/master/SYNTAX.md) to explain a bit further about the syntax. I hope it helps.

This language is still **in development**! There is still a lot of missing key features (e.g. no type-checking) and TODO's so (please) don't use it yet (but it is [turing complete](https://github.com/azur1s/hazure/blob/master/example/rule110.hz) I think) but it is still impressive for me and I'm proud of it :D

I'd love to know what you guys think about my language! I'm also making this alone so i'd love if you guys can help me a bit here, i'm not someone who is really that smart (i'm just 15 years old lol) so just wanted to share you guys some of my stuff :D

Github repo: [https://github.com/azur1s/hazure](https://github.com/azur1s/hazure)",What's the point of compiling to TypeScript if you're already doing the type checking? You can just go straight to JavaScript.
"If Lua is faster and smaller than Python, while being just as powerful and capable, then why is Python so much more popular?",tfurkb,2022-03-17 06:19:31,"I've been thinking about this recently, Lua is a really simple and small language while being just as powerful if not more powerful than Python.

It has a really simple syntax and the interpreter is so small that you can embed it anywhere, it can even be compiled to WASM to run Lua code on a website.

The C API is also really simple to use and you can run Lua code inside C or C code and libraries inside Lua.

With LuaJIT it's probably faster than both Python and Javascript.

But then why is Python so much more popular than Lua, what makes people not want to use Lua aside from embedding it into other programs for simple scripting tasks, am I missing something?","it certainly isn't as capable -- out of the box, lua doesn't even support simple things like regular expressions and JSON parsing

there have been a few efforts to produce a more featureful standard library, though.  [luvit](https://luvit.io), which also includes a whole libuv-based runtime system (basically ""node.js but with lua""), is the most active one i know about, but there may be more.  in any case, having a stable set of libraries you can always depend on to be there is a huge advantage for python."
Zig compiler frontend,sunff2,2022-02-17 20:14:34,,This is brilliant!  I've been pondering quite much about how Zig does all the stuff it does and this is really nice overview.
BCause - a Compiler for the B Programming Language,z2xyd8,2022-11-24 03:21:52,,`libb` is a way better name than `libc`.
"Introducing Penne (v0.2.1), a pasta-oriented programming language that favors the goto-statement for flow control",y31cde,2022-10-13 23:17:13,"Penne imagines a world where, instead of being ostracized for leading to so-called ""spaghetti code"", the humble `goto` statement became the dominant method of control flow, surpassing `for` loops and `switch` statements, and ultimately obviating the need for  exceptions, the invention of RAII and object-oriented programming in general. By applying modern sensibilities to the use of the `goto` statement instead of banishing it altogether, Penne seeks to bring about a rennaissance of pasta-oriented programming.

    fn determine_collatz_number(start: i32) -> i32
    {
        var x = start;
        var steps = 0;
        {
            if x == 1
                goto return;
            do_collatz_step(&x);
            steps = steps + 1;
            loop;
        }
        return: steps
    }

It also has implicit pointer dereferencing (the syntax of which ~~I shameless stole from~~ was inspired by a post by /u/Ansatz66 a few months ago), C and WASM interop and pretty error messages.

    fn foo()
    {
        var data: [4]i32 = [1, 2, 3, 4];
        set_to_zero(&data);
    }
    
    fn set_to_zero(x: &[]i32)
    {
        var i = 0;
        {
            if i == |x|
                goto end;
            x[i] = 0;
            i = i + 1;
            loop;
        }
        end:
    }

It uses LLVM for the backend (specifically clang 6.0 or newer, and *lli* for the interpreter) and is built using Rust. More conventional language features (structs, enums, modules) are yet to be implemented, however I was able to build a very simple game for the WASM-4 fantasy console in a day.

https://github.com/SLiV9/penne",Open sauce development
Why are you building a programming language?,pi84fo,2021-09-05 14:46:41,"Personally, I've always wanted to build a language to learn how it's all done. I've experimented with a bunch of small languages in an effort to learn how lexing, parsing, interpretation and compilation work. I've even built a few DSLs for both functionality and fun. I want to create a full fledged general purpose language but I don't have any real reasons to right now, ie. I don't think I have the solutions to any major issues in the languages I currently use.

What has driven you to create your own language/what problems are you hoping to solve with it?",Because we deserve better programming languages than what we have.
Design decisions I do not regret,kv7efq,2021-01-12 01:30:33,,"> No block comments
>
> These remain a bad idea. Emacs has M-x comment-region and inferior editors likely have something similar. No regrets. In fact, my experience writing more semi-syntactic tooling has only strengthened my conviction. Line comments are easy to parse in an ad-hoc way, and it is particularly nice that there is only one way of writing comments.

Since you mentioned Rust, you may be interested in knowing that at some point the Rust project considered removing block comments as everyone pretty much agreed with you.

One compiler contributor single-handedly turned the tide. He is blind, and therefore uses a screen reader, for him the difference between line and block can be summarized by:

 - Slash Slash Slash foo EndOfLine Slash Slash Slash EndOfLine Slash Slash Slash foo is a method to fooize EndOfLine Slash Slash Slash EndOfLine Slash Slash Slash Pound Panic EndOfLine Slash Slash Slash EndOfLine Slash Slash Slash Panics in case of barredness EndOfLine.
 - Slash Star foo EndOfLine EndOfLine foo is a method to fooize EndOfLine EndOfLine Pound Panic EndOfLine EndOfLine Panics in case of barredness EndOfLine Star Slash.

The demonstration was so brutal that the entire Rust team turned around and acknowledged that while not the recommended style, the cost of maintaining block comments was low enough that it was definitely worth the accessibility benefits for users of screen readers."
Creator of SerenityOS announces new Jakt programming language,utqr52,2022-05-20 17:59:48,,They've done it again... godspeed
Type Theory Forall: Bringing Programming Languages Research outside of the academia,kjhz7u,2020-12-25 00:16:10,,This is excellent. Thanks
"Feeling disappointed with my work - I found out that my language can't handle a certain level of complexity because it's too slow, and now I feel pretty demotivated.",15dx5jq,2023-07-31 05:32:06,,"This is a graphics optimization problem, not a language issue. Drawing 10k triangles one at a time in C is also not going to be fast. You need to put the triangles in a vertex buffer and then draw it with one draw call. The spaceship and the teapot should take exactly the same amount of cpu time to draw."
"""static"" is an ugly word",wwvxcv,2022-08-25 05:53:01,"I hate the fact that ""static"" means so many different things in C and C++.

For variables marked static, they get initialized once at program startup.

For variables outside a function/block/etc, and for functions, static means they are local to the file instead of global.

For class members, static means they are not tied to an instance of the class (but to the class itself).

I'm developing my language and I really would like to avoid using it and instead use something else more meaningful to that part of the language.  Each of these things really means something different and I'd like to represent them separately somehow.  Coming up with the right keyword is difficult though.  For scoping (i.e. case 2), I decided that by default functions/variables are local unless you use a ""pub"" qualifier (meaning public or published or exported).  For initialization at startup, I can't seem to think of anything other than ""once"", or maybe ""atstart"".  For class members, I'll also need to come up with something, although I can't really think of a good one right now.

Thoughts?","Just to be clear, `static` in C always means the same two things:

1. Global lifetime, not created every time the function is entered.
2. Lexical namespace, restricted to the block (or file if outside a block) it's declared in.

It's just that (1) is the default outside a function, and (2) is the default inside a function, so it looks like it's doing different things.

`static` inside a class in C++ follows the same rules: global lifetime, and namespaced to the enclosing class.

You can also use `extern` in C to define a variable as 'global lifetime, global namespace', and that works inside a function the same way it does at the top level. (People used to do this but they stopped in the early 1980s, for good reasons.)

(Just to be clear: everything in C is lexically *scoped*. If you declare a function `extern` inside a function, C forgets about that declaration when it hits the closing `}`. `extern` variables have global *namespace*, meaning all declarations of `extern` variables refer to the same variable, whereas you can have two `static foo` variables in different functions (or different blocks in the same function) and they'll be stored in different locations.)"
Melody - A language that compiles to regular expressions and aims to be more easily readable and maintainable,stpvpn,2022-02-16 15:20:53,,We need better parser combinator libraries
An Introduction to Session Types,lgtqo5,2021-02-10 21:35:25,,"This is really well done! I love the exposition, humor, and the clarity of the explanations"
Copper Language,kmlxfy,2020-12-30 04:13:41,,"This is so classy for a solo project. 
It makes improvements over C in all the right places and it has its own backends. 
I'm really impressed!
This is one language to take seriously.

Also what an absolute madlad to make two entirely complete general purpose programming languages just to make a text editor."
"It's the programming environment, not the programming language",iix2ao,2020-08-30 02:04:09,,"I agree with the conclusion, but for an article about tooling, it uses pretty weak examples to demonstrate a language's legacy. For instance, C's tooling legacy isn't its syntax, but the separate compilation model and serving as a foundation for ABIs in decades to come. While Go's compilation speed is a tooling win, I would argue that its combination of fully static binaries, first-class support for testing and profiling, and easy-to-write static analysis tools are what make the workflow so strong."
Electra: The esolang where you code like an electrician,12omegv,2023-04-17 03:45:24,"Hi everyone! I want to introduce you to an esolang I developed called Electra. In Electra, your code looks like an electrical circuit. It uses list of stacks for memory management. You can find Electra on its [Esolangs Wiki page](https://esolangs.org/wiki/Electra) or on its [GitHub Repository](https://github.com/DolphyWind/Electra-Lang).",[https://en.wikipedia.org/wiki/Electra\_complex](https://en.wikipedia.org/wiki/Electra_complex) 💀
Tomorrow Corporation tech demo: language/compiler/debugger/runtime/editor as an integrated whole,11abzxi,2023-02-24 07:20:04,,"The real demo here is the ""perfect"" time-travel debugging and hot-reloading, that is:

1. Low-overhead
2. Stable (deterministic, doesn't fail or crash)
3. Fast reload/rewind
4. Flexible (doesn't not support things)
5. TTD is unaffected by *modifying the code*, and saves the code state so that when you rewind you see and debug the earlier version
6. Low memory despite managing this
7. Anything else to satisfy ""just works""

And as Jonathan Blow points out, this is **incredibly** hard: I haven't seen any time-traveling debugger that do more than the first 2.5 of what's listed (rr is low-overhead and deterministic, JS hot reload is low-overhead, fast, and usually stable/flexible)

But even if this demo isn't truly what it looks, just what is shown is really impressive. The raw idea of an environment where for every crash, you have a session where you can just step back/forth until you find the cause, fix it, and resume - so that you never have to restart or re-trigger the same scenario - is really impressive."
"Does anyone else question the ""language as tool"" analogy?",zg9gnh,2022-12-09 03:51:47,"Oftentimes, especially if there's a debate between programming language A and B or paradigm X and Y, people will chime in with something like:

  ""Languages are tools. Use whatever is best for the given circumstance.""

  ""All the best developers I know don't really care about languages.""

I actually have a big problem with this analogy. For one thing -- programming languages (besides things like DSLs) are not like hammers or saws. A hammer is pretty much only good at hammering things. A saw is pretty much only good at sawing things. Yet (say) Haskell and Java are both good at pretty much the same things, with maybe some relatively minor differences, and a lot of subjectivity (Maybe person X really likes building web apps in Java, and person Y really likes building them in Ruby, but person X can't stand Ruby, and nice versa).

Maybe there's some things (e.x. some people say Python is really only good for scripting, and statically typed languages are better for larger projects, but there are others who vehemently disagree with these assertions).

Obviously for any given project there will be more or less pragmatic reasons for choosing one language over the other (e.x. Python is the lingua franca and de facto standard for machine learning and data science; on Android you're pretty much limited to Java and Kotlin unless you really want to stray from the beaten path, or if you're using some cross platform framework), but just looking at the languages themselves in isolation, I think they're pretty far from conventional ""tools"", and looking at them that way can be a pretty limiting perspective.

I think a better analogy is that programming languages are like natural languages. You can pretty much express exactly the same things in all of them, even if realistically you have to use loanwords (FFI), or if it's somewhat more clumsy to talk about certain ideas in one or the other. Some languages require you to specify certain information that others may be able to infer from context (type inference). And languages come in families (paradigms), where if you know one language in a family, it's a lot easier to learn the others -- but more of a stretch to learn something completely foreign (e.x. English v.s. Dutch compared to English v.s. Vietnamese. Compare C# v.s. Java to C# v.s. Prolog).

Furthermore, oftentimes the choice of what language to use comes down to culture and what everybody is already familiar with.

It's not a perfect analogy, but I think it's much closer than the ""tool"" analogy. 

Of course there will always be practical aspects, since I think the vast majority of people use programming languages to solve problems (where a minority use them to express or explore concepts), but people use natural language to solve problems as well.

Maybe I'm not being charitable enough, and people who make such arguments don't really think about programming languages as tools in the sense of hammers and saws -- but rather in some more general sense of ""tool"".

However, I feel like this analogy often gets used as a ""thought terminating cliche"". Person A is talking about the merits of language X, and person B is talking about the merits of language Y, and person C, rather than saying something constructive, just says ""It doesn't matter because languages are just tools"".

Has anyone else had similar thoughts?","1. The word ""tool"" is likely used in the more general sense of something you use to accomplish a task. To be generous to people who say ""it's just a tool,"" they likely want to focus the discussion on the pragmatic aspects of a language rather than theoretical properties, aesthetic considerations, and subjective preferences. In particular being willing to use an appropriate language for a project rather than having a pet favorite to use for everything. (No comment on whether that's right or wrong.)

2. Natural languages are also tools in this sense. Incredibly general-purpose tools covering a large percentage of all modern human communication, but still, you use it to accomplish an end. The question of which natural language to use for a specific purpose, or indeed whether to use natural language at all (as opposed to a diagram, or music, or body language, etc.) is a common one.

3. The question of whether a tool can be used for more ""purely creative"" tasks doesn't say much about its ""toolness."" You might have a specific type of paintbrush meant for painting the exterior of a house, and it's not like you can do a palm-sized watercolor with it, but maybe you could paint a mural on the side of a building or something.

4. Re: thought terminating cliche, I think that has less to do with the specific analogy and more to do with the attitude of the person using it. When you have two different tools X and Y, you *should* consider the different merits of each. That might include acknowledgement that X and Y are better suited to different applications. People normally wouldn't write a high-performance game engine from scratch in JavaScript, nor is it particularly pleasant to write a modern web app in C.

5. The question of objective vs subjective is a whole other debate which is independent of the whole tool thing. No matter what property you claim a language has, somebody will contest whether your claim is true (excepting proven formal properties), and even if it is true, somebody will contest whether that property is good/bad."
ChatGPT helped me design a brand new programming language,zeg48j,2022-12-07 03:50:08,,"Inspired by [this post](https://www.reddit.com/r/ProgrammingLanguages/comments/zcti53/building_an_interpreter_for_my_own_programming/), I tried to force ChatGPT to be my programming language design assistant and it went surprisingly well!

Hope you enjoy :) If you do, consider subscribing to my newsletter (bottom of the page)."
"What language features do you ""Consider Harmful"" and why?",ytywy7,2022-11-13 19:24:04,"Obviously I took the concept of Considered Harmful from [this classic paper](https://homepages.cwi.nl/~storm/teaching/reader/Dijkstra68.pdf), but let me formally describe it.

A language feature is Considered Harmful if:

(a) Despite the fact that it works, is well-implemented, has perfectly nice syntax, and makes it easy to do some things that would be hard to do without it ...

(b) It still arguably shouldn't exist: the language would probably be better off without it, because its existence makes it harder to reason about code.

I'll be interested to hear your examples. But off the top of my head, things that people have Considered Harmful include gotos and macros and generics and dynamic data types and multiple dispatch and mutability of variables and Hindley-Milner.

And as some higher-level thoughts ---

(1) We have various slogans like TOOWTDI and YAGNI, but maybe there should be some *precise* antonym to ""Considered Harmful"" ... maybe ""Considered Virtuous""? ... where we mean the *exact* opposite thing --- that a language feature is carefully designed to help us to reason about code, by a language architect who remembered that code is more often read than written.

(2) It is perfectly possible to produce an IT solution in which there are no harmful language features. The Sumerians figured that one out around 4000 BC: the tech is called the ""clay tablet"". It's extraordinarily robust and continues to work for thousands of years ... and all the variables are immutable!

So my point is that many language features, possibly all of them, should be Considered Harmful, and that maybe what a language needs is a ""CH budget"", along the lines of its ""strangeness budget"". Code is intrinsically hard to reason about (that's why they pay me more than the guy who fries the fries, though I work no harder than he does). Every feature of a language adds to its ""CH budget"" a little. It all makes it a little harder to reason about code, because the language is bigger ...

And on that basis, maybe no single feature can be Considered Harmful in itself. Rather, one needs to think about the point where a language goes too far, when the addition of that feature to all the other features tips the balance from easy-to-write to hard-to-read.

Your thoughts?","Harmful: Methods and other Turing Complete behavior in JSON parsers and other data picklers.

It's nifty to be able to marshal up your runnable objects, but there are relatively few real world applications. Meanwhile hackers are using it to sneak executable code through HTTP servers.

Don't assume your data packaging methods carry only data."
"I created my own programming language from scratch, written entirely in Golang, with no idea how to write a programming language. I released v1.0 recently",j6o5gv,2020-10-07 18:00:23,,Hey that look interesting! Can you elaborate on the _math_ token that appears on every operation?
Underappreciated programming language concepts or features?,hm700t,2020-07-06 20:46:50,"Eg: UFCS which allows easy chaining, `it` in single parameter lambdas, null coalescing operator etc.. which are found in very few languages and not known to other people?",Concatenative programming? (eg. `c(b(a()))`)
Write your Own Virtual Machine,deuqx3,2019-10-08 11:51:58,,"This article walks through writing a virtual machine for an LC-3 VM. The simplicity of the language and implementation forms a nice contrast to the one found in Crafting Interpreters, which is the only other tutorial I've looked at that explores implementing a VM. Overall, I found it to be a nice intro-level tutorial."
"Verse programming language: HUGE update to doc: The Verse Calculus: a Core Calculus for Functional Logic Programming (Functional Logic language developed by Epic Games): Confluence proof of rewrite system, Updateable references and more !",11rb6yr,2023-03-15 00:40:47,,That’s a… dizzying amount of colons in that title.
I created a stack machine in Apple Shortcuts :D,1185cxo,2023-02-21 22:58:26,"Shortcuts is not a tool that is designed to implement interpreters with. But I did it anyways. I was bored while I was in a train. So, I came up with the idea.

For a few days, I have been working on a small interpreter that I am creating using Shortcuts. It is basically a [stack machine](https://en.wikipedia.org/wiki/Stack_machine). I named it IOS Stack Machine (ISM), since I started the project in my phone. But it is actually multi-platform, works for Mac, iPad and iPhone.

You write your code in Apple Notes, then run it using the interpreter shortcut.

You go to Notes app, create a new note with .ism at the end of its name (e.g. hello\_world.ism). Then, the note becomes visible to ISM, which means ISM can now run this file.

The language is not a high level language like Python. If that was the case, it would take a lot of time to even parse a given program. The interpreter works with small set of instructions and arguments. I also have another project, which is a higher level language, that compiles into this language, so that you don't have to deal with low level concerns.

You can download it using this link: [https://www.icloud.com/shortcuts/5e1d3ec5e1c6490d886556400ec01b3a](https://www.icloud.com/shortcuts/5e1d3ec5e1c6490d886556400ec01b3a)

Read the documentation from here: [https://github.com/erenyenigul/ios-stack-machine](https://github.com/erenyenigul/ios-stack-machine)

&#x200B;

&#x200B;","This is so cool. Out of curiosity, do you think this could also be integrated with Automator (on macOS) along with Shortcuts?"
Koi: A friendly companion for your shell scripting journeys,mjt5aj,2021-04-04 17:40:28,"Hello and happy Easter!

I've finally completed my language: Koi. It's a language that tries to provide a more familiar syntax for writing shell scripts and Makefile-like files.

I decided to build it out of the frustration I feel whenever I need to write a Bash script or Makefile. I think their syntaxes are just too ancient, difficult to remember and with all sort of quirks.

Koi tries to look like a Python/JavaScript type of language, with the extra ability of spawning subprocesses without a bulky syntax (in fact there's no syntax at all for spawning processes, you just write the command like it was a statement).

Here's a little website that serves the purpose of illustrating Koi's features: [https://koi-lang.dev/](https://koi-lang.dev/). Links to source code and a download are there as well. (Prebuilt binary for Linux only. Actually I have no idea how well it would work on other OSs).

The interpreter is not aimed at real-world use. It's slow as hell, very bugged and the error messages are down right impossible to understand. It's more of a little experiment of mine; a side project that will also serve as my bachelor thesis and nothing more. Please don't expect it to be perfect.

I was curious to hear your thoughts on the syntax and features of the language. Do you think it achieves the objective? Do you like it?

Thank you :)","Wow, I like it. Especially, the syntax to interop with the shell: one can easily distinguish between koi functions `fn(foo)` and shell `fn foo`.

The thing that catches my eye is that native structures can also use whitespace as separators: `[1 2 3]` and `{x:1 y: 1}`. I feel that making a uniform style would help the brain to parse code better: `[echo ""hello""]` could mean both ""two elements: one is variable echo and string hello"" and ""single element with an output of the echo command"". So if you use commas in lists: `[echo, hello, world]` and whitespaces in shell commands: `[echo hello world]`, one could easier parse what's going on.

Also, haven't you looked at functional programming patterns? The Unix pipelining stuff is pretty similar to the way functional languages transform values around. So you can easily write something like this: `docker ps -a |> map(parse_line) |> filter (search_criteria) |> count()`"
Help me identify the language used in a 80's research paper,l4r73m,2021-01-26 00:21:54,,Fortran
Hindley-Milner Type Inference Tutorial,k6c79n,2020-12-04 10:48:16,,"So far this is the most helpful HM Type Inference tutorial I found, each step of the type inference algorithm is explained in a very detailed manner, and code snippets are also provided."
[Preprint] Pika parsing: parsing in reverse solves the left recursion and error recovery problems,gk1uwh,2020-05-15 11:48:49,"I just published a preprint of the following paper: (Update: v2 is now posted)

**Pika parsing: parsing in reverse solves the left recursion and error recovery problems**

[https://arxiv.org/abs/2005.06444](https://arxiv.org/abs/2005.06444)

**Abstract:** A recursive descent parser is built from a set of mutually-recursive functions, where each function directly implements one of the nonterminals of a grammar, such that the structure of recursive calls directly parallels the structure of the grammar. In the worst case, recursive descent parsers take time exponential in the length of the input and the depth of the parse tree. A packrat parser uses memoization to reduce the time complexity for recursive descent parsing to linear. Recursive descent parsers are extremely simple to write, but suffer from two significant problems: (i) left-recursive grammars cause the parser to get stuck in infinite recursion, and (ii) it can be difficult or impossible to optimally recover the parse state and continue parsing after a syntax error. Both problems are solved by the pika parser, a novel reformulation of packrat parsing using dynamic programming to parse the input in reverse: bottom-up and right to left, rather than top-down and left to right. This reversed parsing order enables pika parsers to directly handle left-recursive grammars, simplifying grammar writing, and also enables direct and optimal recovery from syntax errors, which is a crucial property for building IDEs and compilers. Pika parsing maintains the linear-time performance characteristics of packrat parsing, within a moderately small constant factor. Several new insights into precedence, associativity, and left recursion are presented.","Very interesting read. I've spent a lot of time working on PEGs and packrat parsers, so this is a topic I'm pretty familiar with. I have a few questions:

1. Do you have a link to a good resource on recurrence inversion? I'd like to read up on it a bit more, you make it sound like something worth spending some time trying to wrap my head around.

1. I'm a bit skeptical of the usefulness of a ""longest"" operator in a PEG. It seems dangerously close to reviving the performance problems of regexes and the ambiguities of BNF. Can you give any examples where it's clearly more useful than the standard ordered-choice operator?

1. How difficult would it be for pika parsing to handle pattern back references, e.g. a [heredoc](https://en.wikipedia.org/wiki/Here_document) defined in [LPEG Re](http://www.inf.puc-rio.br/~roberto/lpeg/re.html) syntax: `heredoc <- ""<<"" {:delim: [a-zA-Z]+ :} nl (!(nl =delim) .)* nl =delim` I've been working on my own packrat parser, and for me, that has been harder to get working than left recursion (which I implemented using an approach similar to [Warth, et al.](http://www.vpri.org/pdf/tr2007002_packrat.pdf)).

1. How does the pika parser perform with an un-lexable grammar like one that includes string interpolation, especially nested interpolation? (e.g. ruby: `puts ""a #{""b #{""c""} d""} e""`). With forward parsing, it seems like a pretty straightforward linear parse, particularly if you implement the error-catching suggestion I describe below, but I can't imagine how that could be efficiently parsed starting at the end of the string.

1. My reading of the paper was not completely thorough, but I saw that it uses topological sorting of the grammar rules. How does that work with corecursive grammars (e.g. `XYs <- ""x""+ YXs / """"; YXs <- ""y""+ XYs / """"`)?

And a few not-question comments:

1. People often criticize PEGs/packrat parsers for poor error reporting (mentioned a few times in the OP), but in my experience, you can get very good results by treating errors as a *first-class citizen* of your grammar, rather than something orthogonal to the grammar. As an example, suppose you have a grammar with strings that can't span multiple lines. Instead of writing a rule like `string <- '""' [^\n""]* '""'`, you would write a rule like `string <- '""' [^\n""]* ('""' / missing_quote_error)` (where `missing_quote_error` is a zero-width terminal). Using the first version of the grammar, `x = ""hello\ny = 5` will fail to parse, giving you some cryptic error message, if any. However, the grammar that *expects* errors to occur will successfully identify that there is a missing quotation mark and return an AST with a `string` node with a child `missing_quote_error` node, then continue parsing along on the next line. You can define these error-catching rules with varying granularity, like `file <- statement* (!. / (.+ unparsed_code_error))`. It worked pretty well for my language (you can [check out its PEG here](https://bitbucket.org/spilt/nomsu/raw/86c477343d8ae84fc99014a2cc4f844e9c688b9c/nomsu.7.peg) to see a nontrivial example).

2. The lack of left recursion in many packrat parsers has not been much of a practical limitation in my experience. The simplest workaround (which I used in my language) is to just rewrite the rule in a `prefix suffix+` form and then perform a simple AST transformation afterwards. For example, instead of `index <- expr ""."" ident; expr <- index / ...`, you just write it as `index <- (noindex_expr (""."" ident)+ -> foldr); expr <- index / noindex_expr`.

3. If you haven't seen it already, Guido van Rossum has a whole [series of blog posts on PEGs](https://medium.com/@gvanrossum_83706/peg-parsing-series-de5d41b2ed60), well worth checking out. I was in the middle of writing a huge blog post of my own about PEGs when I found it, and it totally derailed me, haha."
Announcing the Frost programming language,ehqlay,2019-12-31 03:19:39,,"Cool name.  In the `head` example, does the program have to read the whole file into memory or is it done in a lazy/streaming fashion?"
Hey I made a new programming language called Yaksha,11vnm65,2023-03-19 23:01:56,"[eps](https://news.ycombinator.com/user?id=eps) on Hackernews told me to post here! First time posting here. !

So here goes:

I  have been working on this for a while. Main goal was to build a usable  programming language. I even end up building few tools for this such as  IntelliJ plugin, etc.

I also plan on building some games with it in future.

Main  use case would be: small games (raylib), tools (static linux binaries  with musl-libc) and recreational programming (wasm4). Works in Windows  also. If you have emscripten in path you can even build these  games/tools (raylib) to WASM.

Website: [yakshalang.github.io/](https://yakshalang.github.io/)

Main Repo: [https://github.com/YakshaLang/Yaksha](https://github.com/YakshaLang/Yaksha)

Doc: [https://yakshalang.github.io/documentation.html](https://yakshalang.github.io/documentation.html)

Library: [https://yakshalang.github.io/library-docs.html](https://yakshalang.github.io/library-docs.html)

Tutorials: [https://github.com/orgs/YakshaLang/discussions/categories/tu...](https://github.com/orgs/YakshaLang/discussions/categories/tutorials)

Let me know what you think. 😀","Hmm, you have to do manual memory management by calling a `del` operator, but it's comparatively difficult to see where objects would be allocated – Does Yaksha have stack-allocated objects that don't have to be deleted? Is there a `new` operator to signify that `del` must be used later?"
"What are some important differences between the popular versions of OOP (e.g. Java, Python) vs. the purist's versions of OOP (e.g. Smalltalk)?",12ev4cm,2023-04-08 02:46:56,"This is a common point that is brought up whenever someone criticizes the modern iterations of OOP. Having only tried the modern versions, I'm curious to know what some of the differences might be.","""Pure"" OOP like Smalltalk is radically different than any modern language and a completely unique programming paradigm. You will have to read https://en.wikipedia.org/wiki/Smalltalk for the full details.

Not only is every piece of data an object, every element of control flow is a method. If statements, loops, return statements, etc. are all methods: instead of `if a then b else c` you have `a ifTrue:[ b ] ifFalse:[ c ]` (brackets denote a ""code block"" AKA closure). And there are no global functions, everything is a method including operators (think `a gcdOfThisAnd: b` instead of `gcd(a, b)`). The entire language is only literals, code blocks, variable declarations, and methods, *that's it*.

Smalltalk is also *extremely* dynamic and supports reflection in pretty much every way imaginable. Not only can you access, inspect, and override every single method (including the control-flow methods and primitive operators described above), your program can modify the IDE and runtime and even itself. Because, the IDE is itself a Smalltalk program and your code is an ""image"" which is loaded and continuously runs in the Smalltalk ""environment"", *the same environment the IDE is running in* (along with any other Smalltalk programs), which is very similar to a virtual machine.

Of course, there are many huge flaws with this approach, including security, performance, enabling very bad design and spaghetti code, etc. But it's also very cool, and some of the things it enables are very useful. Like, the seamless integration of your program and IDE means that you can really customize the IDE to support writing your program, e.g. adding custom UI, not to mention it makes code actions and debugging easier to implement. And the ability to inspect and override everything including primitive structures and private members of other classes enables tracing which isn't really possible in a traditional language.

AFAIK the major SmallTalk distributions are https://squeak.org/ and https://pharo.org/. I've heard that Pharo is more complex and ""practical"", while Squeak is more educational and beginner-friendly. But both stick to their roots with ""everything is an object or method"", extreme reflection, and integrated runtime/IDE."
Syntax Design,yn0ux1,2022-11-06 01:43:13,,Awesome post. Lovely read
Favorite PL paper?,x6hqky,2022-09-05 22:36:38,"What is your favorite PL paper? I'm looking to diversify the set of literature I've read and decided this is a good way to do it. Perhaps you can do the same!

I'll start. My favorite paper at the moment is [Codata in Action](https://www.microsoft.com/en-us/research/uploads/prod/2020/01/CoDataInAction.pdf).","Knuth's [Structured Programming with go to Statements](https://pic.plover.com/knuth-GOTO.pdf).

It's a fascinating look into the history of language development. There's a section about experimental control flow constructs and it's interesting seeing what caught on and what didn't."
2021 in Programming Languages,s1e4op,2022-01-11 22:27:57,,"Woohoo shell is at #12 in the first chart based on Github commits, right behind Rust :)

But there's no discussion of shell in the video apparently ... It's sort of in a weird place where people use it, but some don't consider it a language, and also there is very little work on it.  There are no equivalents of ECMAScript committees or big companies sponsoring it like Swift/Go/Dart/C#, or even small companies like Scala."
"“Programming Languages” Series on Coursera is IMO, one of the best classes on foundational programming language paradigms. I strongly recommend it. You’ll be writing your own mini interpreter in Racket. Here is a full course review.",lvlwb5,2021-03-02 06:02:22,,"I took a modified (easier) version of this course at UW, which cut Standard ML and expanded the Racket/Ruby content. It was fantastic for an undergrad course, and Grossman is a great lecturer/course designer."
Worst language you ever used? Really used not just looked at the manual.,wl7qbs,2022-08-11 04:22:00,I'll start:  APL by far.,"CMake, in a landslide"
Which programming language has the best tooling?,vi2e9d,2022-06-22 18:51:06,"People who have used several programming languages, according to you which languages have superior tooling? 


Tools can be linters, formatters, debugger, package management, docs, batteries included standard library or anything that improves developer experience apart from syntactic sugar and ide. Extra points if the tools are officially supported by language maintainers like mozilla, google or Microsoft etc. 


After doing some research, I guess golang and rust are one of the best in this regard. I think cargo and go get is better than npm. go and rust have formatting tools like gofmt and rustfmt while js has prettier extension. I guess this is an advantage of modern languages because go and rust are newer.",Rust. It's not even a competition. It's like NodeJS but without all the junk. And I'd put the Python ecosystem to the rock bottom. Please don't make me use it.
Programming Language Checklist,q72xzn,2021-10-13 11:53:47,,"I'll bite, and fill it for my own language.

```
Programming Language Checklist
by Colin McMillen, Jason Reed, and Elly Fong-Jones, 2011-10-10.
You appear to be advocating a new:
[X] functional  [X] imperative  [ ] object-oriented  [ ] procedural [ ] stack-based
[ ] ""multi-paradigm""  [ ] lazy  [ ] eager  [X] statically-typed  [ ] dynamically-typed
[X] pure  [X] impure  [ ] non-hygienic  [ ] visual  [X] beginner-friendly
[ ] non-programmer-friendly  [ ] completely incomprehensible
programming language.  Your language will not work.  Here is why it will not work.

You appear to believe that:
[X] Syntax is what makes programming difficult
[ ] Garbage collection is free                [ ] Computers have infinite memory
[ ] Nobody really needs:
    [ ] concurrency  [ ] a REPL  [X] debugger support  [ ] IDE support  [ ] I/O
    [ ] to interact with code not written in your language
[ ] The entire world speaks 7-bit ASCII
[ ] Scaling up to large software projects will be easy
[ ] Convincing programmers to adopt a new language will be easy
[ ] Convincing programmers to adopt a language-specific IDE will be easy
[X] Programmers love writing lots of boilerplate
[ ] Specifying behaviors as ""undefined"" means that programmers won't rely on them
[ ] ""Spooky action at a distance"" makes programming more fun

Unfortunately, your language (has):
[?] comprehensible syntax  [ ] semicolons  [HAS] significant whitespace  [ ] macros
[ ] implicit type conversion  [ ] explicit casting  [HAS] type inference
[ ] goto  [ ] exceptions  [HAS] closures  [ ] tail recursion  [ ] coroutines
[ ] reflection  [ ] subtyping  [ ] multiple inheritance  [ ] operator overloading
[HAS] algebraic datatypes  [HAS] recursive types  [HAS] polymorphic types
[ ] covariant array typing  [HAS] monads  [ ] dependent types
[HAS] infix operators  [HAS] nested comments  [HAS] multi-line strings  [ ] regexes
[ ] call-by-value  [ ] call-by-name  [ ] call-by-reference  [ ] call-cc

The following philosophical objections apply:
[X] Programmers should not need to understand category theory to write ""Hello, World!""
[X] Programmers should not develop RSI from writing ""Hello, World!""
[X] The most significant program written in your language is its own compiler
[ ] The most significant program written in your language isn't even its own compiler
[X] No language spec
[ ] ""The implementation is the spec""
   [ ] The implementation is closed-source  [ ] covered by patents  [ ] not owned by you
[PROBABLY?] Your type system is unsound  [ ] Your language cannot be unambiguously parsed
   [ ] a proof of same is attached
   [ ] invoking this proof crashes the compiler
[ ] The name of your language makes it impossible to find on Google
[ ] Interpreted languages will never be as fast as C
[X] Compiled languages will never be ""extensible""
[ ] Writing a compiler that understands English is AI-complete
[X] Your language relies on an optimization which has never been shown possible
[ ] There are less than 100 programmers on Earth smart enough to use your language
[ ] ____________________________ takes exponential time
[ ] ____________________________ is known to be undecidable

Your implementation has the following flaws:
[ ] CPUs do not work that way
[ ] RAM does not work that way
[ ] VMs do not work that way
[X] Compilers do not work that way
[X] Compilers cannot work that way
[ ] Shift-reduce conflicts in parsing seem to be resolved using rand()
[ ] You require the compiler to be present at runtime
[ ] You require the language runtime to be present at compile-time
[X] Your compiler errors are completely inscrutable
[ ] Dangerous behavior is only a warning
[ ] The compiler crashes if you look at it funny
[ ] The VM crashes if you look at it funny
[X] You don't seem to understand basic optimization techniques
[X] You don't seem to understand basic systems programming
[ ] You don't seem to understand pointers
[ ] You don't seem to understand functions

Additionally, your marketing has the following problems:
[ ] Unsupported claims of increased productivity
[ ] Unsupported claims of greater ""ease of use""
[ ] Obviously rigged benchmarks
   [ ] Graphics, simulation, or crypto benchmarks where your code just calls
       handwritten assembly through your FFI
   [ ] String-processing benchmarks where you just call PCRE
   [ ] Matrix-math benchmarks where you just call BLAS
[ ] Noone really believes that your language is faster than:
    [ ] assembly  [ ] C  [ ] FORTRAN  [ ] Java  [ ] Ruby  [ ] Prolog
[ ] Rejection of orthodox programming-language theory without justification
[ ] Rejection of orthodox systems programming without justification
[ ] Rejection of orthodox algorithmic theory without justification
[ ] Rejection of basic computer science without justification

Taking the wider ecosystem into account, I would like to note that:
[ ] Your complex sample code would be one line in: _______________________
[ ] We already have an unsafe imperative language
[X] We already have a safe imperative OO language
[X] We already have a safe statically-typed eager functional language
[ ] You have reinvented Lisp but worse
[ ] You have reinvented Javascript but worse
[ ] You have reinvented Java but worse
[ ] You have reinvented C++ but worse
[ ] You have reinvented PHP but worse
[ ] You have reinvented PHP better, but that's still no justification
[ ] You have reinvented Brainfuck but non-ironically

In conclusion, this is what I think of you:
[X] You have some interesting ideas, but this won't fly.
[X] This is a bad language, and you should feel bad for inventing it.
[X] Programming in this language is an adequate punishment for inventing it.
```"
Metalang99: A functional language for C99 preprocessor metaprogramming,lswnya,2021-02-26 20:23:05,,My mind is officially blown. I've done some preprocessor metaprogramming in C before but I never guessed you could take it that far.
Closures · Crafting Interpreters,da1ro9,2019-09-27 23:09:48,,"I love reading this book.  Started it a while ago and I think it's a very nice complement to ""Programming Language Pragmatics."""
How would you remake the web?,v9u9uf,2022-06-11 17:51:51,"I often see people online criticizing the web and the technologies it's built on, such as CSS/HTML/JS. 

Now obviously complaining is easy and solving problems is hard, so I've been wondering about what a 'remade' web might look like. What languages might it use and what would the browser APIs look like?

So my question is, if you could start completely from scratch, what would your dream web look like? Or if that question is too big, then what problems would you solve that you think the current web has and how?

I'm interested to see if anyone has any interesting points.","I've worked on a couple ideas for this before! I actually worked on a lang that does this for my undergrad project. Please pm me if you're thinking of working on something similar, i'd love to help :) [Un]fortunately I had a baby during my last year so my uni code & paper is quite unfinished, but I'd be happy to share & walk you through it in pvt.

Modern web dev currently has 2 options for development - the old 'jquery style' approach, and the modern 'react style' approach. If you're familiar with [immediate mode guis](https://en.wikipedia.org/wiki/Immediate_mode_GUI), then the modern approach is similar to immediate mode, and the jquery approach is similar to retained mode.

If you've tried both of these methods, then you know that modern frameworks are *significantly* easier to use for complex apps than the old approach. I'm assuming you agree here, I won't go into this.

The core idea is to embrace this modern development system & enforce it, rather than try and produce a super generic 'browser' that just downloads random code from the internet & executes it whenever. IMO this is a terrible model, but it's the one I most often see being proposed as a 'solution'.

The problem is that these frameworks are basically implementing an immediate-mode interface *on top* of a retained-mode interface (the DOM api). This introduces a shitload of overhead, and weird edge cases where the API inevitably leaks. You also have to do awkward state-change tracking at runtime (e.g. if some global variable updates, then how do you know which components to re-render?) which results in terrible libraries like Redux/Vuex for React/Vue respectively. Some frameworks (mithril.js) choose a simpler approach, but you need to manually redraw everything in certain cases, nothing is perfect here.

My proposal would be a browser which natively interacts with this immediate-mode style of UI. In my undergrad project, I proposed that this would remove the need for a scripting language almost entirely for 99% of web applications. Pages would likely run MUCH faster, and you could have your (possibly insecure) scripting language be an 'opt-in' thing for users when browsing. Currently even pages like Wikipedia won't work the same without javascript, because they need very simple functionality to update the page dynamically. No XSS, yes please :)

Styling would be done inline - no need for a separate styling document. Originally separate CSS was proposed to allow users to add their own custom styling to webpages. This is obviously completely obselete, inline styling is much easier to understand & doesn't result in any code duplication with components. IIUC In modern browsers a lot of time is spent matching up CSS rules to HTML elements. when firefox quantum came out, the main performance gain was [parallelising CSS rule matching](https://hacks.mozilla.org/2017/08/inside-a-super-fast-css-engine-quantum-css-aka-stylo/)

You could create a browser for this lang, but also compile the immediate-mode lang into a html/css/js webapp so developers could use it and deploy to both platforms. Initially the lang would gain traction as a dev framework, & then hopefully worm its way into companies just like other open-source js frameworks did."
`toki pona` is a constructed human language with 140 words: Introducing`toki sona` a toki pona inspired programming language with 14 tokens and a 1000 character interpreter,pph5c3,2021-09-17 00:52:02,,This makes me wonder: can one go the other way and generate a human language from lambda calculus? Is there perhaps a generic Sapir-Curry-Wharf-Howard Isomorphism? :)
My experience crafting an interpreter with Rust,opldyz,2021-07-23 03:40:26,,This blog post is excellent! I'm glad you enjoyed the book. :)
Why are product types so common while sum types are so rare?,minw5w,2021-04-03 00:31:02,"Why do modern languages like Kotlin, Swift, Rust, F# and Scala feature some form of sum types while older languages like C++, Java, C# and Objective-C lack them?

Is it because OOP was the primary paradigm at the time, where typical uses of a sum type are handled instead by subtype polymorphism and virtual dispatch?

I always find it strange when a language makes it easy to say ""this and that"" but difficult to say ""this *or* that"".","Interesting question. I don't think the answer has to do with object-orientation actually. I think it all derives from languages like C, where the attitude to ""type"" is giving interpretation to regions of memory.

Let's say you have two types, P and Q. You know how to represent each one as given number of bytes. Representing the product of P and Q is easy: you just concatenate their representations. Representing the sum is harder: you need to create some kind of tag, followed by space as big as the largest one to switch between them. That's too complex for C to do directly, though it does give you `union` types so you can do it yourself."
"Refinement Types: A Tutorial. ""[W]e show how to implement a refinement type checker via a progression of languages that incrementally add features to the language or type system."" [abstract + link to PDF, 119pp]",jc4gwe,2020-10-16 14:01:24,,"Full abstract:

> Refinement types enrich a language's type system with logical predicates that circumscribe the set of values described by the type, thereby providing software developers a tunable knob with which to inform the type system about what invariants and correctness properties should be checked on their code. In this article, we distill the ideas developed in the substantial literature on refinement types into a unified tutorial that explains the key ingredients of modern refinement type systems. In particular, we show how to implement a refinement type checker via a progression of languages that incrementally add features to the language or type system."
A programming language to make concurrent programs easy to write,iglqtm,2020-08-26 05:40:09,"A friend and I created a programming language that looks like Typescript and makes distributed programs shorter and easier to reason about. Alan's compiler and runtime exploits opportunities for parallelization across the computing resources available without being told to do so.

[https://alan-lang.org/](https://alan-lang.org/)","Do you have a brief description about how Alan is different (besides syntax) from Rust, Pony and Erlang?"
Does the JVM / CLR even make sense nowadays?,12z66qi,2023-04-26 12:21:17,"Given that most Java / .Net applications are now deployed as backend applications, does it even make sense to have a VM (i.e. the JVM / .Net) application any more? 

Java was first conceived as ""the language of the Internet"", and the vision was that your ""applet"" or whatever should be able to run in a multitude of browsers and on completely different hardware. For this use case a byte code compiler and a VM made perfect sense. Today, however, the same byte code is usually only ever deployed to a single platform, i.e. the hardware and the operating system is known in advance.

For this new use case a VM doesn't seem to make much sense, other than being able to use the byte code as a kind of intermediate representation. (However, you could just use LLVM nowadays — I guess  this is kind of the point of GraalVM as well) However, maybe I'm missing something? Are there other benefits to using a VM except portability?","There is an argument that VMs can actually outperform precompiled in some cases. This is possible because it can do things where it optimizes for the code path actually always taken, something the compiler can't ever know. That said, I largely don't think it is needed anymore either.

For my own language, I am designing an IL for use as a package distribution mechanism. I think it makes a lot of sense to have a stable IL for package distribution and an intermediate stage to optimize. In addition, my language allows extensive compile-time code execution, and I can run a simple interpreter over the IL. I think this makes a lot more sense than needing to distribute all packages as source code and therefore needing to support the perpetual compilation of every edition of the language in all compilers. However, actual apps will be natively compiled."
Snekky Programming Language,ohvtpw,2021-07-11 10:10:42,"Snekky is a project I've been working on off and on for the past year. It is a dynamically typed scripting language that compiles to its own bytecode which is then executed by a small virtual machine.

**Disclaimer: I'm not trying to develop the next big programming language here, rather it's a small learning project of mine that I'd like to get some feedback on because this is my first attempt at writing a bytecode language.**

Website: [https://snekky-lang.org](https://snekky-lang.org)

GitHub Repository: [https://github.com/snekkylang/snekky](https://github.com/snekkylang/snekky)

Snekky has all the basic features you would expect from a language, meaning the usual control structures (if, while, ...), arrays, hash maps, functions and for-loops that work with iterators.

Some of its more notable features:

* Pretty much everything is an object.
* Almost all control structures are expressions themselves.
* Closures can be used to implement data structures.
* Array/hash destructuring is supported.
* Functions are first-class citizens.
* A REPL with syntax highlighting and automatic indentation.

Most of the examples can be evaluated directly on the website. More complex projects that I have implemented in Snekky so far are:

* a Discord bot ([Source](https://github.com/snekkylang/snekky/tree/master/examples/discord_bot))
* a simple webserver ([Source](https://github.com/snekkylang/snekky/tree/master/examples/webserver))
* an even simpler programming language ([Source](https://github.com/snekkylang/snekky/tree/master/examples/snekkyscript))

Additionally I have written a decompiler ([CLI](https://github.com/snekkylang/snekkyd) / [GUI frontend](https://github.com/snekkylang/snekkyd-gui)), which can be used to translate Snekky bytecode files back into executable source code.

Any feedback would be greatly appreciated. :)","I like the practical examples, nice work"
A new (but old) way to express complex numbers,lv4to9,2021-03-01 17:25:34,"Sometimes in programming we need to do calculations that involve complex numbers. Most implementations I know of attempt to represent complex numbers by tuples of the form `(re, im)`, where `re` and `im` are floats. Given how float numbers are represented internally, this form is essentially: `((-1)^s * m * 2^e) + ((-1)^s' * m' * 2^e') i`.

I claim that there is a more natural and reasonable way to represent complex numbers, that we just hadn't given enough thought about. Let's stare at the representation of floats again: `(-1)^s * m * 2^e`. Here `s` is either 0 or 1, `m` is a decimal \[usually\] between 1 and 2, and `e` is an integer. We will rethink about the meaning of complex numbers, and try to fit that into this mindset.

What is `i` anyway? It's defined as `sqrt(-1)`, or `(-1)^0.5`; on the other hand, `-i` can be expressed as `1/i`, or `(-1)^-0.5`. Now looking back at the float representation, have you realized something? Yes: the concept of `i` can be perfectly represented in our old float-style as `(-1)^0.5 * 1 * 2^0`. We only gave up the restriction that `s` must be integer.

By loosening the condition of `s` to be any decimal number in `(-1, 1]`, we:

(a) can express any number expressible in the original `a+bi` method

(b) can express some extra things, such as infinities at every angle. In most programming languages, we can do `Infinity*(-1)*2*(-2)*(-1.3) = -Infinity`, so in the complex world, we would also expect to do `Infinity*(1+i)*(1+i)*(1+i)*(1+i) = -Infinity`. Speaking of this, the rectangular form `a+bi` fails miserably as it inherently discriminates against non-orthogonal complex arguments: it cannot distinguish `Infinity*(1+2i)` and `Infinity*(2+i)` but *can* distinguish `Infinity*1` and `Infinity*(-1)`. (Depending on your background, you may know what I'm talking about.) Hence it's great that the new form can store the intermediate infinities as `(-1)^0.25 * Infinity`. Similarly, the new form will represent `+0, -0, 0i, 0*(1+i), ...` all differently as desired

(c) can no more express things like `3 + NaN*i` .... which is actually good because what would that mean

(d) come very close to the polar form representation `(r, phi)` (our `r` is essentially the `m * 2^e` part and `phi` would be `s * pi`). This means many performance boosts compared to the rectangular form, such as making [`abs`](https://en.wikipedia.org/wiki/Absolute_value#Complex_numbers), [`sign`](https://en.wikipedia.org/wiki/Sign_function#Complex_signum), multiplication, reciprocal, division, `sqrt` and `pow` much simpler, although it does make addition&subtraction on non-linearly-dependent numbers more complicated.

(e) are still better than the primitive polar form, because we got rid of the `pi` that often appears in the `phi` term. For example, calculating `pow(sqrt(i), 100000)` under polar form would cause noticeable error, but we won't have that problem because the argument is represented exactly.

&#x200B;

I'm thinking about the following extension of the floating-point standard.

A 128-bit string is interpreted as follows:

    sssssssssssss eeeeeeeeeee mmmmmmmmmmmmmmm
      (43 bits)    (21 bits)     (64 bits)

When `e` is neither all-zero nor all-one, the number represented is `(-1)^(s/2^42) * 1.m * 2^(e-(2^20-1))`, (here `s` is the signed int under 2's complement, and `e` and `m` are unsigned).

When `e` is all-zero, the number represented is `(-1)^(s/2^42) * +0.m * 2^-(2^20-2)`.

When `e` is all-one,

\-- when `m` is all-zero, the number represented is `(-1)^(s/2^42) * Infinity`.

\-- when `m` is all-one, the number represented is Complex Infinity (the result of `(-2)^Infinity`, which btw shouldn't be NaN because we expect abs(that) = Infinity) if the first bit of `s` is 1, and complex zero (`(-2)^-Infinity`) otherwise.  (There are some valid points against this though)

\-- else, the number represented is NaN.

I liked this allocation of precision because 64 is the suggested next-step for mantissa precision, the 21-bit exponent makes the maximal norm 2\^1048576, and the 43-bit sign is precise enough to divide the complex plane but light enough that a real-only use case will waste 42 bits instead of 64.

&#x200B;

Representations of common numbers (omitting some bits to make it look shorter):

    0:       0000000000000 00000000000 000000000000000
    -0:      1000000000000 00000000000 000000000000000
    1:       0000000000000 10000000000 000000000000000 // note that real numbers are expressed exactly like their floating-point representations except with zeros padded after the sign bit
    i:       0100000000000 10000000000 000000000000000
    sqrt(i): 0010000000000 10000000000 000000000000000 // note how this is better than both the rectangular form and the primitive polar form, where either sqrt(2)/2 or pi/4 needs to be used; also note how easy it is to check whether a number is a unit
    0*(1-i): 1110000000000 00000000000 000000000000000
    Inf:     0000000000000 11111111111 000000000000000
    Inf(1+i):0010000000000 11111111111 000000000000000
    CompInf: 1xxxxxxxxxxxx 11111111111 111111111111111 // CompInf != CompInf
    CompZero:0xxxxxxxxxxxx 11111111111 111111111111111 // CompZero == CompZero == -0 == 0*i
    NaN:     xxxxxxxxxxxxx 11111111111 xxxNotAllSamexx

I think it would be really cool to turn this into a 128-bit datatype standard (or at least built-in somewhere). Let's *Divide the Sign.*","That's pretty nice.

One reason the current two-floats representation is so popular, though, is that it uses existing floating-point hardware in CPUs to do the real floating point number operations.

Your representation would be awesome implemented in hardware, but for now it'll have to be implemented in software - and how does it stack performance-wise compared to normal real&imag representations?

As well as programming languages, I enjoy noodling with CPU designs, though - I'm saving a link to this and if I design an FPU one day I'll see if I can make it implement this. Native complex-number operations would be pretty nifty :-D"
I made (again) web interactive programming language relations network visualization,kwknq3,2021-01-14 01:02:41,,"Looks awesome!

Extremely hard to use on mobile though"
My programming language can now run in a browser.,iehbmj,2020-08-22 20:08:09,"Using WebAssembly, I have managed to get my programming language, called AEC, to run in browsers (at least very modern ones).

The first AEC program I ported to WebAssembly is my program that prints the permutations of the digits of a number: https://flatassembler.github.io/permutationsTest.html

Later, I ported my Analog Clock to WebAssembly: https://flatassembler.github.io/analogClock.html

Recently, I made a graphical program in AEC (which I have never done before) by interacting with SVG: https://flatassembler.github.io/dragonCurve.html

So, what do you think about my work?

I've rewritten my compiler completely, the previous version of my compiler (targeting x86) was written in JavaScript, while this version is written in C++. Many people say C++ is a better language than JavaScript. Honestly, I think that newest versions are comparable. I've also changed the syntax of my language a bit and added a few new features (which are a lot easier to implement when targeting WebAssembly than when targeting x86).","on the comment of ""C++ is better than JavaScript"", that's practically a running joke at this point, tho there are people who seriously say that, they mostly reference old versions of JS and how it was made in 10 days

all I can say, you can make anything in 10 days, and then improve it, the initiative duration may set a small run, but at this point there are 3 major JS implementations V8 (Chromium), SpiderMonkey (Mozilla), JavaScriptCore (WebKit [apple]), those weren't written in 10 days, and ECMA isnt the company that first specified JS, they also specify C# FYI. Just because C++'s first specification wasn't released after 10 days, but rather nearly a year or something, it doens't make it that much better. It's lower level making it more powerful, JS on the other hand runs on nearly all platforms the same. 

It's all a give and take. At this point you can do anything with every language if you know the language well enough. JS can be used extremely type safe, and C++ can be used as if types didn't exist. (I know both languages, and have seen the worst of both worlds)"
What is your favourite academic paper on programming languages?,dwt8xu,2019-11-16 00:36:57,"**TL;DR: Title. Reasoning for post below if you're interested. Otherwise treat as a discussion post.** 

Not sure if this is appropiate for the sub so willing to remove. 

In my next term of university I'm taking a module on programming language theory. As part of its assessment I'm expected to give a presentation evaluating a programming language of choice and discussing some academic papers relating to said language. I wanted to spend my holidays delving into programming language theory and reading over potential papers to pick for my next term. 

Wanted ask users of this subreddit if they had any favourite papers. I figure since you guys are already PLT enthusiasts you might already know some good papers I could look at for consideration.","The paper I learned the most from was probably *The Implementation of Lua 5.0* (10 years ago).  Although it's not really a theory paper.

A theoretical paper which is practically important is *Partial Evaluation of Computation Process: An Approach to a Compiler-Compiler* (1971) which introduces Futamura projections.

https://scholar.google.com/scholar?cluster=4740701776297759930&hl=en&as_sdt=0,5

PyPy and I believe Graal use this idea.  And the Souffle datalog compiler.

You may find some other worthwhile papers by following the citations in this paper:

https://arxiv.org/pdf/1611.09906.pdf"
C3 is a C-like language trying to be an incremental improvement over C rather than a whole new language.,oohij6,2021-07-21 11:08:28,,"Commendable. But it doesn't solve C's main problem: hundreds of undefined behaviors, dubious numerical coercions, ambiguous syntax, inability to define mutually recursive structs (and bad struct syntax in general)."
I much prefer `data.action()` to `action(data). Is it an r/unpopularopinion?,mvbaet,2021-04-21 16:33:19,"I'm not talking about functional vs object-oriented, just the style of combining data with actions, and how it impacts the programmer's experience.

The intellisense (aka code completion) is much better.

More often than not you have the data and you want to perform some action, so you write `data|` (`|` being your cursor), then you can put some sort of separator like a `.` press Ctrl+Space and see all the actions that operate on this data.

Compare this to having write the action beforehand. The intellisense experience is much worse. You need to somehow resort to searching the documentation for every function that takes as an argument the data you are interested in.

There is also the pipe operator: `data |> action` (and similar), but by my^TM definition that's just replacing the `.` separator with ` |> `. I'm not sure if editors support code completion for pipe operators cause it's less popular. And it's also not always possible to just write such code easily if your language doesn't have currying.

I think these days your language is judged by the ease of use, so intellisense is naturally a big part of that. 

For context I'm coming from JS-land, Python type of languages, and am talking specifically about Julia and Red (although I wanted to be as general as possible).

What are your thoughts? Do you get used to it?","You'll probably like Smalltalk. :)

I very much prefer pipeline thinking style over the mathematical f(x) notation, exactly because I always start with some piece of data I want to transform into something else. Pseudocode pipeline:

```(""hello world"") → (split "" "") → (map upperCase) → (join ""-"") → (print)```

(Brackets are just for visual separation.)

And I just don't want to name the intermediate values because I don't care about them at all. Piping to the grave, my friend.

Edit: That piece of code is not Smalltalk. Sorry for the confusion. I thought everybody here knew [Smalltalk](https://learnxinyminutes.com/docs/smalltalk/). :)"
"Making PL Ideas Accessible: An Open-Source, Open-Access, Interactive Journal",mqpsdc,2021-04-14 20:44:50,,No interest at all?
The Pervert's Guide to Computer Programming Languages - SXSW [2017],g47mdt,2020-04-19 21:25:27,,"I was really impressed with this dude's handling of Lacan - maybe some oversimplifications here and there but this is easily the most structured presentation of psychoanalytic symptomatology that I've ever seen. Also, the language diagnostics had me cracking up.

That being said, for all this knowledge, it is really funny that he is so badly mispronouncing Zizek."
"Crumb: A Programming Language with No Keywords, and a Whole Lot of Functions",1621mpb,2023-08-27 01:46:52,"TLDR: Here's the repo - [https://github.com/liam-ilan/crumb](https://github.com/liam-ilan/crumb) :D

Hi all!

I started learning C this summer, and figured that the best way to learn would be to implement my own garbage-collected, dynamically typed, functional programming language in C ;D

The language utilizes a super terse syntax definition... The whole EBNF can be described in 6 lines,

    program = start, statement, end;
    statement = {return | assignment | value};
    return = ""<-"", value;
    assignment = identifier, ""="", value;
    value = application | function | int | float | string | identifier;
    application = ""("", {value}, "")"";
    function = ""{"", [{identifier}, ""->""], statement, ""}"";

Here is some Crumb code that prints the Fibonacci sequence:

    // use a simple recursive function to calculate the nth fibonacci number
    fibonacci = {n ->
      <- (if (is n 0) {<- 0} {
        <- (if (is n 1) {<- 1} {
          <- (add 
            (fibonacci (subtract n 1)) 
            (fibonacci (subtract n 2))
          )
        })
      })
    }
    
    (until ""stop"" {state n ->
      (print (add n 1) ""-"" (fibonacci (add n 1)) ""\n"")
    })

 I got the game of life working as well!

[The game of life, written in Crumb](https://reddit.com/link/1621mpb/video/n5xgojpkrhkb1/player)

Here's the repo: [https://github.com/liam-ilan/crumb](https://github.com/liam-ilan/crumb)... This is my first time building an interpreter 😅, so any feedback would be greatly appreciated! If you build anything cool with it, send it to the comments, it would be awesome to see what can be done with Crumb :D","Nice. Very lispy.

If you make functions/blocks implicit return the last expression you can get rid of the `<-`"
Stop Saying C/C++,13lta2u,2023-05-19 19:53:28,,Nitpick: you wrote ASCI C instead of ANSI C
A list of new budding programming languages and their interesting features?,thcqko,2022-03-19 03:14:20,"Looking at [Wikipedia](https://en.wikipedia.org/wiki/List_of_programming_languages_by_type) or Google to find ""cutting edge"" new sprouting programming languages is a lost cause, 100% of what you find is dated by at least 5-10 years. Most lists of ""interesting languages"" are of super popular languages like C, Rust, Haskell, etc..

Are there any people gathering new programming languages anywhere, perhaps in this Reddit group somewhere? I looked around but couldn't find anything.

Basically would like to learn from all the great work being done on programming languages and would like to see some fresh perspectives given the latest work people are doing. People occasionally reference this or that new language, thereby introducing me to it, but it is rare. If no list exists, what are some of the more interesting or intriguing languages out there these days?

To start, some of the ones I've encountered which I find inspiring are:

- [Lobster](https://github.com/aardappel/lobster): With flow-based type analysis and minimal typing.
- [Kind](https://github.com/Kindelia/Kind): A modern proof language (though functional).
- [Dafny](https://github.com/dafny-lang/dafny): A modern imperative proof language.

But perhaps there are ideas you are generating on your own project which isn't even as well established (yet) as these few programming languages. If nothing else, share an interesting feature of a new programming language, so it becomes centralized if there is not already a list.

In particular, I am looking for inspiration / ideas on things like memory management, garbage collection, type inference, type checking, automated theorem proving and formal verification, symbolic evaluation, implementing native types, particular optimizations, interesting / different ideas like borrow checking and ownership, etc.","Wikipedia deliberately does not list languages until they gain traction.

The page for Nim was delete a few years ago."
Const generics and compile time code,mkjz1r,2021-04-05 20:52:32,,"I love Zig's generic system where you just add compile-time function parameters and return types or functions. The only problem is with compile-time argument deduction, since with its call syntax you can't easily skip the generic arguments, compared to C++ or Rust where you just omit the <> part."
Columbia's Alfred Aho and Stanford's Jeffrey Ullman receive 2020 ACM A.M. Turing Award,mh6c9u,2021-03-31 21:40:33,,"While people like bashing it online, my PDF of the Dragon Book has been proving itself indispensable ever since I began getting into languages and compilers.

It is admittedly worse than something like Writing {a Compiler, an Interpreter} in Go or Crafting Interpreters for practical experience and for getting begginers onboard, but when the time comes to add an optimization pass to a procedural language, or to write a LALR parser generator, as examples, there is no other place I would go."
1990: Programming languages - “the Need To Know Guide to Programming Languages” #cartoons,ideqmg,2020-08-21 01:30:51,,C and assembly are still kicking it. Thank God we dont have to deal with basic anymore. Ughhh
ZZ is a modern formally provable dialect of C,ez9k1g,2020-02-05 21:28:36,,"Some questions regarding formally proving things, it is very unclear what the assumptions here are:

Is there a proof that translation of {} → {} is sound:
 - ZZ → SSA?
 - SSA → SMT clauses?
 - SSA → (sub-)C

Since these SMT ""provers"" are not really well-studied, does ZZ verify their proofs by using a small well-studied kernel?"
The Programming Language Design and Implementation Stack Exchange site has entered private beta!,13jvof8,2023-05-17 16:25:57,"It can be found at https://languagedesign.stackexchange.com/.

See also:

- [Its tour page](https://languagedesign.stackexchange.com/tour)
- [What does ""beta"" mean?](https://languagedesign.stackexchange.com/help/whats-beta)
- [How to Ask Questions in Private Beta](https://languagedesign.stackexchange.com/help/how-to-ask-beta)
- [Its meta site](https://languagedesign.meta.stackexchange.com/)
  - [How can you help Programming Language Design and Implementation make it from private to public beta?](https://languagedesign.meta.stackexchange.com/q/4/251)
- [Its ""On-Topic"" Help Center page](https://languagedesign.stackexchange.com/help/on-topic) (still pending customization)

If you want to learn more about Stack Exchange in general, see also [the FAQ for Stack Exchange sites](https://meta.stackexchange.com/q/7931/997587).

It might be nice to see this in the ""Related online communities"" sidebar, considering that other Stack Exchange related links are there.

Related past posts:

- [A proposed Stack Exchange site for programming language development is close to entering beta!](https://www.reddit.com/r/ProgrammingLanguages/comments/12ke1o8/a_proposed_stack_exchange_site_for_programming/)
- [Yesterday, I posted here about a StackExchange site proposal for Programming Language Design. It's moved into the Commitment Phase of the proposal process and needs your help to become a proper site!](https://www.reddit.com/r/ProgrammingLanguages/comments/10v554r/yesterday_i_posted_here_about_a_stackexchange/)
- [Are you interested in designing and building programing languages? We're trying to build a community about that on stack exchange. However, we need more follows and questions to make that happen.](https://www.reddit.com/r/ProgrammingLanguages/comments/yykkef/are_you_interested_in_designing_and_building/)","„here is my opinion about algebraic effects“ - marked as duplicate to „how to algebra“

I just hope that this community will avoid typical SE toxicity. This subreddit is really great as it is, dunno why SE was needed"
"Scallop: a new neurosymbolic programming language, solver, and framework based on Datalog",u5ti39,2022-04-18 02:50:47,,"I was just telling somebody at work that I was looking forward to the day symbolic/logic programming based AI research makes a comeback.

Looks like that time might be coming soon!"
"Oxide, scripting language with Rust-influenced syntax",mflcry,2021-03-29 17:05:32,"[https://github.com/tuqqu/oxide-lang](https://github.com/tuqqu/oxide-lang)

Oxide is a programming language I have been working on on my free time for fun.

This is my first attempt to write a programming language and I did enjoy doing it. The syntax is Rust influenced on surface, but feels like a more traditional C-like language.

I wrote a simple documentation as well as example programs to give an overview of the language.

There are still things WIP, but mostly it is stable with the features it has right now.",Why did you go for C-style for loops instead of iterator-based ones?
Just: A language like Make except not a build system,ljdj30,2021-02-14 08:49:02,"I wrote a command runner, and although it's not quite a programming language, I thought people here might be interested in it.

Just lets you save and run commands from files with a terse, readable syntax similar to Make:

    build:
        cc *.c -o main
    
    # test everything
    test-all: build
        ./test --all
    
    # run a specific test
    test TEST: build
        ./test --test {{TEST}}

Using Make's syntax is definitely a double edged sword. It's familiar, fast to write, and easy to read once you get used to it. However, since recipes and variables are introduced with arbitrary identifiers, adding new keywords is impossible. Also, having recipes be delimited with indentation and contain near arbitrary text complicates the lexer enormously.

There are some features that I'd like to add long term, like modules, a richer type system, and an integrated shell. I'd also like to add function literals, so that we can finally answer the question, ""What if Make had lambdas?""

It is cross-platform, written in Rust, and actively maintained on GitHub:

https://github.com/casey/just/

Just has a bunch of nice features:

- Can be invoked from any subdirectory
- Arguments can be passed from the command line
- Static error checking that catches syntax errors and typos
- Excellent error messages with source context
- The ability to list recipes from the command line
- Recipes can be written in any language
- Works on Linux, macOS, and Windows
- And much more!

Just doesn't replace Make, or any other build system, but it does replace reverse-searching your command history, telling colleagues the weird flags they need to pass to do the thing, and forgetting how to run old projects.",I love just and actually use it in one of my projects! Thank you for your great work!! :D
A GPU|CPU hosted compiler written in 17 lines of APL.,k258ez,2020-11-28 01:17:13,,"The source code for the compiler is on page 210 of the thesis.

I have met the author.  He has a tee-shirt with his entire compiler printed on it."
Does the programming language design community have a bias in favor of functional programming?,uh0sez,2022-05-03 05:55:27,"I am wondering if this is the case -- or if it is a reflection of my own bias, since I was introduced to language design through functional languages, and that tends to be the material I read.","OOP is mainstream, so you won't see as many OOP advocates in r/ProgrammingLanguages where we focus on cutting-edge programming ideas. To many here, OOP is a case of ""been there, done that"". If you look for talks about *the next big shift in programming languages* most talks cover Functional Programming, Category Theory, and innovations in compiled languages (like up-and-comers Rust, Zig, etc.). This is also a community for programming subcultures and alternative paradigms that don't get the light of day. If I had a guess, I'd say this sub has *heavy* overlap with r/haskell (especially given its academic nature).

I'm personally an advocate for combining *Array Programming* principles with *Logic Programming* (to conduct 2nd order logic seamlessly), but I rarely hear either of those things discussed in this sub, despite their expressivity."
Fearless concurrency at a discount? • PL Papers You Might Love,ug1s18,2022-05-01 23:04:21,,Nice summary!
Dr. Dobb's Journal,sjwcwg,2022-02-04 06:28:24,"I'm too young to have seen this when it came out, but I heard about it from someone on r/learnprogramming. A high-level amateur journal trying to bring computing power to the masses, starting in 1976. ([Full repository on the Internet Archive](https://archive.org/details/dr_dobbs_journal).) A reminder of when small computers were really really small.

In the [first](https://archive.org/details/dr_dobbs_journal_vol_01/page/n8/mode/1up) [two](https://archive.org/details/dr_dobbs_journal_vol_01/page/n36/mode/1up) issues they explain how to make a BASIC interpreter, using bytecode for portability. (In the second issue [the guy who wrote the interpreter explains why BASIC is a bad language](https://archive.org/details/dr_dobbs_journal_vol_01/page/n27/mode/1up).)

By Vol. 5 they have a [teensy C compiler](https://archive.org/details/dr_dobbs_journal_vol_05_201803/page/n189/mode/1up?view=theater), plus [an article by Dennis Ritchie et al explaining what C is](https://archive.org/details/dr_dobbs_journal_vol_05_201803/page/n204/mode/1up?view=theater).

There was a time when Forth looked like it might be a contender. [""An ideal language for microcomputers""](https://archive.org/details/dr_dobbs_journal_vol_03/page/n216/mode/1up) ... sure, it's just not all that suitable for ordinary human beings, when cephalopods develop computers maybe all their languages will be concatenative.

I'm still delving into the archive but it's full of interesting stuff.","Sigh. I miss Dr Dobb's.

They were hard to get hold of where I lived...

I remember my excitement in finding a Big Fat one at an airport."
"Creating a fake programming language to weed out bad recruiters, and resume padders",m2rl14,2021-03-11 22:39:34,,Why does this article sound like it was written by AI?
POPL21 (Principles of Programming Languages) videos are popling up in YouTube,kzxyl3,2021-01-19 00:28:22,,Heh
Video: Derivative of a Regex?! (Brzozowski Derivative),khec43,2020-12-21 17:55:22,,"~~BTW I think this is explaining the Antimirov partial derivative?  That is, the derivative with respect to a character.  I think the Brzozowski derivative is actually different, although I didn't go through it.~~ (edit: this is wrong; Antimirov derivatives are related to NFAs, follow-up here: http://www.oilshell.org/blog/2020/12/regex-videos.html)

https://semantic-domain.blogspot.com/2013/11/antimirov-derivatives-for-regular.html

> So for the most part, derivatives have remained a minor piece of functional programming folklore: they are cute, but a good implementation of derivatives is not really simple enough to displace the usual Dragon Book automata constructions.

> However, in 1995 Valentin Antimirov introduced the notion of a partial derivative of a regular expression

Darius points out that Thompson used Antimirov derivatives in his original code in the 60's, though it cites Brzozowski's paper:

https://news.ycombinator.com/item?id=18434225

http://www.oilshell.org/blog/2020/07/ideas-questions.html#regular-expression-derivatives-papers-and-code

So I guess they are closely related enough that Thompson could just come up with it on his own ...

Ad: Someone should either implement submatches with derivatives or find an existing implementation of this :)  http://www.home.hs-karlsruhe.de/~suma0002/publications/ppdp12-part-deriv-sub-match.pdf

from https://news.ycombinator.com/item?id=25491467"
I made a language a year ago. It is just 750 lines of python and implements a Pratt Parser.,innotd,2020-09-06 23:15:12,"[Niko Lang](https://github.com/FurkanTheHuman/niko)

I made this as a school project and really enjoyed making it. I think if you want to learn language design, pratt parsers are really good for starters (not only starters of course. It is pretty fast).

&#x200B;

The language implements a python like language with indentation. It is nothing like a complete language of course but it can do %80-70  of what [lox language](https://craftinginterpreters.com/)  can do and simpler.

&#x200B;

If you want to learn pratt parsers, last night I added some comment lines for readability. Parser code can still be confusing. For that reason read the classic [Simple Top-Down Parsing in Python](http://effbot.org/zone/simple-top-down-parsing.htm) which I use it while implementing it.

I am open to criticism, especially on the interpreting the tree.","Very neat!

The best Pratt parsing article that I’ve found is this one, written by u/matklad: https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html

While it’s in Rust, it’s still pretty applicable to Python. Here’s my implementation of that alrogithm for Lox, in Python: https://github.com/al2me6/pylox/blob/master/pylox/parsing/parser.py"
Calls and Functions · Crafting Interpreters,chjqgk,2019-07-25 13:56:14,,Author here! I'd be delighted to answer any questions or take any feedback you have. :)
"What does it mean to have an ""algebraic"" type system?",10ewz92,2023-01-18 11:41:45,"Or alternatively, what does it mean to **not** have an algebraic type system?

&#x200B;

I sometimes see comments on Reddit or elsewhere that are something to the effect of ""language X has an algebraic type system"", and then go on to talk about sum types or something like that. But that's not quite right, is it? An algebraic type system would/should include sum types, product types, and exponential types, right?

What kinds of things would qualify or disqualify a language as having an ""algebraic"" type system?","Every language has product types, so also having sum types and generics lets you express arbitrary polynomial functors. I think that’s a pretty good interpretation of “algebraic”. But the term is not precise and exponentials are great to have as well of course!"
The Val Programming Language,xk3sum,2022-09-21 20:40:04,,"I could've sworn this was shared before here, but I'm unable to find any post so here goes :)"
The Golden Age of Programming Languages Research,xekv5s,2022-09-15 10:39:02,,"I think this is sort of true for every area in computer science, since it’s so easy to build off of old work.


With that being said I think as far as getting the fruits of PL research to developers has never been harder. Darwinian languages (those which are good enough to proliferate widely and reproduce) like Python, C++, JavaScript have such a stranglehold in deployed code that it’s becoming very difficult to challenge them. 

Although, things like WebAssembly and LLVM mean that new languages can plug into existing ecosystems — as long as PL designers explicitly target integration as a design goal."
Type theory breakthroughs,ib0cgu,2020-08-17 05:01:42,What would you say are some breakthrough papers in type theory over the last decade or so?,"Probably the most significant paper of the past decade in type theory would be [Breaking Through the Normalization Barrier: A Self-Interpreter for F-omega](http://compilers.cs.ucla.edu/popl16/popl16-full.pdf) published in 2016.  Why?  It says that we can build interpreters and compilers for strongly-normalizing languages in strongly-normalizing languages, dispelling the previously-held notion that you couldn't.  This means that we can have an entire programming ecosystem that lives within the class of strongly-normalizing languages, and we don't have to touch Turing-completeness at all.  Why is _that_ significant?  Because static analysis in a Turing-complete language is, in general, undecidable.  Static analysis in a strongly normalizing language, on the other hand, is decidable, and potentially within the realm of tractable.  The importance of this difference simply cannot be understated.  The technical thing that is holding us back more than anything on anything to do with programming languages and compilers and the practice of programming is our ability to perform static analysis."
"Why Does ""="" Mean Assignment? • Hillel Wayne",fg0d1b,2020-03-10 03:35:44,,"In logic programming languages like Prolog, ""="" means **equality**.

Equality is a **relation**. Declaratively, =(A, B) is true *if and only if* A is equal to B. Using infix notation, =(A, B) can be written equivalently as A = B.

Operationally, it means to *unify* the arguments. Thus, it subsumes assignment, matching and testing (i.e., *whether* the arguments, if fully known, are equal)."
Methods and Initializers · Crafting Interpreters,f6twl0,2020-02-20 22:34:14,,Just the side note at the end of this chapter is worth buying the whole book. Must read for everyone in this sub designing a PL.
Resilient LL Parsing Tutorial,13nwcds,2023-05-21 22:23:06,,"Good Reading, about Parsers, Lexers and Compiler Tool Design, but I need more time to compile it on my mind ..."
Programming language design - course materials,11le4wn,2023-03-08 06:48:05,,Very cool seeing the article I wrote about null safety in Dart used here. :)
"This talk by Nicholas Matsakis is the best explanation of the ""compiler as a service"" architecture that I've found so far",hl04eq,2020-07-04 16:39:24,,"I watched the talk this very week :)

For me, beyond compiler as a service, I really -- really -- digged the idea of goal-oriented compiler architecture.

I think a big weakness in statically typed languages is not so much the language, itself, but their compiler.

It's frequently said that statically typed languages are just not as good for fast iteration (edit-compile-test) as their more dynamically typed counterparts. There are 2 changes to compilers that can improve the state of things:

 1. Deferred semantic errors. Don't signal the errors at compile-time, instead embed them as a ""panic"" in the generated code. Execution will only fail if it attempts to pass through.
 2. Goal oriented compilation. In Rust, you type `cargo test xxx::yyy::zzz` and cargo instructs rustc to... compile everything and then execute only this subset of test. From a feedback latency perspective that's terrible! Imagine, instead, a world where the compiler only compiles the code that is necessary for the subset of tests matched and nothing else. _Much faster!_

Now, combine the two, and you get fast compilation of _only_ the code needed for the subset of tests you need _without_ any pesky errors from code paths this subset of tests doesn't execute.

I imagine the speed-up of edit-compile-test cycle could be rather significant."
PSA: The .org domain has been purchased by a private equity company.,e0edp7,2019-11-23 14:49:30,"Found this on twitter: [https://twitter.com/ossia/status/1197950543706112001](https://twitter.com/ossia/status/1197950543706112001)

This may be of interest to you if you have a .org for your programming language (surely I'm not the only one).",There is a movement to stop the sale: https://savedotorg.org
Little Languages Are The Future Of Programming,z11esg,2022-11-21 23:15:04,,"The issue I have with little languages is poor tooling, made even worse with composition of languages. Language tooling is a large investment, requiring a high resolution parser, a language server, linter, etc. It also leads to serious benefits in developer experience. The hard core emacs users who consider the extent of language support to be syntax highlighting may disagree but the bar is much much higher now.

Furthermore composition with other languages is still an unsolved area for tooling. We can’t do type checking across languages and we can’t share type systems. Which in turn means refactoring and linting across languages is not feasible. 

These are not impossible problems to solve but they’re definitely important if little languages are to gain wide adoption."
Simon Peyton Jones did a Talk on the new Verse Language today!,zm4lbv,2022-12-15 06:42:14,"It gives prolog/ logical programming vibes, with a haskell flair. it would be nice to try something like this out today! thoughts? 

[Verse Language](https://www.youtube.com/watch?v=832JF1o7Ck8&t=3298s) Talk","Simon's slides always have such a distinctive aesthetic, I find it comforting"
"Rust is hard, or: The misery of mainstream programming",v3clru,2022-06-03 00:20:13,,"So, I wrote a response: https://www.reddit.com/r/rust/comments/v44tp2/async\_rust\_doesnt\_have\_to\_be\_hard/"
LLVM for Functional Languages: Supporting continuations via custom calling conventions,ojt2ld,2021-07-14 08:43:59,"LLVM is a great backend for imperative code, however functional languages can have difficulty using it as a backend due to requirements such as proper tailcall optimization and call/cc.

### LLVM as an FP backend

The Manticore SML compiler successfully utilizes LLVM by implementing a custom calling convention, `jwa` (""Jump with Arguments""). By marking all functions as `naked` (no prologue/epilogue code emitted) and `jwa` (no registers are ever saved across function calls), LLVM emits code that doesn't ever really use the stack. This allows for their CPS intermediate language to map directly to LLVM instructions; remember that function calls in CPS are just ""jumps with arguments"". You can read more about the details [here](https://arxiv.org/pdf/1805.08842.pdf).

However, that approach has a few downsides. The language must be compiled to CPS, which can lose a lot of information about the call stack and prevents stack allocation of variables. Manticore's runtime uses heap-allocated continuation closures, so this isn't a problem, but if you want to move towards a zero-cost continuation implementation you have to think about a different solution.

### Other Continuation representations

The ChezScheme compiler uses a segmented stack approach for implementing continuations described [here](https://legacy.cs.indiana.edu/~dyb/pubs/stack.pdf). This method maintains stack-allocated variables, splitting the stack whenever a continuation is captured and maintaining the complete stack as a linked list of stack segments. This is a zero-cost implementation since non-continuation calls are normal stack operations and capturing a continuation is O(1).

LLVM has direct support for a feature known as segmented stacks. A function marked with the ""split-stack"" attribute checks the remaining stack space in the prologue; if there isn't enough room for its variables, it uses a libgcc routine `__morestack` to allocate more room, call the function itself, then deallocate. The details are described [here](https://llvm.org/docs/SegmentedStacks.html). The main point is that LLVM can generate prologue code that checks the stack size and knows the current function's frame size.

### Question

As you can probably see where this is going, I wanted to know any possible pitfalls with using a custom calling convention for this kind of segmented stack implementation. A functional language can be compiled to an intermediate language such as ANF/SSA (something that maintains the stack), then lowered directly into LLVM with the custom calling convention. The main questions I have about this are:

1. The ChezScheme calling convention does not use a stack pointer, rather it uses a frame pointer. I'm not sure if this causes issues or if LLVM can be customized to deal with this difference.

2. The calling convention also requires the insertion of a literal word in the instruction stream following `call`s marking the size of the current frame, with the return jumping past that literal. I can write an inline assembly shim, but I would rather let LLVM do this with a calling convention if possible.

3. Is there anything that I'm missing about this idea? It seems to me to be a great backend possibility for languages like Scheme and SML but maybe I'm just too excited.","You may also be interested in [Thorin (PDF)](https://compilers.cs.uni-saarland.de/papers/lkh15_cgo.pdf), which converts everything it can from CPS to normal, stack-enabled SSA, and places where continuation manipulation can't be specialized or inlined away it emits CPS-style LLVM IR like Manticore. Using `tailcc` for CPS calls works essentially just as well as Manticore's custom calling convention, as well.

I would argue that maintaining a segmented stack isn't zero-cost: it does make stack allocation more expensive even in code that doesn't use continuations. Also, capturing the continuation is only O(1) if nothing on the stack is changed (so it doesn't have to be copied), which means no mutable stack-allocated variables but also that you can't overwrite a stack frame for a new call if it's captured. Also, you need to garbage-collect captured stack segments, which will probably be very difficult to do in LLVM. I think you could probably get this to work, but it would take a lot of work and I'm not sure it's the best option."
"Has there ever been a new feature added to a language long after 1.0, which was later removed because of unforeseen problems?",mudz94,2021-04-20 07:56:39,"I've often wondered this. A great deal of responsibility is on the shoulders of language designers, with users generally always clamoring for more functionality (even minimalists appreciate quality-of-life improvements).

But what if you didn't think it through sufficiently? What if a novel feature seems like a great idea at first, until some months/years later when somebody hits an edge case which reveals that it was fundamentally a mistake?

How much deliberation and testing goes into new features for mature languages before being blessed with an official release? Has this ever been insufficient and resulted in a feature being rescinded afterward?",C++: auto_ptr
PLTea (in 30 minutes),l765af,2021-01-29 03:29:36,,the page says the event was jan 28 2020 but i imagine that’s meant to say 2021. anyway i wish i saw this earlier!
Kalaam - A Programming Language in Hindi,hantit,2020-06-17 17:19:11,"https://youtu.be/bB-N8YxMEaI


Kalaam was created as a part of an educational project to help my students under the age of 18 to understand programming through a different dimension. 

As the development of Kalaam continues, expect advanced features and major bug fixes in the next version.

Anyone with a smartphone or a computer can start coding in Kalaam.

Check out the language here: Kalaam.io

To stay updated with the project, share your ideas and suggestions, join Kalaam discord server:
https://discord.com/invite/EMyA8TA","Why do the example programs have so much English in them? I don't know Hindi at all, so in this case it helps me know what's going on in them. I'm just surprised at all the English."
GitHub - marcpaq/b1fipl: A Bestiary of Single-File Implementations of Programming Languages,f30wxi,2020-02-13 08:10:42,,That's cool
Are myths about the power of LISP exaggerated?,158iyza,2023-07-25 02:37:40,"I have read dozens of articles and posts praising LISP and how it gives you supernatural abilities. Yet, to my shame, I have never seriously programmed in it.

From what I understand, it boils down to just 2 things:

1. s-expressions are very easy to parse.
2. There is a special quote operator that turns an expression into a corresponding AST node, and this makes metaprogramming very lightweight, compared to manipulating node streams in other languages with good macro systems.

Is that it, or am I missing something? Many people claim that languages of the LISP family make you incredibly productive. But I rarely find macros to be the primary reason for a programmer's productivity: they are nice to have, sometimes they help you avoid a lot of boilerplate, but ultimately they are less important for success of a product built in the language than a good type system or ability to separate code into composable modules.

People often throw around the term ""homoiconicity"", but I do not really understand its importance: the only benefit I see is that writing macros involves slightly less mental overhead, since you can just write `'(fun a b)` instead of `makeCall(makeIdentifier(""fun""), [makeIdentifier(""a""), makeIdentifier(""b"")])`. But in other languages we don't write macros *that* often.

The examples I've seen also looked dubious to me: for example, I've seen someone define a setter using a macro, something like `(mySet (myGet id) newValue)`. But surely you wouldn't want every library to define setters in such an arbitrary way?

Are myths around LISP slightly exaggerated, or am a missing important points that make this family of languages as good as some people claim? Is the significance of LISP nowadays mostly historical?

For context, I am mentally comparing LISP with other languages I have the most experience with: TypeScript, Rust, Haskell, Python, C#.

I also wonder if the answer to my question is different between the most common dialects: Common Lisp, Scheme, Clojure.","I put Lisp in the same category as Pink Floyd and ""Zen and the Art of Motorcyle Maintenance"". It's not that it is *so totally mindblowingly profound*. It's more that it's *pretty* deep and happens to often hit people before anything else has blown their mind in that way.

If you stumble onto Lisp in your 30s after a decade of interest in programming languages where you've absorbed most of its ideas piecemeal, it will not blow your socks off. But if you're 18 and thought you were hot shit because you wrote a 1,000 line Pascal program, your first shot of Lisp could be very mind-expanding."
How Big Should a Programming Language Be?,11zo078,2023-03-23 23:35:07,,"For those who haven't seen it, [Guy Steele did an excellent talk on this topic](https://www.youtube.com/watch?v=_ahvzDzKdB0)."
What's your opinion on ChatGPT related posts?,11ta0eh,2023-03-17 07:27:59,"In recent weeks we've noticed an uptick in undesirable ChatGPT related posts.
Some of these are people asking questions about why ChatGPT spits out garbage
when presented with a question vaguely related to the subreddit. Others are
people claiming to've ""designed"" a ""language"" using ChatGPT, when all it did was
spit out some random syntax, without anything to actually run it.

The two common elements are that you can't really learn anything from such
posts, and that in many instances the ChatGPT output doesn't actually do
anything.

Historically we've simply removed such posts, if AutoModerator hadn't already
done so for other reasons (e.g. the user is clearly a spammer). Recently though
we've been getting some moderator mail about such posts, suggesting it may be
time to clear things up in the sidebar/rules.

Which brings us to the following: we'd like to get a better understanding of the
subreddit's opinion on banning ChatGPT content, before we make a final decision.
The end goal is to prevent the subreddit from turning into a stream of
low-effort ""Look at what ChatGPT did!"" posts, and to further reduce manual work
done by moderators (such as manually removing such posts).

So if you have any comments/thoughts/etc, please share them in the comments :)","So far I haven’t found a single one of them useful.

I’m not at the point where I’d advocate for a blanket ban yet though. It’s still new enough that someone could post something relevant and actually interesting/clever."
What makes a language fast for a programmer to write?,10wesrf,2023-02-08 06:11:28,"I have been musing on this for a while, but to narrow the question ""What makes a programming language fast iterate with. So far, I have come up with (in order)  


1. Good docs for learning the ""correct"" way to solve problems. When I say correct, I mean the language's canonical solution. I also prefer if the solutions do a good job showing how the code can scale to more complex problems.
2. Good libraries, without them everyone is doing everything over and over all the time. I think that highly opinionated libraries are preferred, especially if that opinion is supported by the language. In short: the lest I have to code, the faster I can go from idea to program.
3. Editor tooling, I noticed this most from moving from C# to Rust. JetBrains Rider is so well integrated with the language that it can very frequently anticipate what I need to write and it has very good error messages. Clion (JetBrain's competitor for rust) is...fine, but I find it has generally worse auto-complete and error messages.
4. Garbage collection, I know that GC's are somewhat unpopular but I really appreciate the ability for me to just worry about my problem and how to represent the data, and let the computer figure out where the memory should go.
5. Tools to reduce code repetition, this may be a personal opinion but being able to reduce the number of places I have to make changes when my code doesn't do
6. Debugger/introspection, being able to stop the program and look at exactly what the state of the program is very helpful...good tools to single step is also super helpful.

I did not include typing in this list, because I honestly can go either way. I think that as long as the types support the canonical solutions in the docs, they don't slow me down. I also find that dynamic typing leads to a lot of ""a has no field b"" errors which frequently slow me down.  


I would also like to note the **big** one. No matter the language or the tooling, the fastest language to write is the one the programmer knows. I would like to compare language features not programmer familiarity.  


What about you all? What makes a language quick for you to write? Are their features that make it quick to write programs and iterate on them?","Consistency's also nice - I often need to look at docs to find out what I need, but sometimes I can anticipate what I'll need based on what I know of the language, reducing the time delays that causes, and consistency makes success at that more likely."
A simple Glide program that reads and transforms CSV data,ygcnz3,2022-10-29 15:00:23,,"A few updates on Glide:

* Lambdas do not need a “ret” statement anymore. All functions essentially return the top of the stack if it exists, otherwise empty. Therefore implicit returns are now a thing.
* I’m still married to the “->” syntax, even though it is harder to write than “>>”, and I’m not entirely sure I want to change it
* As per the video, Glide now has basic I/O (read, write, append)
* A bunch of new built-in functions now exist, some are: time, to\_int, to\_string, to\_float, type etc.
* The csv module is also just a work in progress, and there is no way currently to serialise back to csv format after transformation, but that’s just more library work rather than compiler work

For anyone interested, the documentation exists here: [https://github.com/dibsonthis/Glide-Lang/](https://github.com/dibsonthis/Glide-Lang/)"
What is it like to write a large project in a dynamically-typed language?,r6nq30,2021-12-02 04:03:36,"Every project I have ever worked on with greater than 1000 LOC has been in a statically-typed language. For my personal projects I used to use C++, but now mostly use Rust. At my day job, we use primarily C# and VB.NET.

The type systems in these languages have been invaluable when refactoring code. If I decide some data needs to be a different type, the compiler will immediately show me all the places where code needs to change to account for this.

I worked on a project that was a few hundred lines of Lua, and while I think it is probably one of the best-designed dynamic languages, even small refactors were painful when all the errors were only caught at runtime.

This is, of course, part of the whole reason people love static typing so much, but I have to wonder, what is it like when you write huge codebases in Javascript/Python/Lua/Lisp/etc? It just seems to me like it would be really awful. How do you change any code with any confidence? Just lots of tests?

I ask because dynamic languages, in my experience, are so much easier to design and implement than statically-typed ones. But if you really want your language to be generally useful for non-trivial projects, can dynamic typing ever be the best option?","The largest dynamically-typed codebase I have worked on thus far is GitLab,
which consists of about 700 000 lines of code, of which about 420 000 lines are
written in Ruby (and 150 000 lines of Javascript, oh dear god).

After working on it for little over six years, I firmly believe a statically
typed language would have saved us _a ton_ of effort and more importantly bugs.
We've had a lot of bugs over the years along the lines of ""Under this weird
condition we call non-existing method X on type Y"". Some might say ""Write better
tests!"", but that's easier said than done when you have _so many_ moving parts
involved. Looking back, I feel a statically-typed language would have caught _a
lot_ of bugs before they even made their way to production.

Setting aside bugs, navigating a big Ruby codebase is a challenge. I can
generally navigate around the parts of the code that I know, but outside of that
it becomes difficult. Is the type of this `project` variable a `Project`, or a
`Foo::Bar::Project`, or a `Bla::Bla::CodeProject`? Often the only way to find
out is to run the code (e.g. using the tests), and hope that whatever you
observe holds true in production.

The dynamic nature of Ruby also poses a serious challenge for tools such as
language servers and linters. So much that basically none of them are suitable
for a project of this size. Maybe if you're Stripe and swimming in money you can
dedicate dozens of people to improve this, but for most this just isn't
possible.

Another factor that complicates matters is the number of people working on the
project. If you have only a few developers, you can probably manage large
dynamically-typed projects. But when you have literally hundreds of developers
working on it, things can quickly get out of hand.

In terms of what language I would have written GitLab instead, I don't know.
We've had people suggest various languages over the years (e.g. Java and Haskell
for some reason), but they all come with their own problems. For example, if
GitLab were written in Java I'm sure we'd have 10x the code, and it wouldn't
necessarily be better (factories and beans everywhere). If it was written in
Haskell, we'd be able to employ exactly two developers that keep talking about
monads.

As much as I dislike Go, it's probably the language I would pick if I were to
write GitLab today; at least when it finally gets generics. As much as I would
like to pick Rust, I feel it just isn't quite there yet for web development, and
the compile times would likely be through the roof. I'm also not a fan of the
async APIs in Rust, whereas with Go this is baked into the language (as it
should be in my opinion)."
A tree-sitter based AST difftool to get meaningful semantic diffs,omy0so,2021-07-19 03:56:30,,Neat!
Break into the compiler industry with an internship at Apple,jurvvu,2020-11-16 03:37:39,,Wow looks like an amazing opportunity!
Everything Old is New Again: Binary Security of WebAssembly,icb9ve,2020-08-19 06:29:13,,"Just skimmed the paper.  My summary:

WebAssembly provides a basic guarantee that the sandboxed code can't write directly to the host memory.

However it can admittedly make ""a mess of its own memory"" (the WebAssembly stack and heap).

This means that C code with memory safety bugs has all those bugs preserved, and even more, because there's no such thing as read-only virtual memory pages in WebAssembly, for example.

The first example given relies on a bufferflow in libpng.  If you compile it to WebAssembly, you still have a buffer overflow.  If you put some constant HTML tags in the same WebAssembly binary, and then assume that you read the result back out, and `document.write()` it, then processing untrusted data with libpng leads to an end-to-end exploit.

Basically nothing in the VM can be trusted after any user-supplied data is processed with C code you suspect may have vulnerabilities.

That's a very simple exploit, and pretty bad IMO.  And then they give quantitative data showing that's these attack scenarios are realistic (I didn't read this part closely).

-----

It's weird to me that nobody brought this up before!  This seems like a big design flaw.  Buffer overflows are still buffer overflows.  I thought that separating code from data in wasm would do something, but apparently not enough (e.g. WebAssembly is a Harvard architecture, not von Neumann)

This relates to anyone on this subreddit designing a language that targets WebAssembly. If your language allows memory unsafety (and most efficient languages do, I think Zig and Go do), then users of your WebAssembly target need to be aware of this.

IMO this is a caveat greatly limits the utility of wasm, since the whole point was that you could compile existing C code and use it in the browser.  And I believe those have been the overwhelming number of use cases.  Their original demos were running Doom in the browser, and stuff like that.  Not rewriting Doom in Rust, because that would take forever."
Is there a programming language that will blow my mind?,13xw8jg,2023-06-02 07:03:04,"I've been a C++ developer for 20 years. I spent many years honing my craft under the scrutinizing look of C++ chat on StackOverflow. I can assure you these people were not friendly if my code sucked. I was obsessed with C++ templates for a long time, but now I've toned it down a lot (because they negatively affect compilation time, readability and cause disproportional complexity).

I've kind of settled on the idea that structured programming, recursion, and object oriented programming were the major breakthroughs in programming. I know OOP is controversial, but it manages to find the nice middle ground between abstraction power, usefulness as a modeling tool and a reasonable learning curve.

I've dabbled in Haskell 15 years ago, from the YAHT tutorial (anyone remember that?). I've also played a bit with Clojure but didn't go deep into macros, which is probably where the exciting stuff begins? I like the influence that functional programming has on modern mainstream programming languages.

So, given that I'm a typical mainstream programmer, is there a programming language out there that will blow my mind and will make me question everything I've believed in for all of my life?","Personally, the last language that 'blew my mind' was Elixir. Specifically when doing concurrency. Coming from the world of mutexes and complicated sharing, Elixir makes everything so much easier."
"Best book on writing an optimizing compiler (inlining, types, abstract interpretation)?",12pk48d,2023-04-17 23:01:45,"I have written lots of simple interpreters, but I'm actually mainly interested in compilers, especially optimizing compilers.

I am specifically interested in things like

- Function inlining (because it enables so many other optimizations)
- Using types and effect systems to automatically prove that a certain program is equivalent to some other simpler program, and using this to optimize the code
- Using partial evaluation/abstract interpretation to pre-evaluate as much as possible at compile time, possibly track where values end up, and get rid of unnecessary intermediate steps in code

Are there good resources that explain how to implement such optimizations in some toy language?

I realize that many popular languages are open source, but I'm new to the topic, and reading huge code bases is very intimidating to me.","I keep a lot of resources that I find approachable on my [resources page](https://bernsteinbear.com/pl-resources/). I also have an extraordinarily half-baked [guide](https://docs.google.com/document/d/e/2PACX-1vSPUfmDiniZy0yn9wjqag8lWOg4Kei_3EXy03EB_pQ-5elwacy0IBZjFyOsjrehIldvhUq0_odDY0Ft/pub) that I am slowly putting together.

Engineering a Compiler by Torczon and Cooper has a couple of abstract interpretation passes that might intrigue you, like Sparse Conditional Constant Propagation.

I used to work on the [Cinder JIT](https://github.com/facebookincubator/cinder/) and can help document any passes you find interesting or confusing."
Glide (Now Open Source),yppqpq,2022-11-08 23:36:16,,"Finally got around to making Glide open source. Here's the link: [https://github.com/dibsonthis/Glide](https://github.com/dibsonthis/Glide)

The language (and especially the code) is a work in progress, so please be nice to the spaghetti.

Cheers!"
How do you get a job designing a programming language?,xnb76i,2022-09-25 10:32:45,"Like when Apple made Swift, or Mozilla made Rust, how do you get on that team? Do you need a PhD in designing programming languages, if such a thing exists? What about a CS master’s and experience building your own language? Just curious. Thanks!","PL design isn’t very lucrative. phd is probably the best bet. getting published in PLDI, etc gives you credentials that someone would hire you for. to really design a language fully means creating a compiler and you need lots and lots of low level experience at the ISA level. most of the compiler guys i know have a ton of processor knowledge such as differences in instructions between them and academic features of them that have to be manually turned on and used"
NASA F' Prime Prime language,x02b1m,2022-08-29 02:27:07,"NASA released their F' Prime ( terrible name for searchability ! ) flight software framework to open source a few years ago. https://nasa.github.io/fprime/

Apparently it's been used at least on the Mars copter, possibly other experimental spacecraft. Note there's another widely known and used NASA open source flight software stack cFS ( core Flight System ) available as well, it's built very differently.

At the core of F Prime is a modeling language called .. F' Prime Prime ( https://fprime-community.github.io/fpp/fpp-users-guide.html ). It looks basically like a modeling DSL that may be a little more general purpose than it may appear at first. It's obviously designed for flight software, hence concepts like telemetry and commands are all baked in. But all of those are useful in a few other application domains as well, like general robotics, or perhaps more broadly, any hard real-time systems.

At the core of it is a directed node graph with statically typed data connections and ports between them ( not unlike Lustre, broader concept in many other modeling languages like Modelica ). Core data types and arithmetics are pretty small, you'd find those in any data definition language ( i.e CDDL ). 

It can generate various outputs from the model ( XML, C++, Python but not constrained to any ).

Maybe an interesting alternative to ROS to build realtime processing topologies on. Would be interested to hear others thoughts on this.","+1 for F prime. It’s flown on a number of small spacecraft as well as Ingenuity. All around a great framework, also the team at JPL is super supportive and will give you advice if you reach out to them. I’d never thought about its uses outside of flight software, but it would probably work well!"
The Race to Replace C & C++ (2.0),qwsol9,2021-11-18 23:52:07,,This is a great conversation about languages and compilers that barely mentions C and C++.
An experimental O(1) Garbage Collector,jxwfzp,2020-11-21 04:08:55,,"It's not really fair to call it O(1) if the compaction pass is O(n). Compaction is part of GC.

You made a variant of stop and copy GC. Call it what it is."
"Logic programming languages: why are they languages, rather than libraries?",hc32v5,2020-06-19 23:49:37,"I know very little about logic programming. I played with Prolog many years ago just to see what it what it was about and that's about it.

One doubt I've always had is: why logic programming is implemented by languages?

In my ignorance it felt like the resolution algorithms could have easily been implemented as libraries for other languages. You should be able to create an object in C, C++, python or whatever to which you add logic rules. This library resolves them and does what a logic programming language would do...

Prolog and similar languages feel to me like Domain Specific Languages: syntax sugar to work more easily with a logic resolution library.

Is that the case, or am I saying something stupid?","What you're asking is not stupid, you just seem to be missing a point here. Prolog may be well-suited for querying data, but it's a Turing-complete language, and may be used for general purpose programming. It's just based on a different computational model than what you may be used to (as C, Python, etc).

While you may easily see Prolog implemented on top of C, each Turing-complete computational model is capable of simulation each other, so you may as well implement a C interpreter on top of Prolog. This motivates the study of Prolog as a language by itself (and logic programming as a paradigm).

Of course that's from a theoretical point of view... from an implementation point of view, one could implement a Prolog compiler as either AOT or JIT (in which case it would be simple to use it as a library), or simply as an interpreter. And, in fact, both kinds of implementations do exist."
"Thought some might be interested in my esolang, Snek",12xcnv2,2023-04-24 19:38:51,"Snek is a stack-based 2-dimensional esoteric programming language inspired by asciidots and befunge, amongst others. I designed it with the goal of having a very limited character set and, as with all esolangs, to be as annoying as possible. The [esolangs wiki](http://esolangs.org/wiki/snek) page has some example programs and an explanation.

A challenge I've attempted in the past is making a compiler to snek. Not to compile snek to a sane language but to compile a sane language *to snek*. It turns out to be quite hard due to the positional nature of the code, might be a fun challenge to try.",This is terrible.. I love it.
"Zig Is Self-Hosted Now, What's Next?",ydrz3k,2022-10-26 15:37:32,,"Do you guys want to hear a joke?

What's 6 years old yet doesn't have a compiler ""stable enough"" to parse tabs?

[The Zig programming language](https://github.com/ziglang/zig/wiki/FAQ/4003c860ff980a854346976bddbaea55f1b1a789#why-does-zig-force-me-to-use-spaces-instead-of-tabs)"
Peridot: A functional language based on two-level type theory,umjfgn,2022-05-10 22:02:10,,"Hey! I'm Peridot's author. Peridot is a language based on two-level type theory which allows for the compiler backend to be written declaratively in userspace. The language is really two languages tied together: a logic language, and a dependently typed functional language. The former is built for metaprogramming - high-level optimizers and compilers can be written that translate the latter language into a target language of choice. An in-depth explanation of the language's rationale can be found [here](https://github.com/eashanhatti/peridot/blob/master/notes/RATIONALE.md).

Currently I've implemented:

1. An interpreter for the logic metalanguage, including unification
2. An interpreter for the object language (the functional one)
3. Basic metaprogramming capabilities
4. An embedding of a subset of C to be used as a target language
5. GADTs

The main things left to be implemented:

1. Dependent pattern matching
2. A constraint system for the logic language
3. An actual user interface, haha

Feel free to ask questions! For those interested, Peridot is the direct continuation of [Konna](https://github.com/eashanhatti/konna), which [I've posted about before](https://www.reddit.com/r/ProgrammingLanguages/comments/rpe65y/konna_my_programming_language/)."
"In my language, inverse functions generalize pattern matching",qraed1,2021-11-11 10:14:46,"I am still developing my **Ting** programming language. Ting is (will be) a pure logical/functional, object-oriented language.

Early in the design process I fully expected, that at some point I would have to come up with a syntax for pattern matching.

However, I have now come to realize, that a generalized form of pattern matching may actually come for free. Let me explain...

In Ting, a non-function type is also it's own identity function. For instance, `int` is set of all 32-bit integers (a type). `int` is thus also a function which accepts an `int` member and returns it.

## Declarative and referential scopes

Instead of having separate variable declaration statements with or without initializers, in Ting an expression can be in either *declarative scope* or in *referential scope*. Identifiers are declared when they appear in declarative scope. When they appear in referential scope they are considered a reference to a declaration which must be in scope.

For compound expressions in declarative scope, one or more of the expression parts may continue in the declarative scope, while other parts may be in referential scope.

One such rule is that when function application appears in declarative scope, then the *function part* is evaluated in referential scope while the *argument part* continues in the declarative scope.

Thus, assuming declarative scope, the following expression

    int x

declares the identifier `x`, because `int` is evaluated in *referential scope* while `x` continues in the declarative scope. As `x` is the an identifier in declarative scope, it is declared by the expression.

The *value* of the above expression is actually the value of `x` because `int` is an identity function. At the same time, `x` is restricted to be a member of `int`, as those are the only values that is accepted buy the `int` identity function.

So while the above may (intentionally) *look* like declarations as they are known from languages such as C or Java where the type precedes the identifier, it is actually a generalization of that concept.

    (int*int) tuple                         // tuple of 2 ints
    (int^3) triple                          // tuple of 3 ints
    { [string Name, int age] } person       // `person` is an instance of a set of records

Functions can be declared by *lambdas* or through a *set construction*. A function is actually just a set of *function points* (sometimes called *relations*), as this example shows:

    Double = { int x -> x * 2 }

This function (`Double`) is the set (`{`...`}`) of all relations (`->`) between an integer (`int x`) and the double of that integer (`x * 2`).

## Declaring through function argument binding

Now consider this:

    Double x = 42

For relational operators, such as `=`, in declarative scope, the *left* operand continues in the declarative scope while the *right* operand is in referential scope.

This unifies `42` with the result of the function application `Double x`. Because it is known that the result is unified with `x * 2`, the compiler can infer that `x = 21`.

In the next example we see that `Double` can even be used within the definition of function points of a function:

    Half = { Double x -> x }

Here, `Half` is a function which accepts the double of an integer and returns the integer. This works because a function application merely establishes a relation between the argument and the result. If for instance `Half` is invoked with a bound value of 42 as in `Half 42` the result will be `21`.

Binding to the output/result of a function is in effect using the *inverse* of the function.

## Function through function points

As described above, functions can be defined as sets of function points. Those function points can be listed (extensional definition) or described through some formula which includes free variables (intensional definition) or through some combination of these.

    Map = { 1 -> ""One"", 2 -> ""Two"", 3 -> ""Three"" }                  // extensional
    
    Double = { int x -> x * 2 }                                     // intentional
    
    Fibonacci = { 0 -> 1, 1 -> 1, x?>1 -> This(x-1) + This(x-2) }   // combined

In essense, a function is built from the function points of the expression list between the set constructor `{` and `}`. If an expression is *non-deterministic* (i.e. it can be evaluated to one of a number of values), the set construction ""enumerates"" this non-determinism and the set will contain all of these possible values.

## Pattern matching

The following example puts all of these together:

    Square = class { [float Side] }
    
    Circle = class { [float Radius] }
    
    Rectangle = class { [float SideA, float SideB] }
    
    Area = {
        Square s -> s.Side^2
        Circle c -> Math.Pi * c.Radius^2
        Rectangle r -> r.SideA * r.SideB
    }

Note how the `Area` function looks *a lot like* it is using pattern matching. However, it is merely the consequence of using the types identity functions to define function points of a function. But, because it is just defining function argument through the result of function applications, these can be made arbitraily complex. The ""patterns"" are not restricted to just type constructors (or deconstructors).

*Any* function for which the compiler can derive the inverse can be used. For identity functions these are obviously trivial. For more complex functions the compiler will rely on user libraries to help inverting functions.

Finally, the implementation of a non-in-place quicksort demonstrates that this ""pattern matching"" also works for lists:

    Quicksort = { () -> (), (p,,m) -> This(m??<=p) + (p,) + This(m??>p) }",Very nicely done! Using the type names as typed identities is a clever trick.
Lang Jam - language programming weekend jam,ovf0jy,2021-08-01 05:03:38,,Finally! :D
Hobby project - Orb programming language,naw1xh,2021-05-13 02:41:43,"GitHub: [https://github.com/vplesko/OrbLang](https://github.com/vplesko/OrbLang)

Documentation: [https://vplesko.github.io/OrbLang/](https://vplesko.github.io/OrbLang/)

Hi, all.

This is a project I've been picking away at for some time. I finally got it to a polished enough state where I figured I might as well show it to someone.

The initial idea for Orb was to be somewhere between C and C++, meaning a general-purpose compiled language with various quality of life features, but not too complex. As I was developing it, I was coming up with all sorts of features to put in: templates, generator functions, etc. At the same time, I was learning more and more about other languages I haven't used before.

So then, I got this crazy idea of introducing a powerful macro system and offloading much of compiler's work into the language itself. Many constructs which would normally be part of a programming language (for loops, enums, many operators...), in Orb are simply macros defined in libraries that come installed with it. Users are encouraged to define their own macros and mold the language according to their needs.

It's been quite a lot of fun working on this. The documentation explains the language and there are some code examples at the end. Feel free to take a look and try it out, and let me know what you think!","Neat! Maybe I have missed, I am clicking thorugh documentation and I cannot find call syntax conventions explained, since you dont use parenthesis for arguments. You have to explain how expressions ""func arg arg (complex arg arg) (arg arg);"" will be evaluated, since this was unclear :/...or I am blind :)

EDIT: I am blind.   
here is it ""For example, if it resolved to a function, the node represents a call to that function. If it resolved to a special form, the node represents usage of that special form."" in [https://vplesko.github.io/OrbLang/pages/advanced\_concepts\_node\_based\_syntax.html](https://vplesko.github.io/OrbLang/pages/advanced_concepts_node_based_syntax.html).

...But that makes it, basically, just a regular LISP? :)"
An Object-Oriented Language for the '20s,m479v3,2021-03-13 22:25:02,,"I missed the section on why you want to create an OO language in the first place.  If you stick to the immutable fragment you have a kind of FP language with convoluted semantics (see the section on higher ranked types) and some warts. For example, what’s your story on object identity?  Every value having identity is pretty bad. And gets worse if it can have mutable state attached."
Are there any languages who allow parameters to be part of method name?,l6d3qd,2021-01-28 04:05:48,"Let's look at this Python example:

    def does_show_end_before_time(show: Show, time: datetime) -> bool:
        return show.end_time < time

I've always wondered if there's any language out there who would allow me to write something like


     def does_{show: Show}_end_before_{time: datetime} -> bool:
         return show.end_time < time

with the call

    my_show = Show(...)
    end_time = datetime(...)
    ends_early = does_(my_show)_end_before_(end_time)","Smalltalk famously does this, e.g.:

    canvas drawCircleWithRadius: 5 atX: 2 andY: 7

This method (message) above is named `drawCircleWithRadius:atX:andY:`."
Effekt: A language with effect handlers and a lightweight effect system,i3eyg5,2020-08-04 15:31:58,,The ide integration is a nice touch. Only having downward funargs seems like a big limitation though
This plain English programming language makes me rethink the notion of natural language programming,h0fpmi,2020-06-11 01:34:20,"For the longest time, I have dismissed the notion of natural language programming as a silly idea.

But after coming across the following and starting to go through the tutorial, I may change my mind yet:

[https://osmosianplainenglishprogramming.blog/](https://osmosianplainenglishprogramming.blog/)

Apart from the documentation, the article on the site is definitely worth a read.

I notice also that one of the creators, u/GerryRzeppa/ is also here on Reddit.","I guess I'm in there minority here, but I'm really not a fan of this kind of thing.

First, you don't get to just ignore ignore syntax here. You go from having to know the syntax of a programming language to having to know the syntax of a programming language that looks like English, instead. In their toy examples, it seems all well and good, but imagine having to debug this because the compiler didn't interpret your sentence how you thought it would...

Second, even ignoring the syntax issues (which I think is a fairly large problem), you still have to deal with the ridiculous levels of verbosity. Compare an assignment in that language to `a = b` in most languages..."
Go statement considered harmful,gojp2j,2020-05-22 21:50:14,,“X considered harmful” often are “I don’t like x”. But let’s give it a chance.
Why is Zig so much more successful than Crystal and Nim?,10hu5md,2023-01-21 23:48:25,"Zig wants to be a better C, which is a place that is already pretty well filled by Rust. Zig might have some advantages over Rust, but imho it also has some disadvantages. So besides the old and mighty languages (C and C++), there is also a ""cool new kid"" around the block. Plenty of competition for Zig. Nim and Crystal are closer to C# or Java, when considering what type of programs you'd use them for. And as opposed to Zig, there is no other cool new kid around that is already semi-established. Even though both languages are older (much older in the case of Nim) they have less stars, and less commits on github, and I subjectively percieve much more hype around Zig. Is it just because Andrew is so much better at marketing?

Honestly, I'm a bit upset, a highlevel language which has tons of features and garbage collection without extra runtime dependencies and still reasonable speed seems to be much more usefull to me than a slightly better C, that advertises itself with missing features, when I already have Rust anyway.

EDIT: Ok, I forgot, there is Go, which also roughly falls into the same spot as Nim and Zig. So competition wise they're even with Zig, and they're still performing worse. Also, Go is such a terrible language ...

Edit2: just to be clear, I'm not upset about Zigs success, i just whish Nim and Crystal would be more successful.","I think the issue with Nim and Crystal is that if you want a language with garbage collection, you already have so many options out there. If you want a language without garbage collection, you’ve basically got C, C++, Rust, and Zig. If you think that C++ and Rust are too complicated, then you’ve really just got C and Zig."
job designing/implementing programming languages for quantum computing,xdmlog,2022-09-14 07:51:54,"Hey all, I hope this isn't off-topic here, but I wanted to give notice of a [job opportunity](https://recruiting2.ultipro.com/HRL1001HRLLB/JobBoard/8137e11c-7bd2-40bd-944e-52183e76b433/OpportunityDetail?opportunityId=cf64b3bd-2b52-4084-8385-75ddbc136e79) at HRL Laboratories that deals with all sorts of programming language design and implementation issues. HRL Labs researches quantum computers based on a technology called ""exchange-only silicon dot qubits"". They're really cool, and easier than you might think to understand.

Technologically:

- We primarily use Common Lisp. I think that's already interesting in its own right, but...

- We implemented [Coalton](https://coalton-lang.github.io/), an open-source strictly evaluated functional programming language with a Haskell-like type system. It is embedded in Lisp, and so it compiles to native machine code. We develop this as a part of our work. 

- We have some of the developers behind [Quil](https://quil-lang.github.io/), one of the top industrially used languages for quantum computing. It's one of the languages we use for quantum. 

The team of which the position is a part has the overarching goal of using programming language design as a means to:

- Make quantum computing easier for experimentalists

- Make software more verifiable and debuggable

- Make software that's efficient and outperforms other industry-standard techniques

Now, the job does have requirements that make a lot of interested applicants ineligible:

- It's US citizen and US resident only, because of the need for security clearance eligibility

- It requires relocation into the orbit of Malibu, Calif. (I'll admit, we do have a beautiful office.) You'd be working out real hardware, sometimes on secure networks.

With that said, we have a healthy company-wide WFH policy that is 50% time, but our team has more leniency and can accommodate more.

On the whole, we are just looking for really good, well-rounded software engineers who have a particular interest in working on different languages in this domain.

I'm happy to receive questions about any of the above here, even if it's just related to quantum languages, Lisp, Coalton, Quil, or whatever.

Interested folks can also PM me, or just apply through the front door (linked above).

Thanks!","Wow, I wish I could take this opportunity, but I am obviously in know place in my life for this right now or in the US at all. Can I apply and start working in about.. 10 years or so?"
Counterexamples in Type Systems,w5yb62,2022-07-23 15:48:09,,"This is a fun reference.  The chapter on [polymorphic references](https://counterexamples.org/polymorphic-references.html) discusses one of my favourite problems when you mix mutability and polymorphism.  This issue also crops up in Haskell if you use `unsafePerformIO` at top-level, which I have seen surprise some people.  Often the ""unsafe"" in `unsafePerformIO` is taken as effect ordering being difficult to predict, but in fact it's so unsafe that it subverts type safety entirely.  [Covariant containers](https://counterexamples.org/general-covariance.html) is a similar problem, but using subtyping instead of polymorphism."
Cognate - concatenative programming in English prose,vmvyr6,2022-06-29 03:44:51,,"Hi everyone!

I've been developing Cognate for a while now on this subreddit's corresponding discord server, so some of you may know it from there.

Cognate is unique spin on both concatenative programming as well as natural language programming. Unlike most concatenative languages, Cognate uses prefix notation, evaluating semicolon-delimited statements right-to-left. Cognate achieves natural language programming by simply ignoring identifiers beginning with lowercase letters, allowing comments to be interleaved with code - called ""informal syntax"". Brackets define closures, so a simple cognate program might look like this:

`Map (+ 1) over the Range from 1 to 10;`

This informal syntax allows complex programs to be verbose and easier to understand, while allowing trivial functions to be written concisely. Giving the programmer the freedom to write what they want simplifies the language and gives the programmer freedom in their explanations.

Cognate is a rather dynamic language, yet it compiles to fairly efficient C. It performs some compile time typechecking - making it gradually typed. A new optimizing compiler is in the works which should yield even faster performance.

website: [cognate-lang.github.io](https://cognate-lang.github.io)

github: [github.com/cognate-lang/cognate](https://github.com/cognate-lang/cognate)"
Time complexity as part of the type system?,scsddv,2022-01-26 08:28:22,"The thought just occurred to me & I thought it was interesting. Is there any other work around this topic?

The idea is to encode the time complexity of functions as a function of their parameters, and then the developer can 'limit' the time complexity of functions to some value - e.g. 'this function must run in less than O(n^2) time', or similar.

Use case: I've run into performance issues for code that grew to be O(n*m) (& therefore effectively `n^2`) where previously `m` was constant, then for some workloads your program explodes.

Note: I think this is probably a dumb idea in practice, but it's kinda neat

Here's an example of what I'm talking about

    // Define some 'typeclasses' for a simple map, & implement it 3 times
    
    type Map[k, v]
      where
        get :: Map[k, v] -] k -] v
        put :: Map[k, v] -] k -] v -] ()
    
    type HashMap[k, v, size]
      implements Map[k, v]
        get :: O(1)
        put :: O(1)
    
    type BTreeMap[k, v, size]
      implements Map[k, v]
        get :: O(log(size))
        put :: O(log(size))
    
    type LinearMap[k, v, size]
      implements Map[k, v]
        get :: O(size)
        put :: O(1)
    
    // Later in your code, we can check cool stuff
    
    fn check_all_present(foo: Array[x, n], valid_values: Map[x, bool, m]) {
      for val in foo { // Looping over foo, this takes O(n * xxx) where xxx is the time complexity of the loop body
        if !valid_values.get(v) { // This takes m when valid_values is not a HashMap
          return false;
        }
      }
      return true;
    }
    
    // Therefore, check_all_present is O(n*m) when valid_values is not a HashMap
    // Now let's use this in a more interesting example:
    
    // Extract an array of features from the input text
    // The complexity here is 'limited' to O(n). This is enforced at compile time
    fn parse_features(input: String[n]): Array[Feature, x : x < n] 
        limit O(n) {
      ...
    }
    
    // Here, we also limit the complexity to O(n), and call both functions from earlier
    fn compute_data(input: String[n]) 
        limit O(n) {
      let valid_features = ...;
      let results = parse_features(input);
      if check_all_valid(results, valid_features) {
        println(""Extracted valid features from the text input"");
      }
    }
    
    // Now, compute_data is limited to `n` time complexity where `n` is the length
    // of `input`. We call `parse_features` once, but that's O(n) too so that's
    // fine. Then we call `check_all_valid` - and here's where things get
    // interesting!
    // If valid_features if a BTreeMap or LinearMap, then we get a compile error here
    // *unless* the size of valid_features is constant & known at compile time -
    // e.g. if it's a hardcoded dictionary

Fun, right? 

Problem 1: How do you unify `n` and `m` in `compute_data`? Intuitively maybe I know that `m` and `n` are both large, & therefore a limit of `O(n)` should be a compile error - but the compiler doesn't know that.  I suppose you can just take some expression, unify all variables to the same one & reduce it? so `O(j*k*l)` just becomes `O(n^3)`..?

Problem 2: How can I order asymptotic complexity? e.g. how do i know which time complexities are 'less than' `O(n^2)`? obviously `n*log(n)` & `n` are, but what about something weird like `(n^1.8)*log(n)` ?

Any research / examples which implement what i'm describing here?","There has been work on this! 

https://dl.acm.org/doi/abs/10.1145/3498670

This paper augments a dependently typed language with constructs to specify cost, along with the ability to distinguish between the cost-relevant (intensional) behavior of code, and the cost-irrelevant (extensional) behavior of code.


http://isilmar-4.linta.de/~aehlig/university/pub/12-raml.pdf

This paper augments a simpler language (basically OCaml without a module system) with automatic cost inference.

An implementation exists here: https://www.raml.co/interface/

The same group recently released this paper: https://dl.acm.org/doi/abs/10.1145/3434308"
Interview with Odin programming language creator Ginger Bill,quhc3b,2021-11-15 22:12:47,,"so, now I've got to go learn odin. kinda exciting."
"Compiling to Assembly from Scratch: the book's compiler ported to Python, OCaml, Rust",mlw26i,2021-04-07 14:34:20,,"Hey folks, you have probably seen my book [Compiling to Assembly from Scratch](https://keleshev.com/cas) before on this Reddit. It's a beginner-friendly guide to compilers and assembly. The book uses a simple subset of TypeScript for the code, but now the compiler is also available in Python, OCaml, and Rust.

I believe the compilers have some educational value even without the book, so check it out! Happy to answer any questions.

Thanks Brendan Zabarauskas (@brendanzab on GitHub) for the Rust port."
"Dijkstra's ""Why numbering should start at zero""",maft1w,2021-03-22 13:24:06,,"Fairly recently, there was some discussion here about whether numbering should start at zero.  I recall some posters dismissing Dijkstra's position, who seemed not to have read his presentation of it.

Since I saw this classic link reposted on [lobste.rs](https://lobste.rs), I thought I'd link it here as a basis for further discussion.  (For my part:  I read this as a student, and was persuaded by it)"
qdbp: my take on pure object oriented programming,12xo1hj,2023-04-25 00:21:47,"Hi all,
I am pleased to announce qdbp, a language that I have been working on for about a year. qdbp is my take on pure object oriented programming. The base language is fairly minimal - it has no if expressions, loops, switch, monads, macros, etc and can easily be fully demonstrated in 15 lines of code. However, many of the aforementioned constructs and much more can be implemented as objects within the language.

qdbp has a website ([qdbplang.org](https://qdbplang.org)) where the language is explained further, but here is a quick rundown of its notable points

- qdbp has two kinds of objects: prototype objects(anonymous records) and tagged objects(polymorphic variants)
- Unlike virtually every other OOP language, qdbp has no support for mutation
- Also unlike virtually every OOP language, qdbp has no inheritance. Instead, it features prototype extension that can accomplish most of the same goals
- The language has a fully inferred static type system based on a slight modification of [Extensible records with scoped labels](https://www.microsoft.com/en-us/research/publication/extensible-records-with-scoped-labels/)

If you are interested in learning more, qdbp has a [github repo](https://github.com/dghosef/qdbp) and a [website](https://qdbplang.org) that contains a [tutorial](https://www.qdbplang.org/docs/tutorial), [examples](https://www.qdbplang.org/docs/examples) including implementation of many common constructs(if, defer, for, etc) as objects, and a detailed [design rationale](https://www.qdbplang.org/docs/rationale). The compiler works but, as with most language announcements, is a little rough around the edges.

Thanks for reading! I have been lurking on this sub for a long time, and it has been a great resource and inspiration. I would greatly appreciate any feedback, good or bad. And, if anyone wants to join the project, I could always use a contributor or two ;)

To end, here is an obligatory code sample, implementing and using a `switch` expression(this is also on the website's homepage)

    switch := {val |
      {
        Val[val]
        Result[#None{}]
        Case[val then|
          self Val. = val
            True? [
              result := then!.
              {self Result[#Some result]}]
            False? [self].
        ]
        Default[then|
          self Result.
            Some? [val| val]
            None? [then!.].
        ]
      }
    }
    str := switch! 5.
      Case 1 then: {""one""}.
      Case 2 then: {""two""}.
      Case 3 then: {""three""}.
      Case 4 then: {""four""}.
      Case 5 then: {""five""}.
      Case 6 then: {""six""}.
      Default then: {""> six""}.","So I view OOP as primarily a mechanism for controlling state - that is, by encapsulating a bundle of state inside of an object and only allowing methods on the object to manipulate that state in set ways, it's impossible for that state to become internally inconsistent.

Without mutability, what's the particular advantage of an OOP approach?"
Language wars: the personal factor,106ah7j,2023-01-08 13:08:02,"What we want from a language is that it reduces our cognitive overhead and lets us as near as possible just think about the algorithm. And it would be really weird if we all had the same brains.

For example, the people who like terse variable names like them for exactly the same reason I like variable names to be littleSentencesInCamelCase. It’s less mental overhead. *It’s just easier to read*.

Sometimes preference may be learned. I suspect (without actual research) that whether you’re a type-firstian or a type-lastian depends mainly on which language you were brought up on.

And then there’s syntactic whitespace. (P.S: please don’t make me tap Rule 2, the last discussion we had on this became most uncivil, the phrase “I’m sorry” does not sound *at all* contrite when you follow it up with “I thought you were a professional”. Thank you.)

I like syntactic whitespace for the same reason the people who like braces like braces. For them, braces cause less muddle, for me, syntactic whitespace causes less muddle. (The difference doesn't have to be much, nor the muddle significant, for this to be a factor: everyone wants as little friction as possible.) My own preference is because although I was brought up in Pascal, with its honking great `begin` and `end` tokens, my brain instinctively reads the indentation as the source of truth, whereas the compiler knows that it isn’t. For me, the overhead of keeping my braces redundantly in line with the whitespace costs more than the redundancy saves me.

(Another consideration here is fit with the rest of your coding style, or the style imposed by your favorite lang. If you write your code in lots of small shallow functions, then braces become less meaningful. This is why functional languages tend to use syntactic whitespace.)

There are people who don’t like type inference. For them, the additional overhead of reasoning about types when you read the code exceeds the savings of not reasoning about types when you write the code.

We had another thread about Go, I see. Again, the people who like it like it for one of the main reasons people who don’t like it don’t like it: it’s small. So many features it doesn’t have. (I’ve seen people say they were going to quit Go because of the introduction of generics.) If you’re perfectly happy writing and reading verbose code built with a small number of tools that you know very well, then you’re a different species from the programmer who can’t live without a Turing-complete type system and macros.

**Alice**: “Your fave language is terrible, it willfully ignores the last 30 years of PL research!”

**Bob**: “Your fave language is terrible. It *contains* the last 30 years of PL research!”

**Alice**: “Your language is a Blub language!”

**Bob**: “Your ‘language’ is a collection of mutually unintelligible dialects so large that no one person has mastered all of them.”

Etc, etc.

Each wants code that’s easier to reason about, but Bob thinks Alice’s code is too diverse and abstract, whereas Alice thinks Bob’s code is too diffuse and insufficiently DRY, and they both blame each other’s choice of language. And they are both right. (All languages are a tradeoff, so all languages contain an irreducible amount of suction.)

This is without so far mentioning the truism that different languages are suitable for different tasks. (Recently I had a fan of Lean tear into me for asking if it would ever be useful to me personally. I was actually hoping the answer would be “yes”.) You have to “show people the door before you show them the key”: if they’ve never wanted to do the things you do in your favorite language, of course they’re only going to see its difficulties and not its virtues. And conversely, when someone shows you their favorite language, that’s what *you’re* going to see.

In conclusion, maybe these would be reasons to flame people less? I don't have a better conclusion, but this isn't the toast from the Best Man so I'll just stop now.",There is also a lot of defense of the investment one has made in gaining proficiency in one's favourite language.
Cppfront: Herb Sutter's personal experimental C++ Syntax 2 -> Syntax 1 compiler,xgcb7h,2022-09-17 12:15:26,,"The ""quick & dirty"" first page example is very good.

Not everyone has the time to download a full example from a repository.

I was working on a C alike lambda concept P.L., as well as other do, and this is a good one 👍"
Functional Programming in Lean - an in-progress book on using the Lean theorem prover to write programs,v8lm0g,2022-06-10 01:09:44,,"What's the big reason(s) to use Lean over Coq as an engineer interested in verification? Thus far, Lean seems to be more accepted by the mathematicians than the engineers. Is this true? Is there reason for that? Coq has the most educational resources which makes it more appealing as a newbie. But, perhaps I'm missing something with Lean?

_P.S. Thanks for ""The Little Typer"", OP :)_"
Why are imperative programs considered faster than their functional counterparts?,rsce6j,2021-12-31 05:14:16,"So I find algorithms, efficiency, and just languages in general very fascinating, and recently something that has grabbed my attention and time is functional programming and all the ideas of no mutalable state and such (as a side note, are there any jobs in that more theory type?). In my quest, I've heard a lot of people say that function programs or that functional languages are slower or less efficient. So my question is, is reduced efficiency an issue with the functional paradigm or an issue with current functional languages? For instance, about a year or two ago I made a post about abstraction and efficiency and one commentor made the point that Haskell *could* be as fast as say C, however the compiler hasn't had the time to mature to the same level as say GCC. That would be a language problem. What I'm wondering is if functional programming in and of itself has speed hurdles that imperative programming does not. Sorry for this lengthy post, but I look forward to the responses","> So my question is, is reduced efficiency an issue with the functional paradigm or an issue with current functional languages?

It's a little of both.  I think it's an open problem whether persistent data structures are fundamentally less asymptotically efficient than mutable ones.  But even such a comparison depends on how you define the rules of the game: in the RAM model (or PRAM, whatever), I have a hunch that you can show that some algorithms simply require mutation or they will suffer asymptotically.  Of course, the RAM model doesn't scale - constant-time random access cannot exist in our universe; eventually you'll need something like *sqrt(n)* or *log(n)* for accessing random memory, and perhaps under those assumptions there will be no asymptotic difference.  But maybe you don't care about a hypothetical arbitrarily large computer (say, the size of the solar system), but are OK with the constant access time that can be guaranteed for any actually existing computer on Earth.  (Although in practice cache effects makes random access look more like *log(n)* anyway...)

But such theoretical boundaries are not the reason current functional languages are (correctly) perceived as relatively slow.  I would say that in practice, the main difference is that functional programming by necessity involves a whole bunch of abstraction that the compiler then has to work hard to avoid paying a performance overhead for.  In contrast, straightforward imperative programming tends to come with less such abstract baggage.  But this is a really blurry line: both OCaml and Haskell lets you write unsafe imperative code and generate pretty much the same assembly as a C compiler would.  This can be convenient in practice, although it's not really the *point* of functional programming.

> For instance, about a year or two ago I made a post about abstraction and efficiency and one commentor made the point that Haskell could be as fast as say C, however the compiler hasn't had the time to mature to the same level as say GCC.

I don't think it's as easy as saying that since GHC has less hours put into it than GCC, that's why it generates slower code.  It also has to solve a problem that is more difficult: removing all that abstraction takes effort, while GCC has to do much less.  Consider that the straightforward compilation schemes for Haskell involves generous heap allocation and boxing, while the straightforward compilation scheme for C involves registers and stack.  GHC has to do a lot of strictness and escape analysis to figure out which values can go unboxed in registers or on the stack, which a C compiler does not.

There's also the question of programming style.  Functional programming is *about* letting you program in this very abstract manner with higher order functions and polymorphism and people *do so*.  If you programmed that way in C, people would think you were a lunatic, and probably the compiler would not generate very fast code.

There are functional languages (and their attendant compilers) that specifically focus on performance.  They usually cut down the language to make the compiler's job easier, or provide the programmer with extra tools for annotating their program, to guide the compiler.  In principle, functional languages provide much more information to the compiler, and should allow more aggressive optimisation than a C compiler (e.g. selecting data layout for good locality, or parallelising).  In practice, most functional compilers spend most of their compile-time boiling away the overhead of abstraction, just to get to the point where a C compiler would start."
What are the advantage of Object Oriented languages over Functional languages? Particularly mutability.,qyf3k0,2021-11-21 05:20:55,"Edit: As @VideoCarp1 pointed out, mutability is not exclusive to OO and for that matter immutability is not exclusive to functional.

Everyone talks about advantages of functional languages over object oriented.. I'm curious to hear everyone's thoughts on the advantages of Object oriented instead. Particularly been thinking about mutability.

&#x200B;

**Benefits of Immutablity**

\- Hard to accidentally change a variable you didn't intend to change.

\- Less likely to accidentally refer to the wrong variable

This study suggests using a functional language does indeed reduce bugs significantly:

[https://web.cs.ucdavis.edu/\~filkov/papers/lang\_github.pdf](https://web.cs.ucdavis.edu/~filkov/papers/lang_github.pdf)

**Benefits of Mutability**

\- Performance and memory wise mutability gives some advantages, though proper compiler level optimization helps a lot on this front.

\- Managing state seems much harder with functional code.. such as having a car object in a game and you have to throw away the old car every frame when you update it's speed, I think your main loop would have to be recursive with tail optimization to keep track of game state? Doesn't feel as clean. Not a functional coding pro.

\- Seem all around easier.. and a little bit like functional languages are the Esperanto of programming languages but that might just be my bias from having more experience with OO.

\- Can use iteration instead of recursion, which is often easier to reason about

\- At the end of the day you need to interact with mutable information anyway in functional programming.. files, database, networks, etc. So 'purely' functional isn't completely honest

&#x200B;

One of the data points I found particularly interesting is in this study it was found that Object Oriented  languages tend to be more productive than functional languages, found another study that indicating the same thing on this front though I can't find it at the moment:[https://medium.com/smalltalk-talk/smalltalk-s-proven-productivity-fe7cbd99c061](https://medium.com/smalltalk-talk/smalltalk-s-proven-productivity-fe7cbd99c061)

I do believe at a minimum modern languages should support immutability by default but wondering if there is a better way or specific cases when mutability is needed.","It's definitely a good question. I think pure functional programmers would argue that even functional languages like Haskell support opt-in mutability, so maybe the advantage of OOP/imperative languages would have to be in mutability by default, which I think can be better sometimes.

One case that comes to mind is when refactoring a stateful algorithm, or changing a stateless one to stateful. In Haskell, you would have to rewrite all of the pure code with the monadic version e.g. change map to mapM, use bind (<-) instead of let, etc. Or you may be using `State Int` to store a mutable field but then realize later that you need more fields, so you need to change it to use a record type instead. On the other hand in an OOP program I can add new fields and change the state no problem.

So overall I think Haskell at least ends up being more brittle for these cases."
"Chumsky: an ergonomic Rust-y parser combinator library with support for rich error messages, multiple error recovery, infallible parsing, LL(k) grammars, and much more",qhygrb,2021-10-29 06:40:36,,"I've been using this library for a hobby project and it's pretty cool.

Are there any breaking changes in this release?"
ALGOL 60 at 60: The greatest computer language you've never used and grandaddy of the programming family tree,gklgaa,2020-05-16 08:56:02,,"Algol 60 was an interesting language in many respects. It had no data structures, just scalars and arrays. And it was the first language specified using the Backus-Naur-Form (BNF) which is still in use today. I recently wrote a parser for it (on my way to a Simula 67 compiler): [https://github.com/rochus-keller/Algol60](https://github.com/rochus-keller/Algol60).

EDIT: the timing of the referenced article and this post were well chosen since the  language report appeared indeed in Mai 1960, see [https://dl.acm.org/doi/10.1145/367236.367262](https://dl.acm.org/doi/10.1145/367236.367262)."
Nomsu: a dynamic language with natural-language-like syntax and strong metaprogramming that cross-compiles to Lua,ba9l0k,2019-04-07 06:10:56,"I'm really happy to announce the release of my language, [Nomsu](https://nomsu.org/)! This sub has been a big inspiration for me along the way (though I'm mostly just a lurker), so I hope you folks like my language. Some of Nomsu's inspirations include Moonscript, Lua, Python, Racket, and Smalltalk. I've already done a bunch of writing about the language in preparation for its release, so feel free to check it out on the language's website:

* [Nomsu's Syntax](https://nomsu.org/syntax)
* [Why Nomsu Exists](https://nomsu.org/why)
* [Installation instructions](https://nomsu.org/get-started)
* [Some technical details](https://nomsu.org/under-the-hood)
* [Nomsu Tooling](https://nomsu.org/tools)
* [Public source code repo](https://bitbucket.org/spilt/nomsu)

Some cool features of Nomsu include:

* Minimalist, but extremely flexible mixfix syntax defined with a Parsing Expression Grammar
* Hygienic macros, homoiconicity, and other metaprogramming features that allow most of the language's functionality to be self-hosted, and allow for easy extension of the language
* A bunch of self-hosted tooling, including a code autoformatter, automatic version upgrading (on-the-fly or in-place upgrading files), syntax-aware find-and-replace, a tool for installing third party libraries, a REPL, and a Ruby Koans-style interactive tutorial
* Fast compile time, on the order of tens of milliseconds to run a big file. Nomsu has a bit of spin-up time, but once a file is loaded, it will execute as fast as regular Lua code, which is very fast when running with LuaJIT
* Nomsu code can be precompiled into readable, idiomatic Lua code for extra speed and can use Lua libraries easily
* A strong commitment to good error reporting for both syntax and run-time errors, including useful suggestions for how to fix common mistakes
* A future-proof versioning system that allows multiple different versions of Nomsu to be installed on your computer without everything breaking
* Cross-platform support for mac, linux, and windows

And of course, the obligatory code sample:

    (sing $n bottles of beer) means:
        for $i in ($n to 1 by -1):
            $s = ("""" if ($i == 1) else ""s"")
            say (""
                \$i bottle\$s of beer on the wall,
                \$i bottle\$s of beer!
                Take one down, pass it around...
            "")
        say ""No more bottles of beer on the wall.""
    
    sing 99 bottles of beer

I'm happy to answer any questions, and I'd love to hear your feedback!",This is incredible!  I've never seen a language with a built-in tutorial before; what gave you the idea to include this?
Breaking out of all loops without using GOTO,qah8lt,2021-10-18 14:47:43,,"Have you look into ""labeled break"" yet? It's a more general idea of what you are trying to do."
How to learn type checking and type theory?,pjkdzf,2021-09-07 18:28:21,"Particularly interested in things like Per Martin Lof theory, what ""intuitionistic"" means and why do we care, different types of type systems like first/second order, their advantages and limitations, dependent types and the broad classes of ideas around it, linear types, and potentially even things like Homotopy Type Theory. 

I want to take a canonical course in this world of logic, CS, abstract math or whatever you would call it. I have a basic understanding of discrete math and I've been Wikipedia/reddit surfing a lot the past few months but I still have no sense of the landscape and still discover really fundamental ideas every now and then. My ultimate goal is to develop a wonky, gradually typed, dependent type system if it's possible (I want my tensors to be type annotated with their shapes and my neural net code to be statically verified at edit-time okay). I don't care too much about Category Theory, which is super common to run into, which, for the love of God, I can't break into despite a dozen attempts. Seems like mental masterbation to me, no offense to anyone.","Look at Types and Programming Languages by Benjamin C. Pierce, this covers the fundamentals of type systems, then look at Advanced Types and Programming Languages by Benjamin C. Pierce which begins to talk about more complicated type systems such as dependant types"
There is no escape from Futhark,o8znb4,2021-06-27 23:48:55,,"Nice write-up!

But I'm not sure what this means: *""Reflection mechanisms in languages like Java and C# are a similar example - while they don’t allow us to subvert the virtual machine itself, we can break the type systems of the languages as much as we wish.""*

Also, I think this is a type: *""lays of arrays""* \-> did you mean ""lays out arrays""?"
migc: Conservative GC for C/C++/Rust/Zig and other compiled languages using mimalloc,lw60wa,2021-03-03 00:28:28,,Hello to all! I developed this library just as proof that mimalloc can be used for implementing fast GCs and as an simple example on how conservative GC implementation can look. It does not have much but its functionality is close to what [tinygc](https://github.com/ivmai/tinygc) has except migc is much faster since it uses mimalloc for allocation and does not use linked list to search for pointers and migc does not support threading at all. Entire implementation is around 300LOC of C code.
Metamath C: A language for writing verified programs,hczeof,2020-06-21 10:53:23,"Hi all, I am currently in the design phase for a new language, provisionally called ""Metamath C"" after the Metamath Zero proof language and the C programming language. Currently, there is a [design document](https://github.com/digama0/mm0/blob/master/mm0-rs/mmc.md) styled as a language reference, as well as the beginning of a [program written in MMC](https://github.com/digama0/mm0/blob/master/examples/verifier.mm1), translated from the same program [written in C](https://github.com/digama0/mm0/blob/master/mm0-c/verifier.c). There is currently no compiler or even a parser for MMC (EDIT: there is a parser now), but it is built as a DSL inside the MM1 proof assistant (which does exist and is reasonably stable, though still beta), which comes with a scheme-like metaprogramming language.

The concrete syntax of the language looks like Lisp because the MMC compiler is implemented as a function in the metaprogramming language that takes a big lisp s-expression containing the code. The only extension you need to know about is that `{x op y}` is syntax for `(op x y)` and is used for binary operators like `:=` in the language. (There is also the use of `(f @ g x) = (f (g x))` for cutting down on the common ""parenthesis hell"" issue of standard lisp syntax but I am not using it in the MMC code snippets.)

I'm hoping to get some suggestions for the language structure, primitive functions, possibly also the underlying logic framework, as now is the best time to take action on such remarks before the implementation starts in earnest and the design calcifies. I encourage you to check out the [design document](https://github.com/digama0/mm0/blob/master/mm0-rs/mmc.md) for the basic enumeration of planned features, and I will stick to a broad overview in this post.

## What's this about?

The goal of this language is to assist in the development of programs with the highest possible correctness guarantee. Currently, the target is programs running on x86-64 Linux only. We have a [specification of the semantics of x86-64 and Linux](https://github.com/digama0/mm0/blob/master/examples/x86.mm0), with essentially describes formally the execution behavior of (a small subset of) x86 instructions, and of Linux system calls accessible through the `syscall` instruction. These are meant to be underapproximations of what these interfaces actually support, but they can be extended over time. When we distribute an executable for linux, it is generally in the form of an ELF binary, so we can state what it means for a sequence of bytes to encode an ELF binary and what it means for that binary to execute, do some IO, and eventually exit.

With this, we have enough to write a specification of a program: to say ""this program tests whether the input value is prime"" or ""this program says 'hello world' on stdout"" or ""this program plays tetris"". But there is a huge gap between the level of abstraction of the desired specification and the level of abstraction of the program that implements this specification, and this is where compilers come in.

A compiler traditionally takes as input some text describing a program, and then it does some magic and spits out a binary file that purports to do the same thing. We modify this recipe a little: A verifying compiler takes as input some text describing a program and also a proof of correctness of the program, and it spits out a binary file and also a proof of correctness of that binary file (relative to the same specification it was provided). That is, while it is doing its usual compiler thing it is also manipulating proofs of the steps it is going through so that the proofs get progressively lower level in tandem with the program and we maintain correctness all the way through.

The cool thing about this approach is that the compiler itself need not be correct. This is not a verified compiler in the sense of CompCert. Some people call this ""translation validation"" but that doesn't quite capture it. The compiler may perform ""nondeterministic"" steps from the point of view of the proof: for example during register allocation it is free to just come up with an allocation map and then prove correctness. But this is not nearly as large-step as taking the input and output of a complicated program like gcc and seeing if we can figure out what just happened. It might be possible but I doubt this will result in a very high success rate, and no one wants a compiler that fails most of the time.

## Language extensibility

The level of abstraction of the language is at roughly the level of C, although it is also influenced by other languages like Rust. Because everything is defined with respect to rock bottom semantics, literally any programming language idiom can be implemented, provided the correctness of the technique can be proven. This is sort of the analogue of things like `macro-rules` in scheme: the language itself is extensible with new ""primitives"". The syntactic details of this are still being worked out, but for example you can define `for` to be a new language construct in terms of `while`, provide places for the proof obligations, prove some general lemmas about bounded for loops (that will not be replayed on every use of a for loop), and then it will be just as if the language had always had `for`.

## Correct first, pretty later

Because we are starting from a well defined semantics, if you can get your code to compile, *it is correct*, no joke. MMC will never produce an incorrect program, although you can give the program a not very informative postcondition if you like. But additional work enables more productivity enhancing language features, and these help make your program/proof more maintainable. Because MMC is currently a DSL inside MM1's metaprogramming language, you can write arbitrary programs to write your program too (although this has a compile time cost).

## Crashing is okay

One unusual property of MMC programs is that they are memory safe but can segfault. The reason for this unusual state of affairs is that segfaults are not themselves harmful: they signal an error to the parent, which can then use this information as it likes. Whatever the specification promised could not be delivered. This is basically a quality of service issue. It would be nice to say ""this program always terminates successfully"", but this is a fantasy - just try unplugging the computer and see if that still holds. (Crash safety is an interesting research area but requires more hardware modeling than exists in this x86 model.)

Instead, we make the promise that *if* the program runs to completion and returns error code 0, then your specification is satisfied. One upshot of ""memory safety modulo segfaults"" is that we can do call stack handling a la C: no need to worry about stack overflow, because we will hit the guard page and crash before we hit allocated memory and corrupt our own state. (Note also that this is a theorem, not an assumption. If this reasoning is incorrect the proof will not go through.)

## Imperative programming + Functional programming + Separation logic = <3

The constructs of MMC are designed to simultaneously mimic a total functional programming language (like Agda/Coq/Lean), and also an imperative programming language like C. Compilers have long recognized that programs should be translated into static single assignment, where mutation becomes more like a let binding, and goto programs become mutually recursive functions. MMC uses a syntax that reliably lowers to both descriptions, so that you can write a program with C-like performance control and also Haskell-like semantic reasoning.

Simultaneously, separation logic is being manipulated by passing around ""hypotheses"" as if they were variables. The compiler handles the work of manipulating separating conjunctions so that the proof effort is focused on the tricky bits. (This is inspired by reading the [RustBelt](https://plv.mpi-sws.org/rustbelt/popl18/) project in reverse, where a Rust program is viewed as an ergonomic way of manipulating the separation logic semantics ascribed to it by RustBelt.) The MMC compiler will be like a borrow checker on steroids, because it is manipulating much more expressive proofs.

## A soft type system

Because of the presence of dependent types, type checking is undecidable (or more accurately, type checking is decidable but disproportionately likely to not accept something that can be proven okay). We embrace this using a soft type system. Types are really just separating propositions which have been handed to the type checker for safe-keeping. You can at any point steal a variable's type, giving you ownership of the typing predicate for the variable (and giving the variable itself a basic type that is duplicable). For example, if `x: own T` then you can use `typeof!` to change the type of `x` to `u64` and obtain a proposition `h: (x :> own T)` that asserts that `x` points to some data that is a `T`. You can then do arbitrary logic on this, perhaps proving that `x :> own U` instead, and then use the `pun` operator to re-animate `x` as `x: own U`, whereupon the type system will be able to infer that `*x: U` and so on.

## Fast compilation

Of course the compiler doesn't exist yet, but the compiler will be designed to be very goal directed and straightforward, so that I believe even large projects can be compiled and verified on a timescale of 1-10 seconds. Future tool integration may support calling out to SMT solvers and the like for the many simple proof obligations that MMC kicks up, but the point in the design space I am trying to hit is where proofs are simple but explicit, and the language has boilerplate-eliminating (not boilerplate-automating!) features so that both the user and the computer don't have to work so hard.

## The broader context

This language is being developed in service of the [MM0 project](https://github.com/digama0/mm0), which is a plan to build a minimalistic, practical, and blazing fast verification framework that is capable of proving its own implementation correctness. A major part of the project is the implementation of a binary verifier for the MM0 language, which is a [medium size C program](https://github.com/digama0/mm0/tree/master/mm0-c) (about 2000 lines), and so the MMC language was born as the proof and program input to make this happen. There are already exporters for translating MM0 proofs into other languages like HOL and Lean, and the MMC program verification framework is with respect to a comparatively weak axiomatic foundation, namely Peano Arithmetic, which means it can be embedded in basically every existing proof assistant.

## Contributing

MMC is not ready for users, but it is ready for armchair language designers like yourselves. (If you want to work on the compiler with me, please get in contact and we can co-author the eventual paper on this.) Please don't make comments about the lispy syntax, as this is subject to change (I've written 6 parsers already for the MM0 project and I'm not in a hurry to write a 7th). I'm not an expert on separation logic, but some amount of it is needed for efficient modular verification of the variable-passing proofs used in the present version of MMC. Please give a shout if you are a separation logic expert and see something that you think can't be implemented, as it would be bad if I find out later that certain core language features are unimplementable and the language has to be redesigned.","Too many goals in different directions IMHO
""Be fast ""  is the biggest failure.  Honestly, compile a week for a 1k program  and be sure its correct will be a huge step in quality."
What are some interesting language features that may not be well known?,8vcrzb,2018-07-02 04:22:54,"One that comes to mind is Go's `defer`, which allows you to postpone the execution a function until the very end of the current function.

Another cool one is Python's `while else` (or `for else`) control flow. The `else` allows you to execute code if the loop exited without breaking.

List comprehensions in Python and Haskell (or any functional language) are also very interesting.","In no particular order, off the top of my head:

* [Failure and generators in Icon](https://en.wikipedia.org/wiki/Icon_(programming_language\)#Generators). Basically, *every* expression can yield zero or more values, and as many are evaluated as needed.

* [Explicit interface implementation](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/interfaces/explicit-interface-implementation) is an odd corner of C#, but a really helpful one when you need it. It gives you a way to implement multiple interfaces that happen to have colliding method names.

* [Mixins in Dart](https://www.dartlang.org/articles/language/mixins). This is Dart's approach to multiple inheritance, based on [an approach by Gilad Bracha](http://www.bracha.org/mwp.html). A class can have a single superclass and as many mixins as it wants. Mixins are explicitly linearized. You can think of a mixin as a function that, given a superclass, returns a new class.

* [Full coroutines in Lua](https://www.lua.org/pil/9.1.html). Unlike generators in Python, C#, etc. a coroutine in Lua can yield anywhere in the stack and the entire stack is suspended. It's much more powerful and flexible.

* [User-defined number literals in C++](https://en.cppreference.com/w/cpp/language/user_literal) give you a way to not just implement your own units libraries, but given them first-class number literals too.

* [`inner()` in BETA](http://journal.stuffwithstuff.com/2012/12/19/the-impoliteness-of-overriding-methods/). Unlike other OOP languages where you *override* a method, in BETA, the *base class* method chooses when to call the subclass method, if at all. Neatly avoids all the ""don't forget to call `super()` at the right time"" problems of conventional overriding, and non-virtual methods also fall out of it naturally.

* [Non-local returns in Smalltalk](http://wiki.c2.com/?SmalltalkBlockReturn). Lots of languages these days have some sort of user-definable block-like syntax to enable DSLs that look like built-in control flow structures. But without non-local returns, the behavior of ""return"" is different in user-defined structures versus native ones. Non-local returns puts them on the same footing.

* [Prototypes in Self](https://en.wikipedia.org/wiki/Self_(programming_language\)#Prototype-based_programming_languages). Overall, I don't think I want to use prototypes in a real language I spent significant time in. But it's a brilliantly simple model for reuse.

* We take it for granted these days, but I always felt Java's ""one superclass many interfaces"" was a clean, elegant way to avoid the complexity of full multiple inheritance.

* The C preprocessor can be a really ugly tool, but man is it a *useful* one and can let you create really beautiful abstractions when used carefully."
Good languages for writing compilers in?,13eztdp,2023-05-12 04:59:18,"I'm trying to write a compiler and I've rebooted the project several times using different languages. I'm running on an Apple Silicon Mac and only interested in generating Aarch64 asm (and maybe RISC V later).

I've used mostly OCaml because I find it reliable and responsive but things like painful FFI and lack of full-width ints are a menace.

Mlton hasn't been updated in 2 years and Aarch64 supports looks alpha.

Poly/ML uses tagged 63-bit ints too.

C was great for my toy JIT but I don't want to rewrite expression trees in it.

I'd like my compiler's source code to be as small and simple as possible so Rust seems out because of the borrow checker and lack of pattern matching over RC'd trees and lack of tracing GC.

Has anyone tried Swift for this? What else is good?

Has anyone tried using term rewrite languages for this? I'm considering making my own just to see if it helps.","Personally, I can happily use any language with static types and garbage collection to implement a compiler. It's definitely more fun if the language has nice support for pattern matching, but not essential."
Modularity - the most missing PL feature,127h71c,2023-03-31 18:46:34,[https://www.pathsensitive.com/2023/03/modules-matter-most-for-masses.html](https://www.pathsensitive.com/2023/03/modules-matter-most-for-masses.html),"The Indiana version of `concepts` in C++0x could be used to implement modules *properly* in C++. This wasn't by accident: the core authors of the Indiana proposal were strongly influenced by Walid Taha and he was going through a ... phase ... with ML's (MetaOCaml, Template-ML, etc.).

His explanation of modules in one of our crappy classrooms was really eye-opening.

Bjarne did *not* like the use of `concepts` for modules — he just wanted them (concepts) to be predicates-on-parameters in the sense of implementing ""generics"". He got his way (torching my dissertation, along the way), and what we have is just syntactic sugar around the janky-ass Boost type-level predicate system.

Walid pointed out that — with discipline — you can use the Unix object file as a module. We can define regular-old-C-code and then late-bind that code (opaquely) to the API. The unit of interchange is then the object. Is it as nice as ML? HELL NO. Is it better than concepts-in-C++? YES."
Why don’t more new languages compile with GCC instead of LLVM?,11edmdj,2023-03-01 00:55:39,"I’ve been planning a hobby language for a while now, and although it’s a hobby language, I’d still like it to be a compiled language and have a nice optimizing compiler.

Initially I thought my only option, without manually writing a compiler for several different architectures, was LLVM, but I’ve recently been reading [GCC Tiny](https://thinkingeek.com/gcc-tiny/) and it actually doesn’t seem like too much work to compile a new language with GCC.

Edit 2: I suppose I should clarify that “not too much work” is *relatively speaking* lol. Working with compilers, or language implementations in general, is of course quite complicated.

Maybe it’s just my perspective and the use of GCC is more common than I think, but if not, is there any reason that LLVM is the common go-to?

Edit: Typo","If I had to guess, part of it is that there isn't a lot of press on using GCC as a backend when there are at least 2 recent languages that use LLVM as a backend (Rust and Swift).

Another part of it might be that GCC is a C++ project and a cursory glance didn't show me bindings in other languages while I know for a fact there are many LLVM bindings in other languages. So you'd be limited to C++, maybe C and Rust - I'm not sure how easy / ergonomic it would be to use the C++ API from those languages."
Writability of Programming Languages (Part 1),10uzi0q,2023-02-06 14:15:33,"Discussions on programming language syntax often examine *writability* (that is, how easy is it to translate ""concept to code""). In this post, I'll be exploring a subset of this question: how easy are commonplace programs to *type* on a [QWERTY](https://en.wikipedia.org/wiki/QWERTY) keyboard?

**I've seen the following comments:**

1. `camelCase` is easier to type than `snake_case` ([with its underscore]([https://www.reddit.com/r/ProgrammingLanguages/comments/10twqkt/do\_you\_prefer\_camelcase\_or\_snake\_case\_for/))
2. Functional languages' pipe operator `|>` is mildly annoying to type
3. Near constant praise of the ternary operator `?:`
4. Complaints about R's matrix multiplication operator `%*%` (and other monstrosities like `%>%`)
5. Python devs' preference for apostrophes `'` over quotations `""` for strings
6. Typing `self` or `this` everywhere for class variables prone to create ""self hell""
7. JSONs are largely easier to work with than HTML (easier syntax and portability)
8. General unease about Perl's syntax, such as `$name` variables (and dislike for sigils in general)
9. Minimal adoption of APL/BQN due to its Unicode symbols / non-ASCII usage (hard to type)
10. General aversion to codegolf (esp. something like `1:'($:@-&2+$:@<:)@.(>&2)`)
11. Bitwise operators `&` `|` `^` `>>` `<<` were so chosen because they're easy to type

In [this thread](https://www.reddit.com/r/ProgrammingLanguages/comments/ybjtax/glide\_data\_transformation\_language\_documentation/), Glide creator u/dibs45 followed recommendations to change his injunction operator from `->` to `>>` because the latter was easier to type (and frequently used).

Below, I give an analysis of the ease of typing various characters on a QWERTY keyboard. Hopefully we can use these insights to guide intelligent programming language design.

**Assumptions this ease/difficulty model makes—**

1. Keys closer to resting hand positions are easiest to type (`a-z` especially)
2. Symbols on the right-hand side of the keyboard (like `?`) are easier to type than those on the left-hand side (like `@`).
3. Keys lower on the keyboard are generally easier to type
4. Having to use SHIFT adds difficulty
5. Double characters (like `//`) and neighboring keys (like `()`) are nearly as easy as their single counterparts (generally the closer they are the easier they are to type in succession).
6. A combo where only one character uses SHIFT is worse than both using SHIFT. This effect is worse when it's the last character.

|Symbol(s)|Difficulty|Positioning|
|:-|:-|:-|
|`space` `enter` `tab`|1|largest keys|
|`a-z`|2|resting hand position|
|`0-9`|3|top of keyboard|
|`A-Z`|5|resting hand position + SHIFT|

|Symbol(s)|Difficulty|Notes|
|:-|:-|:-|
|`. , / // ; ;; '`|2|bottom|
|`[ ] [] \\ - -- = ==`|3|top right|
|`: :: "" < > << >> <> >< ? ??`|4|bottom + SHIFT|
|`{ } {} ( ) () \| \|\| _ __ + ++`|5|top right + SHIFT
|`* ** & && ^ ^^ % %%`|6|top middle + SHIFT|
|`$ # @ ! !! ~ ~~`|7|top left + SHIFT|

**Character combos are roughly as difficult as their scores together—**

|Combo|Calculation|Difficulty|
|:-|:-|:-|
|`%*%`|6(%%) + 6(*)|12|
|`<=>`|4(<) + 3(=) + 4(>)|11|
|`!=`|7(!) + 3(=)|10|
|`\|>`|5(\|) + 4(>)|9|
|`/*`|2(/) + 6(*)|8|
|`.+`|2(.) + 5(+)|7|  
|`for`|3 \* 2(a-z)|6|
|`/=`|2(/) + 3(=)|5|

\*This is just a heuristic, and not entirely accurate. Many factors are at play.

**Main takeaways—**

1. Commonplace syntax should be easy to type
2. `//` for comments is easier to type than `#`
3. Python's indentation style is easy since you only need to use TAB (no `end` or `{}`)
4. JS/C# lamba expressions using `=>` are concise and easy to write
5. Short keywords like `for` `in` `let` `var` are easy to type
6. Using `.` for attributes (Python) is superior to `$` (R)
7. `>>` is easier than `|>` or `%>%` for piping
8. Ruby's usage of `@` for `@classvar` is simpler than `self.classvar`
9. The ternary operator `?:` is easy to write because it's at the bottom right of the keyboard

I'd encourage you to type different programs/keywords/operators and take note of the relative *ease* or *friction* this takes. What do you find easy, and what syntax would you consider ""worth the cost"" of additional friction? How much do writability concerns affect everyday usage of your language?","The problem is that keyboards have a lot of variation between countries. You can't design only around US keyboards, or french keyboards, or whatever.

For example, look at lua's unusual choice of inequality operator: \~=

It's awful to type on a french keyboard such as mine, and probably not that great on an US keyboard either, but if you look at the Brazilian keyboard layout you can see plainly why the designer of the language made that choice: it's a lot easier to type on their keyboard than !=.

So optimizing for your own keyboard layout might result in quirky choices that will seem arbitrary and unhelpful to the rest of the world. This will make the language harder to use because it will not leverage people's habits (from other languages) as well.

For instance, as a french developer it would make sense for me not to use curly braces as they are an absolute pain to type on a french keyboard (you need two keys and the two curly brace delimiters are a thousand kilometer apart on the keyboard). However I might want to make use of the µ symbol for something or another, because it is on the other hand very easy to type on a french keyboard (the french keyboard layout fucking sucks btw)

But I won't do either of these things because it would just make my language harder to use by unnecessarily deviating from programing language norms, and ease of reading is also ultimately a lot more important than ease of writing."
What features would you want in a new programming language?,102r0ke,2023-01-04 10:13:59,"What features would you want in a new programming language, what features do you like of the one you use, and what do you think the future of programming languages is?",quotient types!
When is JIT Faster Than A Compiler?,wlzdk8,2022-08-12 03:04:36,,">An interpreter that significantly transforms your code or generates machine code tends to be called a compiler.

Correct. I approve this description."
Two pattern matching algorithms implemented in Rust,v1zaxh,2022-06-01 04:28:43,,"Last week I shared [this
post](https://www.reddit.com/r/ProgrammingLanguages/comments/uwyje4/ml_pattern_match_compilation_and_partial/)
about a pattern matching algorithm from 1996. Since then I spent some more time
looking into alternative algorithms, in particular [this
one](https://julesjacobs.com/notes/patternmatching/patternmatching.pdf). I spent
the last week implementing said algorithm in Rust, alongside two implementations
of the Sestoft paper.

I'm sharing this here as I figured others may find these implementations useful
in helping them better understand how pattern matching works. In particular,
I've explained various bits in pieces in the READMEs of the two implementations.

The sestoft1996 implementation is correct as far as I'm aware. The jacobs2021
implementation may need some additional testing, as I only _just_ finished
implementing the last bits and pieces (guards to be precise). I'm currently not
planning on implementing additional algorithms, but I may add more in the
future."
"Building a new .NET language, doing to C# what Kotlin did to Java",sw9dag,2022-02-19 20:50:57,"A while ago we have started to plan out a new language, primarily targeting the .NET platform. We have been using C# for our projects and while we believe it has advanced way faster than Java did in terms of feature set, it also needs its own Kotlin in the ecosystem to get rid of some annoyances and bring in long-needed features.

The language is still in the early design stages. We have a stronger and stronger idea each day of what we really want, but we want to be careful with each decision we make. If you are interested and would like to contribute or write down your ideas, please do so as an [issue in this repository](https://github.com/LanguageDev/Fresh-Language-suggestions)! We even welcone love/hate/miss/wish lists of (primarily .NET) languages, as we would really like to know what each developer loves already, what they miss or what they might hate, when using something like C#.

(We also have a Discord server, if you'd like to join and discuss ideas there)","Have you looked at some other languages for .NET?

In particular, [Nemerle](http://nemerle.org/About) is quite close to C#, has powerful meta-programming capabilities, and has been around for a while.

Even if Nemerle is not suitable for your needs, it's worth asking; why does it not have widespread use? What are you doing differently which will convince C# developers to look at and use your language?

Perhaps Nemerle was just too ahead of its time."
The past and present of Futhark,rrdnm6,2021-12-30 01:35:04,,This is one of the most exciting projects out there!
Discover ‘deep’ programming language bugs with Xsmith,mgfadv,2021-03-30 20:50:45,,"Isn't this called ""fuzzing"" ? I'm not really familiar but I thought thats what this is"
I made a state machine compiler,m8rk8d,2021-03-20 05:27:23,"Check out a demo here: [https://clnhlzmn.github.io/Makina-demo/](https://clnhlzmn.github.io/Makina-demo/)

Source code here: [https://github.com/clnhlzmn/makina](https://github.com/clnhlzmn/makina)","cool, do you have a specific application in mind?"
A language design for concurrent processes,l1m4wr,2021-01-21 08:02:09,"I found an interesting language near the bottom of the pile of forgotten languages. Compel by Larry Tesler (RIP) from 1968. I thought it only fitting to announce it here.

[A language design for concurrent processes](https://www.computer.org/csdl/pds/api/csdl/proceedings/download-article/12OmNBtCCA6/pdf) (PDF)  


>Compel was the first data flow language. This paper introduced the single assignment concept, later adopted in other languages.

[Wikipedia](https://en.m.wikipedia.org/wiki/Larry_Tesler#Early_career) says:

>This functional programming language was intended to make concurrent processing more natural and was used to introduce programming concepts to beginners.

The 1996 thesis [A parallel programming model with sequential semantics](https://apps.dtic.mil/dtic/tr/fulltext/u2/a450812.pdf) (PDF) says:

>In 1968, Tesler and Enea described the use of single-assignment variables as a sequencing mechanism in their parallel programming notation, Compel. In Compel, the single-assignment restriction enables automatic compile-time scheduling of the concurrent execution of statements.

And I have to add that I like its use of `:` for assignment. Here's a taste:

    input;
    out: (a - e) / d;
    a: 6;
    e: a * b - c;
    d: a - b;
    b: 7;
    c: 8;
    output out;",Pay no heed to the bot. Nice post!
Interview with Crystal language creators,jqzkof,2020-11-09 23:25:57,,"Continues to look very good!

Edit: Also, very cool that this isn't something coming out of one of the predictable Bay Area mega-corps. I've suggested Crystal to a number of people interested in learning a new language instead of e.g. C/C++ or Go."
Beyond Hindley-Milner (but Keeping Principal Types),ijij9o,2020-08-31 03:14:56,"**Resource Post**

A lot of work has gone into type inference over the decades, and I believe the sweet-spot of Hindley-Milner (Damas-Milner?) type inference has been made even sweeter with some advances that I'm 95% sure play nicely together. I find myself motivated to share those resources here, since they've been incredibly helpful in designing and implementing my own language, and I'll include why I believe these extensions are worthwhile. A lot of modern languages have at least one or two, sometimes more.



Maybe this can start a small discussion about the state of type inference, but I'm also posting this as a collection for posterity. Y'know, in case anyone else is as possessed by principal types as I am and wants a path to get started adding advanced type inference in their own language.



Papers I rely on:

* [Type Inference, Haskell, and Dependent Types](https://adam.gundry.co.uk/pub/thesis/thesis-2013-12-03.pdf) - Despite the title, the biggest thing I gained from Gundry's thesis is something called 'statements-in-context'. It basically puts type constraints inside the typing environment in a way that makes dependencies clear. He goes on to show how this supports type inference for units of measure, and other extensions based on 'Abelian-unifying types' (limited integer constraints? general 'tagging' of types? dividable phantom types?). **This type inference algorithm style is now the core of the inference algorithms I've been writing.** It's almost like a small-step operational semantics, which is usually easier for my brain to thinking about compared to big-step declarative algorithms.
* [Typing Haskell in Haskell](https://web.cecs.pdx.edu/~mpj/thih/thih.pdf) - There is a dearth of good resources showing to do type inference with Haskell-style ad-hoc overloading. This paper shows how, and although it may require some Haskell knowledge to work through it, I found it crystallizes the essences of qualified types. The way Jones separated of the handling of predicates from other aspects of constraint generation/solving made it very straightforward to combine with Gundry's algorithm. Which means we should be able to get ad-hoc overloading and abelian-unifying types into the same type system!
* [Extensible Records with Scoped Labels](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/scopedlabels.pdf) - Easy-to-use records make me feel at home in a programming language. Daan Leijen shows how to extend the type-unification portion of HM to achieve extensible 'scoped' records. Since it's mostly a unification change, it's again fairly straightforward to port to a Gundry-style inference algorithm. Scoped labels may not have many uses in HM-inferred languages, but the simplicity of the extension, both for understanding and implementation, makes it highly attractive. And you get extensible scoped variants basically for free!
* [Koka: Programming with Row Polymorphic Effect Types](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/paper-20.pdf) - Lucky Daan, he gets to show up twice! This one basically shows how to apply the row-polymorphic insights of his records paper to 'effect types'. If you can implement the system from 3, this one is not too different. Some later papers that Daan co-authored, less focused on type inference, show that these effect types are rich enough to represent user-defined algebraic effects.

In all, these four papers extend classic HM inference with:

* Abelian-unifying types (for units of measure, limited compile-time integer constraints, etc.)
* Qualified types (good for Haskell-style ad-hoc overloading)
* Extensible records and variants
* Effect types rich enough to handle a powerful form of user-defined algebraic effects. Most of the benefits of monads, with less of the... monads

Since I suspect these extensions combined together still have principal types, one could use all of these features without ever having to write down a function type signature in the source code. Although that's likely bad practice, it's very cool to know the compiler can do it.



I have honed in on these four extensions for three reasons:
1. I believe they generate principal types that remain easy to understand in their most frequent use cases, certainly for programmers already familiar with ML-descended languages.
2. Each type extension adds more expressive power in the both the term and type languages.
3. They aren't 'too forgiving'. One user-experience problem I've encountered in a few of the richer type inference extensions is that 'weird code' starts to become well-typed, or simple code starts to get 'weird types'. Usually polymorphism is the culprit.



As a last note, the recent thesis by Stephen Dolan on principal type inference for algebraic subtyping was a worthy breakthrough, with a [nice overview here](https://lptk.github.io/programming/2020/03/26/demystifying-mlsub.html). I am currently uncertain whether it plays nicely with any of the extensions mentioned above, and by [Dolan's own admission](http://lambda-the-ultimate.org/node/5393#comment-93539), the generated principal types may be susceptible to the 'too forgiving' problem I mentioned above.



I hope you find these resources useful. If there's any other extensions you find interesting that I haven't covered here, I'd love to hear about them!","It sounds like you may also want to look at algebraic subtyping, a way of inferring compact principal types in the presence of subtyping.

I wrote [an ICFP Functional Pearl paper](https://infoscience.epfl.ch/record/278576?ln=en) on it, containing a simplified exposition and implementation of the approach. My implementation is [available online](https://github.com/LPTK/simple-sub) complete with a web-based [demo](https://lptk.github.io/simple-sub/).

I tried to make a good job of motivating subtyping in the context of functional programming in [the corresponding ICFP talk](https://www.youtube.com/watch?v=kZ0q9RGW21I&t=790s), which I recommend watching as introductory material.

I've also been planning to show how to extend the system with row polymorphism (which is surprisingly easy and makes row polymorphism even more pleasant), but it seems /u/Uncaffeinated already beat me to it [in his own implementation's blog series](https://www.reddit.com/r/ProgrammingLanguages/comments/iiu8s9/subtype_inference_by_example_part_9_match/)."
"""Unlike ordinary JIT compilers for other languages, Ruby’s JIT compiler does JIT compilation in a unique way, which prints C code to a disk and spawns common C compiler process to generate native code.""",a60i0m,2018-12-14 10:26:53,,"Gross. If I implemented JIT compilation that way, I would not be telling people about it."
"Capy, a compiled programming language with Arbitrary Compile-Time Evaluation",16cs6js,2023-09-08 06:09:17,"For more than a year now I've been working on making my own programming language. I tried writing a parser in C++, then redid it in Rust, then redid it AGAIN in Rust after failing miserably the first time. And now I’ve finally made something I'm very proud of.

I’m so happy with myself for really going from zero to hero on this. A few years ago I was a Java programmer who didn’t know *anything* about how computers really worked under the hood, and now I’ve made my own low level programming language that compiles to native machine code.

The language is called [Capy](https://github.com/capy-language/capy), and it currently supports structs, first class functions, and **arbitrary compile-time evaluation**. I was really inspired by the Jai streams, which is why I settled on a similar syntax, and why the programmer can run *any* arbitrary code they want at compile-time, baking the result into the final executable.

Here’s the example of this feature from the readme:

```
math :: import ""std/math.capy"";
    
powers_of_two := comptime {
    array := [] i32 { 0, 0, 0 };
    
    array[0] = math.pow(2, 1);
    array[1] = math.pow(2, 2);
    array[2] = math.pow(2, 3);
    
    // return the array here (like Rust)
    array
};
```

The compiler evaluates this by JITing the `comptime { .. }` block as it’s own function, running that function, and storing the bytes of the resulting array into the data segment of the final executable. It’s pretty powerful. log10 is actually implemented using a comptime block (`ln(x) / comptime { ln(10) }`).

The language is missing a LOT though. In it's current state I was able to implement [a dynamic String type stored on the heap](https://github.com/capy-language/capy/blob/master/examples/string.capy), but there are some important things the language needs before I’d consider it fully usable. The biggest things I want to implement are Generics (something similar to Zig most likely), better memory management/more memory safety (perhaps a less restrictive borrow checker?), and Type Reflection.

So that’s that! After finally hitting the huge milestone of compile-time evaluation, I decided to make this post to see what you all thought about it :)",This is so sick dude!
Announcing Dart 3,13e5ilp,2023-05-11 06:31:36,,"Dart has come a long way since version 1, I still vaguely remember how Dart 1.x had a pluggable type system and was supposed to be a better JS. /u/munificent and his colleagues did such great work to make a neat and powerful programming language. Hopefully it can get more exposure and appreciation from developers. Of course most people writing code in Dart are doing this for Flutter as it is now, but it will be really nice to see Dart being used in other areas as well. The backend seems to be a good candidate, and maybe web assembly too."
Everything I wish I knew when learning C,z6y21r,2022-11-28 22:31:40,,"> Python only has one integer type, which is a signed bigint. Compared to C/C++, this renders moot all discussions about bit widths, signedness, and conversions – one type rules all the code. But the price to pay includes slow execution and inconsistent memory usage.

Popular common lisp implementations demonstrate that this is false; you can have coherent semantics and good performance.

> Java is built heavily around the 32-bit int type, especially for counting arrays. This means Java can’t run efficiently on underpowered 16-bit CPUs (common in embedded microcontrollers), and Java can’t directly address large arrays on 64-bit systems. By comparison, C/C++ makes it possible to write code that runs efficiently on 16-bit, 32-bit, and/or 64-bit CPUs, but requiring a great deal of care from the coder.

Java could have indexed arrays using a 'pointer-sized' integer type analogous to c's size\_t or ptrdiff\_t, while still having a fraction of the pitfalls of the latter.  I therefore find the restriction a bit silly.  That said, 'directly' is doing a lot of heavy lifting here; you _can_ address large amounts of data by segmenting, and, given the realities of caches and TLBs, are unlikely to see a performance deficit for most problems."
"Vale 0.2 Released: Higher RAII, Concept Functions, Const Generics, FFI, Modules, Faster Compiles, set Keyword",uoap02,2022-05-13 04:45:43,,"Using `a = 1` as the syntax for declaration and having a longer syntax for assignment is really interesting. It makes perfect sense to give the shorter syntax to the more common, safer operation. At the same time, it's so unusual that I wonder if the unfamiliarity tax will be too high."
On low-effort poll posts,jmaj0n,2020-11-02 05:32:27,"Recently, there have been a lot of fairly low-effort posts including polls[^1](https://redd.it/jintn9) [^2](https://redd.it/jlnmzf) [^3](https://redd.it/jm64or).  These posts don't generally lead to good discussion.  I propose that they be banned; or, at least, allowed only with caveats.

There are two problems:

1. Polls are not a good way to solicit opinions from people.

2. Posts in this community which use polls tend not to include enough information to get effective feedback.

---

# 1

All the poll posts have concerned design choices in a language presumably created by the post's author.  That is, the only person who's deeply invested in the language's design is the post's author.  Languages like C and Haskell are designed by a commitee all the members of which are deeply involved in the language; they have strong opinions, and they have _reasons_ for having those opinions.  Answering a poll is thus a meaningful expression of those opinions.  When somebody makes a poll post on reddit, the language in question is generally not even mentioned (and I would venture, doesn't even exist).  Respondants don't have good reasons to select different responses to the poll because they don't have enough information or personal investment to do so.

_Comments_ by outsiders can be helpful when deciding on design aspects of a language or implementation.  But **someone who's not deeply invested in a language can't meaningfully express an opinion just by choosing an option on a poll**.

---

# 2

The second problem—which, granted, is more incidental than fundamental—is that the poll posts tend to be **low-effort**.  They're generally about design, which is highly language-specific, but they don't give enough details for anyone to respond meaningfully.  Something like ‘[Garbage collector or not?](https://redd.it/jm64or)’  This was the entirety of one of the recent poll posts.  How is anyone supposed to give a meaningful response to that?  Answer ‘only on Tuesdays’?

This applies not only to poll posts, but in general to posts that solicit opinions, but the situation seems to be particularly bad for poll posts.  **No one can give you feedback on a piece of design unless you provide information about _how_ that design fits into a larger whole.**

---

So I put it to you: what do you think should be done about poll posts?

[View Poll](https://www.reddit.com/poll/jmaj0n)","I've disabled polls until further notice. The points laid out in the post are all valid, *and* polls are an issue on old reddit - which is used by 30% of our desktop users, and, as far as I know, 100% of our mods."
Design Flaws in Futhark,ecc1fo,2019-12-18 20:34:33,,I love the transparency here in talking honestly about your own mistakes.
IntercalScript: A satirical self-hosted language with alegbraic subtyping,e84bqd,2019-12-09 11:33:09,,"> Linux Installation :

// body 
> Windows Installation:

Get a better OS , then see above 


Lol"
Introducing Ripple: A language for exploring links and connections via side effects,12ktce4,2023-04-13 23:43:07,"Ripple (name unconfirmed) is a new PL I've been designing that focuses heavily on side effects in pursuit of exploring relationships and connections between entities.

Ripple is open source, you can check out the repository here: [Ripple on Github](https://github.com/dibsonthis/Ripple)

Below is a basic Ripple program:

    var length, area, diff = 0
    
    length::onChange = () => {
        area = length ^ 2
    }
    
    area::onChange = (old) => {
        diff = area - old
    }
    
    for (1..10) {
        length = length + 1
        print(""L: "" + string(length) + 
              "" - A: "" + string(area) + 
              "" - D: "" + string(diff) + ""\n"")
    }

The way it works is pretty simple.

We simply define functions that can fire whenever specific variables change. I'm calling these ""hooks"" for the time being. (I want to keep this general, in case I add more hooks later down the line. Currently only the `onChange` hook is implemented)

In the above code there are 2 hooks, one for `length` and one for `area`. Whenever length changes, it updates area's value, and whenever area changes (as a side effect, or ripple, of the first hook), it updates diff's value.

The above code then loops through and updates only length. The rest of the updates happen automatically due to the hooks we implemented.

This is a printout of the results:

    L: 1.000000 - A: 1.000000 - D: 1.000000
    L: 2.000000 - A: 4.000000 - D: 3.000000
    L: 3.000000 - A: 9.000000 - D: 5.000000
    L: 4.000000 - A: 16.000000 - D: 7.000000
    L: 5.000000 - A: 25.000000 - D: 9.000000
    L: 6.000000 - A: 36.000000 - D: 11.000000
    L: 7.000000 - A: 49.000000 - D: 13.000000
    L: 8.000000 - A: 64.000000 - D: 15.000000
    L: 9.000000 - A: 81.000000 - D: 17.000000

Ripple is still very much a work in progress, but the repo can be found here: [Ripple](https://github.com/dibsonthis/Ripple)

**Important Note:** Yes, I know side effects may be seen as an anti-pattern, and I am fully aware that this may be a bad idea in many situations. But I wanted to play around with the concept and see what interesting stuff I (or the community) can come up with.

Also, I got pretty demotivated working on languages with the hopes that they may be adopted and used in production, and therefore have to implement all the good things like type safety etc. This language here is just for fun and to keep my sanity in check.","This language vaguely resembles rule based languages, did you take some inspiration from things like Prolog?"
GingerBill and Casey Muratori on LLVM compile times,105xd1z,2023-01-08 03:29:30,,"In the linked Twitter thread u/gingerbill says (about the feasability of not using LLVM but something else):

> The complex part of LLVM is already solved. It's just the passes and if you have an SSA based optimized, then you can effectively copy the passes from the general literature (not necessarily LLVM).

I'm not a compilation expert, but this sounds very naive to me. I would take this whole discussion with a grain of salt.

LLVM is the result of years of efforts by many people and, sure, maybe some of this effort was misguided or wasted on bad design, but getting good results in this space *probably* requires more than deciding on a couple passes and copying code from ""the general literature"".

I'm willing to believe the idea that you can probably get something ""good enough"" with much less work, and get a system that is more convenient in many ways. But even for such a ""good enough"" system -- optimizing more for simplicity and less for absolute performance -- I would hazard the guess that it takes a lot of effort to get to a satisfying place, despite the existence of ""general literature"" on individual compilation or optimization passes.

When people criticize complex systems and explain that it would be fairly easy to do something much better, sometimes they are right, but most of the time they are overconfident and wrong. They sound just like this."
Alternatives to borrow checking?,yd1g1s,2022-10-25 18:31:21,Most of you probably know at least something about rusts borrow checker and how it makes memory management easy without the need for a slow(ish) garbage collector. Are there any alternative rule sets that make it so that reference counting is enough for memory management? Pythons hybrid reference counting and GC approach doesn't count.,"Lobster uses [compile-time reference counting](https://aardappel.github.io/lobster/memory_management.html), meaning you have the semantics of reference counting, but *almost* all of it is inlined and optimized to the point where no counter exists at runtime and objects even live on the stack

See [this tweet](https://twitter.com/wvo/status/1329825799571075074) for links to a presentation on the topic (slides + recording)"
What's a good general-purpose programming language?,uc9hsl,2022-04-26 18:33:55,,"Thanks for getting the ball rolling (again) on helping people to understand why the number of programming languages on offer keeps growing and growing and growing.

There are so many permutations of design choices that are clearly and self evidently correct to some people, and not at all to others.

Have you noticed how often we talk past one another without an ounce of curiosity, insight, understanding or wonder?

And so it goes!"
A Flexible Type System for Fearless Concurrency,tyiztq,2022-04-08 02:09:52,,"Very cool! The world needs more innovation on this front, there's a lot of potential here. This is a very thorough and in-depth work! They've clearly been working on this for a long time, and with some really interesting techniques.

What they describe is also similar to Vale's [Seamless, Fearless, and Structured Concurrency](https://verdagon.dev/blog/seamless-fearless-structured-concurrency) in that it's using regions to assist the type system with fearless concurrency, though their approach has less of an emphasis on giving multiple threads safe simultaneous access to data, and more emphasis on Pony-style `iso`lated subgraphs. Their ""dominating"" reference is akin to Vale's `uni` `iso` reference.

If the authors can see this: I know that one of the benefits of this is the lack of annotations, but if you're ever willing to bend on that, there's a lot of potential in combining `iso` with a region borrow checker as shown below. An `iso` region is completely separate, no objects pointing in or out. We can relax that restriction and make it so outside objects still can't point inside, but inside objects _can_ point outward. We would track which references point outward with generic region parameters:

```
struct MySubgraphObject<'a> iso {
  a_member_in_this_region: SomeObject;
  a_member_pointing_outside: 'a OtherObject;
}
```

...and we would use it from the outside like `MySubgraphObject<'this>` where `'this` refers to the current region.

It's similar to Rust's lifetimes, but applied to regions instead of objects, and without an aliasability-xor-mutability restriction which precludes the usefulness of a lot of references. I think Rust gives these annotations a bad reputation; they're actually very easy to use when there's no aliasability-xor-mutability principle.

I think this could be a very useful mechanism to bring into languages; it can allow us to make use of region-based static analysis without precluding references between regions."
Qi - A programming language designed to be written in Chinese,ssw831,2022-02-15 14:15:49,"After seeing how almost all computer languages nowadays are based on english, I decided to make my own language (Qi) that would allow you to program solely with the pinyin keyboard.

You can check out the project [here](https://github.com/AnonymousAAArdvark/qi) ([https://github.com/AnonymousAAArdvark/qi](https://github.com/AnonymousAAArdvark/qi)).

Also, a star to the project would be **greatly** appreciated!","FYI: There is already a programming language named Qi: an interesting
functional language that combines Lisp with ideas from the ML family of
languages.  It was released in 2005 and abandoned in 2009, and the next
iteration of the language is called Shen, which defines a minimal ""kernel""
that allows the language to be easily ported to many different
platforms/runtimes.

I am just commenting to let you know in case you are unaware of the
unfortunate name collision.

References:

* <https://en.wikibooks.org/wiki/Software_Engineers_Handbook/Language_Dictionary/Qi>
* <https://www.cliki.net/qi>
* <https://shenlanguage.org/>
* <https://shen-language.github.io/>"
Are there any interesting programming languages based on particular data structures?,lliyuo,2021-02-17 09:37:29,"Lisps are notorious for being very list-centric, both in 
 implementation, and features, and get a lot of power from this idea.

Forth is built around a single stack. This, and the way it executes, opens it up to being concatenative.

Prolog (or a naive implementation of it) is essentially an interface to the disjoint set data structure with backtracking. (Yeah this one might be a bit of a stretch)

Are there any other languages that get a lot of mileage out of basing themselves on a particular data structure?

Edit: Another possible example that comes to mind is how you can get a lot of Javascript's early features from the ""objects/dictionaries with prototypes"" data structure.","I'd say almost all good languages have this form of compositionality.  Basically ""everything is an X"":

https://lukeplant.me.uk/blog/posts/everything-is-an-x-pattern/


- Shell and Tcl: everything is a string.
- C: ""everything"" is a pointer + offset.  structs and arrays are just syntactic sugar around these extremely simple calculations. 
 Noting that `a[i]` vs. `i[a]` are **defined** to be the same may help you see that.
- Lua: the authors explicitly state that there's exactly one compound data structure (the table), one unit of code (function), and one abstraction of control (coroutine)
- Python: less pure, but dictionaries play a huge role, and so does the meta-object protocol.  Everything is an object (including dictionaries!).
- R: data frames (and vectors).  Lack of scalars is annoying.
- Matlab: matrices
- Mathematica: M expressions I think?  There is an important evaluation difference with S expressions
- SQL: tables.  Lack of scalars is annoying, but it composes.

This isn't a binary all-or-nothing thing, but it does improve compositionality to have fewer concepts.

I plan to have some blog posts about this in the future, e.g. related to  this thread about OS design.  File descriptors and opaque strings are the ""single type"" of Unix (and plan 9).  Fewer types can be more compositional.

https://lobste.rs/s/izpz2n/on_missed_opportunities_static_types#c_pbfxpy

----

edit: Also this is why I say that shell is the *least common denominator* of languages.  It deals with bytes, and all 
those data types can be serialized as bytes.  And they HAVE TO BE in a distributed system, one way or another.  (JSON has become another narrow waist between languages, and that's why a shell should also grow JSON support.)

Shell bridges different paradigms.  The X can turn into a Y and then you compute in the Y domain, or on a remote machine that knows how to deal with Y.  It's the language of heterogeneity."
"Star: An experimental programming language made to be powerful, productive, and predictable",lak510,2021-02-02 09:34:27,"https://github.com/ALANVF/star

For the past 2 years, I've been working on a programming language called Star.

My main goal has been to create a language that's completely consistent without making the rest of the language a pain to work with. I wanted to achieve consistency without putting inconvenient barriers in language in order to remove ambiguity and edge cases. Instead, I started from scratch in order to fix the mistakes I see far too often in languages today. Maybe this means that I simply change `==` to `?=`, use ""alien syntax"" for type annotations, or just flat out completely redesign how generics work. Maybe this means that I introduce variable-length operators that makes code visually self-documenting, or that I use a very different syntax for character literals. Whatever the case may be, it was all for the sake of keeping the language consistent.

This might sound like a bit of a stretch so far, but please just stay with me for a bit longer.

One of my absolute favorite languages of all time is [Raku](https://raku.org/). Not because it has absolutely everything (although that's an added bonus), but that it's very consistent despite having an overwhelming amount of language features. Raku is definitive proof that a language can be feature-rich without being impossible to learn a complete disaster in general, and that's something I really admire.

I often get remarks about ""seemingly useless"" features in Star like (nested) cascades, short-circuiting xor and ""nor"" operators, and pattern matching on classes. My reasoning has always been that I've never seen a reason *not* to have these kinds of features. Why *shouldn't* we have a ""nor"" operator, which would end the debate between `!(a || b)` and `!a && !b`? When would it be *inconvenient* to be able to pattern match on an instance of a class? Why *can't* variants inherit from other variants? It's important to consider *all* use cases of these features rather than just your own use cases. The more we use and spread new ideas like these, the easier it'll be to determine just how useful they actually are. Simply writing them off as ""wow imagine having `--------->` in your code lol"" doesn't really benefit anyone.

Any feedback on this project would be appreciated. Thank you.","the syntax feels really clean to me, nice work.  in particular the lack of commas in function/method calls -- it reminds me of objective-c but with the brackets in a more convenient place.  looking forward to seeing it implemented!"
Why isn't design by contract more common?,l238o5,2021-01-22 01:34:01,"I am thinking about making a language where writing bugs is as hard as reasonable, and I am currently in the early research phases of this project.

I think there would be a lot of benefit in explicitly writing down all the preconditions and post conditions in the code, in a way the compiler can understand. The current way of writing docstrings above each function with input-output behavior seems like a poor man's design by contract.

Why isn't doing this more common?","There are a few reasons that I'm aware of.

Firstly, a lot of problems just aren't easily expressible as a contract. What would the contract for, say, a parser look like? Let alone actual real-world problems that have to deal with often-extremely-messy real-world state. How can we model the state of the console, or the state of the API requests sent to my credit card provider (note the difference between that, and *the state of my account*, which may or may not contain information about credit card)?

Even within domains that are amenable to being modeled with contracts, fully specifying a program is *hard*. To return to the parser example, one easy spec might be:

```
parse: REQUIRE matches_grammar(s), ENSURE true
```

We can't prove anything about the output of the `parse` function, so we give a trivial postcondition (I'm assuming that we're layering contracts on top of some reasonably-featured type system, like Rust's or Elm's). To some extent this is by design -- this forces us to add extra logic to the call sites of any downstream functions, which will almost certainly have nontrivial preconditions, so that's fine. But what if we find ourselves needing to parse non-arbitrary input, say, from a verified (but opaque) external source, so that we know that the output has certain qualities? Now we either need to write new conditions on `parse`, or we eat a runtime penalty due to checks that we know will pass (the compiler doesn't know anything about our trusted input).

The final problem is that statically-checked contracts become unmanageable *really quickly*. This is related to the above problem, in that writing precise contracts often involves encoding huge amounts of domain-specific data (or even implementation details), as incomplete contracts can cause code to stop compiling. I don't know if you've ever looked at CompCert, but its proof of correctness is [100,000 lines of Coq](https://www.absint.com/compcert/structure.htm) *in addition* to the actual source code, produced over six person-years of work. For a lot of users, this is just an unacceptable amount of overhead.

That said, not all is lost! There is research being done into [gradual verification](http://www.cs.cmu.edu/~aldrich/papers/vmcai2018-gradual-verification.pdf), which lets you partially specify a program statically and have the compiler insert runtime checks where necessary (think gradual typing, like the `dyn` type in C#). Last I checked, there is [a backing theory](https://2020.splashcon.org/details/splash-2020-oopsla/104/Gradual-Verification-of-Recursive-Heap-Data-Structures) for doing this in mainstream, Object-Oriented languages like Java, so we might see this become easier in the near future."
Is there a gap between how developers think compilers work and how they actually work?,j0uncm,2020-09-28 01:05:56,"I've read somewhere (I was convinced it was on the LLVM website, but apparently it's not) that there is a huge gap between how most developers think compilers work and how they actually work. Do you think that's true?

I think it is somewhat true. When I was doing competitive programming, some competitors blamed the compiler for a program which didn't include any new or rarely used language construct apparently misbehaving. Now that I've developed two compilers for my programming language, I understand that's a silly suggestion. Compiler bugs are not triggered by programs containing only pointers, loops and other often used language constructs, they are nearly always triggered by using experimental features of the compiler. And, when they are triggered, they almost never result in a program that's misbehaving, they usually result in compiler refusing to compile valid code, and sometimes they result in compiler crashing or producing invalid assembly which is rejected by the assembler.

Still, I think most programmers understand compilers are programs, which, like all programs, have bugs. Moreover, I think most programmers have good intuitions which grammars are plausible as grammars for programming languages and which are not. I think most programmers intuitively understand why the [fictional programming language of Jurassic Park](https://scifi.stackexchange.com/questions/135417/what-can-we-infer-about-the-programming-language-used-in-jurassic-park-the-bo) is rather unlikely to be a real programming language, even though they wouldn't know to build a parser for a real-world programming language themselves.

I was wondering what you think.","I think there is a phenomenon of inexperienced people blaming the compiler (or interpreter) for their programs not working.

It's hard to remember now, but a long time ago I would think the same thing.  Maybe back in high school or college.

What most likely cured me of that is unit testing and learning how to debug (and unfortunately I ""programmed"" for a long time without really knowing how to debug).

When you know how to isolate a bug, then the bug rarely ever gets isolated to the compiler, and you stop thinking about that so often.

----

That said I have been tickling some features like this in C++ recently -- not compiler bugs, but language bugs essentially:

https://old.reddit.com/r/cpp_questions/comments/j0khh6/how_to_constexpr_initialize_class_member_thats/"
Optimization · Crafting Interpreters,fvldlp,2020-04-06 05:05:29,,"This is really interesting: I also recently rediscovered NaN Boxing, going through the exact same train of thought as represented here. I sadly didn't manage to find enough resources on this topic to finish an implementation."
Introducing rudra - A dynamic general-purpose high-level functional-programming language with familiar syntax that compiles to native binaries,wz3pii,2022-08-27 22:05:44,"Hi all!

I wanted a high level dynamic functional language like Clojure - but with more traditional Algol-like syntax and compilation to native binaries - and came up with [rudra](https://github.com/divs1210/rudralang).

I wanted the language to have the following properties:

- **Ergonomic** familiar syntax, destructuring everywhere
- **Extensible** top-level functions are polymorphic by default
- **Immutable data structures** by default
- **Concurrency-friendly** mutability using Clojure-like [`atom`s](https://clojuredocs.org/clojure.core/atom)
- **Full numeric tower** examples: no integer overflows, `pow(-1, 0.5)` is `0+i`
- **Recursion-friendly** many algorithms are simpler when defined recursively - they should be written as such

I haven't found the time to work on it for a while, so I thought it would be better to put it in the public domain in its current form to get some feedback.

Please let me know your opinions on it!

Thanks!",Wow I'm impressed how short the implementation is! How did you do native code generation?
Modules: Overcoming Stockholm and Duning-Kruger,vqx19e,2022-07-04 10:24:35,"Graydon Hoare's (u/graydon2) provocative [""What Next?""](https://graydon2.dreamwidth.org/253769.html) post claims: ""most languages have module systems with serious weaknesses.""  He goes on: ""Many languages have no module system at all, and many that have one have it only as a way of (say) managing namespaces or compilation order.""

He lays out his goal posts here:  ""there's a bewildering array of design constraints to navigate (generativity, opacity, stratification, coherence, subtyping, higher-order-ness, first-class-ness, separate compilation, extensibility, recursion) when arriving at a practical, usable module *system.*"" This is a great list if you know what all these design constraints are, but Graydon provided no glossary nor bibliography (other than two links). I was not familiar with most of them.

With some research, I have acquired this rudimentary understanding:

* Generativity.  As with types, should modules be given parametric polymorphism (generics) and, if so, how?  Should generation act like functors (which instantiate newly everytime) or generics (which instantiate based on unique parametric values, or even possibly less often where the generated code is provably the same)?
* Opacity.  I think this relates to encapsulation notions of public vs. private, except applied to records of types instead of values. Transparent means the types are visible from outside, opaque means they are not, and translucent represents a mix.
* Stratification. This suggests layers, but of what?  Given we have higher-order-ness as a separate constraint, I am going to speculate this relates to whether modules can contain modules, and so on, whether by definition or by dependency (typically in the form of a DAG).
* Coherence. For effective modularity, we want modules to exhibit high intra-module coupling and low inter-module coupling, which is also termed high coherence. Given this is normally assessed against a designed system, I am not clear what features we would expect of a module system that make it easier for a designer to arrive at high cohesion naturally.
* Subtyping. As again with types, should modules support subtype polymorphism (e.g., traits/interfaces) and if so, how?  Static dispatch, virtual dispatch (vtables), message-passing, message-queuing?  Nominal vs. structural? 
* Higher-order-ness. Just as there are higher-ordered types (kinds of types ala Haskell), should there be higher-order modules, and what should that mean?
* First-class-ness. Generics and existentials allow us to specify types as parametric values, making types first-class in the sense that we are passing them as values. If we support generic modules, will we also allow modules as parametric values?  This is something that 1ML supports carefully, as paradoxes lie in the wings if you are not careful.
* Separate compilation.  What is the relationship between modules and compilation units.  In D and Rust, a module is no larger than a source file, and that creates some interesting stitching challenges. Other languages handle it differently, allowing multiple source files to be considered part of the same module/namespace.
* Extensibility.  I believe this is related to the expression problem: can modules be extended beyond their original definition, and in what ways?
* Recursion.  Recursion introduces cycles. If they are not [weakened](https://pling.jondgoodwin.com/post/weakening-cycles/), we introduce paradoxes. We have ways to weaken cycles in recursive functions and types.  Do we allow recursive modules (modules that can refer to each other in a cycle vs. DAG), and so, how do we weaken them?

I am looking for feedback and discussion, particularly:

* Please refine and improve my understanding of any of the above
* Outside of research & academic interest, which of these features would be a valuable addition to a commercial programming language, because its value outweighs the added complexity. I can see ways that generic modules and module subtyping might be useful, but if they are, why do so few languages support them? Perhaps because it is sufficient that types have this capability?
* Do you have helpful paper or post links for further research and understanding, beyond those found in Andreas Rossburg's excellent paper on 1ML?","You should also check out:

- Benjamin Pierce's slides on modules: http://www.cis.upenn.edu/~bcpierce/papers/modules-icfp.ps -- I think Advanced Types and Programming Languages also has a bunch of discussion on modules, but I don't have my copy on me right now.
- Any of the papers by Derek Dreyer on modules. For example, you can check out his work with Rossberg on MixML.

---

> generativity

This is about applicative vs generative functors (OCaml defaults to applicative but also supports generative functors, I think SML only has the latter). The difference is about type equality:
   
    module M1 = F(M)
    module M2 = F(M)
    let conv : (m : M1.t) : M2.t = m

This code will compile if F is applicative and will give a type error if F is generative.

> opacity

This is about opaque ascription (aka sealing) vs transparent ascription. Opaque ascription hides type equalities, including for type aliases (not just ""data types""). The latter is notably missing in other ML-esque languages like Haskell, Rust and Swift, which only allow data types to be abstract.

(By ""data types"", I mean ""new type definitions"", such as with `data` and `newtype` in Haskell, and `struct` etc. in Rust/Swift.)

> stratification

This is about having a separate module language vs term language (""stratifying the language into two layers"") or having a unified language for the two.

> coherence

I suspect this is probably referring the point about having canonical modules for certain things vs not guaranteeing that. I don't recall any serious paper advocating for _incoherence_.

See the Modular implicits paper for more details.

> subtyping

ML modules are normally structurally subtyped (that enables better modularity), whereas most languages have limited structural subtyping (they may have nominal subtyping).

> higher-order-ness,

This is about where higher-order modules (aka functors in ML) are available, and if so, can they be used everywhere ML structures can be used, or whether they must always be fully applied.

> first-class-ness

This is about whether modules can be used as first-class values or not (e.g. as in 1ML). Last time I checked, OCaml has a limited form of this which requires explicit packing and unpacking, I don't know about the latest stuff.

> separate compilation

Do you require MLTon or Ur/Web style whole-program compilation or do you allow for separate compilation? 

As Pierce notes in his talk:

> “True” separate development requires that all dependencies between modules be mediated by explicit interfaces.

Most modern languages eschew explicit interfaces in favor of compiler-generated ones. Then you need to do a bunch of gymnastics in a compiler to extract good parallelism and incrementality from builds.

> extensibility,

This is about extending and combining structures / signatures.

> recursion

Recursive modules introduce more problems around type equality, via the so-called ""double vision"" problem. The MixML paper discusses an even more hairy version of this which they call the ""cross-eyed double vision"" problem. But they're useful too in some situations.

---

> Outside of research & academic interest, which of these features would be a valuable addition to a commercial programming language, because its value outweighs the added complexity.

This depends on the other features the language has! :P If the language has type classes with associated types, there is already some overlap with ML style modules.

> I can see ways that generic modules and module subtyping might be useful, but if they are, why do so few languages support them?

1. Implementation complexity
2. Feature interaction complexity: sophisticated module systems are hard to retrofit
3. Arguably, the ergonomics of modules in OCaml and SML aren't that great, making them unappealing to adopt.
4. Overlap with other language features
5. Other languages attach greater importance to other language design factors whereas OCaml/SML attach greater importance to modularity."
One weird trick to make your simple code gen (almost) as fast as C,v1ab6f,2022-05-31 05:25:13,"Following on from my question about [globaller register allocation](https://www.reddit.com/r/ProgrammingLanguages/comments/uz0fqc/globaller_register_allocation/) I just ported the most substantial benchmark yet to my IR to test my code gen. Applying the obvious changes (mostly inlining and adding some missing instructions) got me down to 18% slower than C (Clang -O2 -ffast-math). One final counter-intuitive change now has me at just 6% slower than C.

Before I describe this weird trick I need to explain a bit about my design. Firstly, this 358-line Aarch64 code gen is for an FPL. Specifically a minimal ML dialect. It relies *entirely* upon tail calls: no loops. My infinite register IL is *extremely* simple with only 64-bit int or float values. There are only two instructions:

* Constants , e.g. `pi = 3.14159;`.
* Function calls with any number of registers in *and out*, e.g. `i64 quot, i64 rem = divmod(num, denom);`.

Instruction blocks are an arbitrarily-long sequence of instructions ending with a conclusion that is one of just two possibilities:

* Return an arbitrary number of registers, e.g. `one, two, three`.
* An if expression testing two registers and tail calling one of two instruction blocks, e.g. `if a<b {a} {b}`.

A function has an arbitrary number of arguments and an arbitrary number of results. This is *crucial* for my weird trick. Furthermore, these are all passed in registers.

The core of this ray tracer is a non-tail recursive function `intersect` that has the signature:

    val intersect : (ℝ×ℝ×ℝ) × (ℝ×ℝ×ℝ) × ℤ × (ℝ×ℤ) → (ℝ×ℤ)

The two 3D vectors are constants for the duration of the recursion, created and passed in by another function. Therefore, they do not need to be spilled and, ideally, would be kept in the same registers for the entire recursion. The ℤ is the current scene tree and it changes with every call so must be spilled. The value of the type `(ℝ×ℤ)` is an accumulator: the closest ray intersection.

The weird trick is to change the function to have this signature:

    val intersect :
      (ℝ×ℝ×ℝ) × (ℝ×ℝ×ℝ) × (ℝ×ℤ) × ℤ →
        (ℝ×ℝ×ℝ) × (ℝ×ℝ×ℝ) × (ℝ×ℤ) × ℤ

Accumulating a lot more data appears silly but by making the return type of `intersect` match its argument type we've got the data going in and out via the same registers so the reshuffling `mov`s around calls is massively reduced. Hence the speedup.

IMO Clang is employing a smorgasbord of nifty tricks (fused multiply-add, heavily optimised constants, branchless selection, SIMD) but they merely offset poor register allocation on this benchmark. For example, here is the code between recursive calls generated by Clang:

        mov     x2, x0
        mov     x3, x1
        ldr     x0, [x19, #48]
        mov.16b v0, v13
        mov.16b v1, v12
        mov.16b v2, v11
        mov.16b v3, v10
        mov.16b v4, v9
        mov.16b v5, v8
        mov     x1, x2
        mov     x2, x3
        bl      _intersect

vs my code gen:

        add     x1, x29, x28
        ldr     x2, [x1]
        mov     x29, x1
        mov     x1, x2
        bl      _intersect

This raises many questions:

* Do any existing compilers do this? AFAIK C compilers will adhere to the ABI even internally in recursive calls which means multiple return values going via the stack.
* Can this be automated to create a minimalistic form of interprocedural register allocation?
* Does this relate to conventional compiler techniques for C-like languages where the recursion would be expressed as loops with an explicit stack?",This reminds me of some discussion in Appel's *Compiling With Continuations* (the chapters on callee-save registers and register allocation). Have you read it? It sounds like your design has similarities.
Candy,lizzh4,2021-02-13 20:49:36,"We're thrilled to announce [Candy](https://github.com/candy-lang/candy), a language that u/JonasWanke and I have been designing and working on for about the past year. We wrote a Candy-to-Dart-compiler in Dart and are currently making Candy self-hosting (but still compiling to Dart).

Candy is garbage-collected and mainly functional. It's inspired by Rust and Kotlin.

Here's what a Candy program looks like:

    use SomePackage
    use .MySubmodule
    use ....OtherModule Blub
    
    fun main() {
      let candy = programmingLanguages
        where { it name == ""Candy"" & it age < 3 years }
        map { it specification }
        single()
        unwrap()
    
      let greatness = if (candy isGreat) {
        ""great""
      } else {
        ""meh""
      }
    
      0..3 do {
        print(""Candy is {greatness}! (iteration {it})"")
      }
    }

Here's a quick rundown of the most important features:

* Candy's type system is similar to Rust's.
* Candy's syntax is inspired by both Rust and Kotlin and includes syntactic sugar like trailing lambdas.
* You can define custom keywords, so things like `async fun` can be implemented as libraries.
* Most noteworthy to this subreddit: Like Smalltalk, we follow the philosophy of keeping magic to a minimum, so we don't have language-level ifs and loops. You might have seen the `if` in the example code above, but that was just a function call to the built-in `if` function, which takes a `Bool` and another function, usually provided as a trailing lambda. It returns a `Maybe<T>`, which is either `Some` wrapping the result of the given function or `None` if the `Bool` was `false`. Also, `Maybe<T>` defines an `else` function that takes another function. And because we don't have dots for navigation, we get a clean if-else-syntax for free without baking it into the language.

The [Readme on GitHub](https://github.com/candy-lang/candy) contains a more comprehensive list of features, including variable mutability, the module system, and conventions enforcement.

We'd love to see where Candy goes in the future and can't wait to hear your feedback!","An `elif` would also be convenient for the maybe-based ifs, which I think is ingenious.

For the `do` loop, how does the current element get sent to the function? In particular, if the lambda given there does not accept a parameter, is there some overload/logic to handle that case?"
Philosophies of Rust and Haskell,kvgpq4,2021-01-12 09:00:49,,"> The designers themselves may have different motivations than those I describe.

Well, you can always go back to graydon, the original Rust designer:

> There is part of me that winces when someone goes looking for a justification for Rust above and beyond safety. Safety in the systems space is Rust's raison d'être. Especially safe concurrency (or as Aaron put it, fearless concurrency). I do not know how else to put it.
> ~ https://graydon2.dreamwidth.org/247406.html

That post also has the original slide deck, which I also find really interesting.  From slide 6:

- Rust is a language that mostly cribs from past languages. Nothing new.
- Unapologetic interest in the static, structured, concurrent, large-systems language niche.
  - Not for scripting, prototyping or casual hacking.
  - Not for research or exploring a new type system."
Interview with Haxe language creator and game dev Nicolas Cannasse,i76ku9,2020-08-10 23:09:18,,"Wow, that's a lot of supported targets! I imagine testing and keeping up with CI must be a challenge."
"Emojicode is an open-source, full-blown programming language consisting of emojis.",hywr4d,2020-07-28 01:32:26,,"Looks like a really interesting experiment. Some of the emoji replacements look like replacements for the sake or replacement, but e.g. bucket of popcorn representing dictionaries is a good idea!"
"Hakaru: a simply-typed probabilistic programming language, designed for easy specification of probabilistic models and inference algorithms",hejono,2020-06-24 02:09:07,,Our of curiosity: why the name?
"LATT, an esoteric programming language revolving around clocks",gpyo39,2020-05-25 06:19:08,,Will version two introduce a date complication?
Superclasses · Crafting Interpreters,fkri11,2020-03-18 23:39:17,,"I liked his game design pattern book, this should be fun to read."
C3 (a C-like language) now has a domain,e1vv55,2019-11-26 17:56:19,"I've finally gotten a domain for C3: [http://www.c3-lang.org](http://www.c3-lang.org). Please have a look if you haven't checked out updates for the language recently.

C3 is a C-like language with most of C and some modest improvements but no new paradigm:  


* Modules
* Generics
* Error model
* Struct subtyping
* Built-in safe arrays
* Zero cost simple gradual & opt-in pre/post conditions.

It's on the r/ProgrammingLanguages discord  [https://discord.gg/RczBvt](https://discord.gg/RczBvt)   
Everyone is also welcome to offer suggestions by filing issues here: [https://github.com/c3lang/c3docs/issues/new](https://github.com/c3lang/c3docs/issues/new). The unfinished compiler is here: [https://github.com/c3lang/c3c](https://github.com/c3lang/c3c)

Most looked for right now: people with a vision for a good module system for C3. ;)","- Why are pre/postconditions in comments? Comments changing the meaning of code is surprising; you now have to check comments when something misbehaves, and you cannot comment contracts out.

- You say your defer is scope based, but it seems function based instead. From your 'defer' section:


    func void test(int x)
    {
    defer printf(""A"");
    if (x == 1) return;
    {
        defer printf(""B"");
        if (x == 0) return;
    }
    printf(""!"")
    }

    test(1); // Prints ""A""
    test(0); // Prints ""BA""
    test(10); // Prints ""!BA""  -->> i would expect this to print ""B!A""

- your error handling mechanism is smashing Result<,> into a kinda awkward exception try/catch syntax style.Why not go for either normal sumtypes(id prefer that) or real eceptions - bothconcepts programmers will understand

- removing const / the compiler checking for const correctness and instead putting it into comments while saying ""hope you made no mistake  or its UB, NDR "" is a HUGE step backwards from c++. whats your rationale there?

- are pointers still nullable? (looked like it to me).a statically typed language should imo use that type system to allow catching such bugs at compile-time."
pLam now supports Lists with list library! Check it out and play around,cuwy29,2019-08-25 02:14:49,,Source (with a lot of documentation and more examples) at GitHub: [https://github.com/sandrolovnicki/pLam](https://github.com/sandrolovnicki/pLam)
COBOL: Why has it become a meme?,t3w03g,2022-03-01 09:45:11,"I understand that COBOL is an""ancient"" language, that a lot of corporations have used,and that a lot of old code is written in COBOL. As an engineer that writes primarily in Python, C/C++ and Fortran, what has caused COBOL to ""die out"" as opposed to C/C++ (and Fortran) that are still used for development of new projects?

Has the language simply not been able to adapt? If so, is that due to some inherent flaw in the language design/paradigm? Or is it just because other languages do the same job better?","I've been a COBOL programmer for over 25 years, and here are my thoughts.

A COBOL application is generally very monolithic, with minimal use of called subroutines.  Instead of subroutines (COBOL does have subroutines, but more about that later) COBOL has ""paragraphs"" (and god forbid ""sections"").  All paragraphs within a program are simple labels for code.  You can either ""GO TO"" a label, in which case when a paragraph is completed the code flow continues to the next label, or you can ""PERFORM"" them.  When you perform a paragraph, at the end of the paragraph code returns back to where it was performed from.  Pretty standard stuff.  The main difference is paragraphs do not allow for parameters.  All variables are declared at a ""global program"" level.  This leads to huge programs with a large number of variables where with any number of paragraphs are able to read and update those variables.  Can lead to quite the mess.

As I said, COBOL does have subroutines, but at least in my shop they are not used for general program logic.  They are generally used only for cases where many separate programs want to perform the same routine.  But for one programs business logic, logic that is not really shared by any other programs, we usually stick with the simple performing of paragraphs.

Additionally, unlike most ""modern"" (post 1969!) languages, COBOL uses built in compile time ""statements"" rather than invoking of library functions or user defined functions.  Often a COBOL statement will compile down to simply machine instructions, though sometimes where are calls to the COBOL runtime.  As a developer you cannot simple add a new COBOL statement; only the compiler developer can do that.  You can create COBOL subroutines, as I stated above, but it's not done to the extent that it is in other languages.

Another big difference is that most COBOL data structures are ""static"" in nature.  They have a defined size at compile time that never changes, and they are ""statically"" allocated.  Since most programs process ""one thing at a time"" (one account, one transaction, etc.) the same static area is simply reused, rather than allocating new storage.  This makes things very fast, but not very flexible.  

Character strings, for example, are fixed length.  This caries over from legacy reports that have fixed length columns (fields) within fixed length lines (usually 80 or 132 characters total) and printed using fixed width fonts.  Even most of the legacy database formats are heavily based on this ""fixed length"" paradigm, which makes it very ""difficult"" to extend the length of any particular field.  

There are recent COBOL standards that do describe ""more modern"" features such is object orientation, user defined functions, dynamic length strings, etc.  Many of these new features have not yet been implemented in any meaningful way in legacy compilers, however, and their integration is often awkward in any case.

Anyway, there's probably a lot more to say, but I'll leave it at that for now."
ACM Transactions on Programming Languages and Systems (TOPLAS): all papers (since the 1979 beginning) now open access,se5e1j,2022-01-28 02:42:20,,"Why does ACM think that the modern thing of popping up a cookie dialog is a best practice?

I was an ACM member in the 90's until I realized two things: that their own computerized membership processing was bad, and their #1 thought was always about getting more money. Getting quality content to practicing developers always seemed to be job #8."
Why Static Languages Suffer From Complexity,s7shox,2022-01-19 23:22:32,,"When I looked at Smalltalk, I was amazed how much was in there.  
It uses classes and closures dynamically. Just the small implementations, [Squeak](https://squeak.org/) or [Pharo](https://en.wikipedia.org/wiki/Pharo), all have full graphical operating systems built in.  
Additional browser, zip-file and pdf-file support, etc.  

Dynamic languages can help with very fast development. Most languages do not even have 10% of the library support.  
But every language seems to have wall(s), when a certain complexity level is reached."
Enso 2.0 Syntax Reference is out!,n063p0,2021-04-28 11:41:16,,You should add to your front page prominently that it is free and open source. Your site looks like some generic Saas product and I assumed that I had to pay or subscribe for using it.
Safe Systems Programming in Rust,mb07jg,2021-03-23 06:51:20,,"> In this article, we begin by giving the reader a bird's-eye view of the Rust programming language, with an emphasis on some of the essential features of Rust that set it apart from its contemporaries. Second, we describe the initial progress made in the RustBelt project, an ongoing project funded by the European Research Council (ERC), whose goal is to provide the first formal (and machine-checked) foundations for the safety claims of Rust. In so doing, we hope to inspire other members of the computer science research community to start paying closer attention to Rust and to help contribute to the development of this groundbreaking language."
Strema: a programming language built live on stream,lz1000,2021-03-06 20:53:30,,"The language part I fear is over my head, but I'd like to say that I like the chill, no-commentary format of coding streams. I guess you won't hit the top charts like someone with a fake, explosive personality, but I feel that you can't keep that level of energy throughout the stream. The silent coding is more akin to what happens usually when coding and I feel it's more natural. All I wanted to say."
What is the unit of a text column number?,lmno8w,2021-02-18 22:14:10,,"If legibility to machines is the more important than legibility to humans, why not use byte offset and let IDEs and tools translate that into line/col numbers from the source of the file? Line/col numbers are useless to machines and humans alike unless you have the actual source file anyway."
Kuljet: A language for small HTML/DB applications,ki3nav,2020-12-22 19:33:33,,"I really like this, please keep going."
GRIN is a compiler back-end for lazy and strict functional languages with whole program optimization support,j4xbuv,2020-10-04 19:15:00,,"I really like this and will try to look into it eventually. Did you throw this in as a response to the [LIJP](https://github.com/cheery/lijp)?

I like that GRIN did throw in the abstract syntax as well."
Silq: The first intuitive programming language for quantum computers,hcnrk9,2020-06-20 23:06:47,,"For those who know nothing about quantum computing, think of it as being like programming using linear algebra:  you start with some vector representing the state of the system and then apply a sequence of matrices; arguably the interesting part--and what makes a quantum computer more powerful than an analog computer--is that some of the matrices are *projectors* that correspond to measuring the state of the system and getting a random result.  The projectors are important because if an error--which will in general be analog--sneaks into one of the operations we can choose projectors in such a way that they force the error to become one of a discrete set and therefore correctable by a digital action.  This is in contrast with analog computers where we don't have any way to correct analog errors.  Thus, in a way we get many of the benefits of an analog computer without the noise issue that cripples them in practice.

One aspect of quantum computing that makes it so powerful is the mysterious sounding term *entanglement*, but really what that just means is that quantum variables are allowed to have arbitrary distributions over all of the possible values rather than being restricted to a product of single-bit distributions.  As a result, a quantum variable can essentially store an amount of information that is exponential in the numbers of bits, and operations on the variable operate on all of the possible values it could take simultaneously resulting in massive parallelization.  Unfortunately, there is a catch: when you measure the variable at the end of the computation, you don't get to see the full distribution, you only get to see one randomly chosen value.  Thus, you need to do some amount of work to massage the distribution so that the value you get is the answer you were looking for.  In some cases you don't have to do too much; for example, the algorithm for factoring large numbers uses a Fourier transform to essentially shape the distribution so that there are peaks at multiples of the factors, and this transformation can be done efficiently so you get the exponential speedup.  There is another algorithm known as the Grover algorithm that lets you search for the input to a binary function that causes it to return true, and marking the input with this property is something that can be done very quickly, but this just changes the sign, which does not alter the probability; massaging the distribution so that the answer you want is the one that you are most likely to get takes so much work that you only get a quadratic speedup (which nonetheless can be quite substantial for large problem sizes)."
Linking Vortex to SDL,12u92us,2023-04-22 00:02:58,,"I thought a pretty good use case for C interoperability in Vortex was to dynamically link to the awesome SDL library. Loads of fun getting this to work. Hoping to keep adding to the module until it has enough features to start building useful things.

You can check out the repo here: https://github.com/dibsonthis/Vortex"
Verse Language Reference,11yrcnl,2023-03-23 02:09:42,"The UEFN (Unreal Editor for Fortnite) beta was released today, and that means you can now finally try out Verse (in the context of game development). The Verse language docs are now available as well: https://dev.epicgames.com/documentation/en-us/uefn/verse-language-reference

One thing I found interesting is how the [not operator](https://dev.epicgames.com/documentation/en-us/uefn/operators-in-verse#notoperator) works. I was wondering how it would work because I was under the impression that the language just wouldn't have booleans, but it seems more like it doesn't have booleans as we know them, but an expression `not p` returns no value when `p` fails and returns `true` when `p` succeeds. `true` seems to mean ""expression succeeds but discard its effect"".","Does anyone know how much this is implemented? Is there syntax highlighting, compilation, etc. How many APIs are exposed? (Ik I’m too lazy to look myself)"
Bun is a fast all-in-one JavaScript runtime,vufufy,2022-07-09 01:19:16,,"Honestly I was about to shit on it because I thought it'd be another thing like V. 

But it actually looks amazing. And it's built in Zig! 

I'll definitely check it out. The slow build and install times is one of the things I always hate when I have to touch JavaScript."
"What syntax design choices do you love, and what do you hate?",tt7kq9,2022-04-01 03:27:39,"I've recently started working on a language of my own as a hobby project, and with that comes a lot of decisions about syntax. Every language does things a bit differently, and even languages that are very similar have their quirks.

I'm interested in hearing outside opinions; what are some aspects of syntax design that you love to work with, and what are some that make you dread using a language?",PHPs holla holla get dolla $$$ everywhere
"Since ""meta programming"" is going to forevermore result in unrelated Facebook stuff",qirsz8,2021-10-30 10:12:15,"What are we going to say instead of ""meta"" now. Maybe not even limited to programming - the word is definitely going to be completely blown out...

""transcendental programming""?

""reflexive programming""?

""selfref programming""?

I'm bummed regardless...","""metaprogramming"" as a keyword should still result in the correct results. Google is pretty good about this."
Why do modern (functional?) languages favour immutability by default?,ot3mcp,2021-07-28 13:30:49,"I'm thinking in particular of Rust, though my limited experience of Haskell is the same. Is there something inherently safer? Or something else? It seems like a strange design decision to program (effectively) a finite state machine (most CPUs), with a language that discourages statefulness. What am I missing?","It makes it difficult to reason about the behaviour of large programs, when a value might change from under you at any point because of a completely unrelated section of program.  (This difficulty extends to the compiler, as it happens; immutability enables many optimizations.)  *Function*al programming languages encourage the modeling of programs as collections of _functions_, in the mathematical sense of a pure mapping from input to output.  This enables modularity and reduces the number of things you have to think about when considering the behaviour of a given module or function.

(This should not be taken as an endorsement of functional programming or immutability, nor of the opposite approach; I am just stating the common arguments and motivations.)"
Interpreter Generators: A Brief Look at Existing Work,o16mf6,2021-06-16 22:47:53,,"Very interesting, thank you for sharing this"
"As a Vim enthusiast, had an idea for a language...",n3doud,2021-05-03 03:20:53,"Im one of those coders that loves to never touch the mouse, and throw Vim commands around to chop and serve code up as quickly as possible.
I've wanted to code a language just for fun, and I had the idea: 

What would a language look like if it was designed to be as ""Vim compatible"" as possible?

So basically choosing syntax that fits nicely with Vim commands. maybe that means grouping things by paragraphs so you can do 'dap'  or using several unique symbols in a line so using 'f<char>' is super useful. maybe having no braces or parentheses like ruby. 
Any ideas?","another thought: what would a language look like if the editor for the language and the language itself were co-developed?

it's honestly hard to think of improvements since  we are so used to editing code, but there might be some cool ideas here."
Futhark 0.19.1 released,lxowxl,2021-03-05 00:40:46,,I was thinking what language I should use for functional HPC. This looks interesting.
"Play: A statically typed Forth, that targets web assembly",jzfyuw,2020-11-23 19:30:06,,I'm the language creator. AMA.
Programming only with classes,iukm19,2020-09-17 22:32:00,,"That is to class what lambda calculus is to function :p.
That is pretty fun and could be the base of a small esolang. Very on topic here if you ask me."
Haskell Cheat Sheet 😁,fum1pw,2020-04-04 10:50:33,"Hello everyone!

I've been learning some Haskell, for fun, and made some notes \^\_\^

[Two-column PDF](https://alhassy.github.io/HaskellCheatSheet/CheatSheet.pdf) ; [Repo](https://github.com/alhassy/HaskellCheatSheet) ; [Single-column PDF](https://alhassy.github.io/HaskellCheatSheet/CheatSheet_Portrait.pdf) ​

* Covers Haskell basics: Types, tuples, lists, folds, ADTs, typeclasses
* Functors and examples, and non-examples ---with ‘intuition’ for the laws
* Applicatives: \[Non\]Examples and do-notation
   * A ‘formal’ inductive definition of do-notation
* Monads as applicatives + join ♥‿♥ ---with \[Non\]Examples
* Four evaluators using maybe/writer/reader/state monads
* A list of useful reads

Hope this helps \^\_\^","That ain't a cheat sheet, that's a textbook"
r/ProgrammingLanguages on Import Mechanisms,1340z3r,2023-05-01 03:40:52,"I've searched this channel for useful tidbits. Here's a summary of what I've gleaned:

# Motherhood Statements:

* Copy / remix elements you like from languages you already know.

# How shall I expose imported names?

* Some language treat imports like macro-expansion, inserting the text of a file directly in the token stream.
* Often the import adds a global symbol that works like object-field access. (Python does this. Java *appears* to, but it's complicated.)
* Author of NewSpeak considers imports harmful and insists on extralinguistic dependency injection for everything.
* Globular imports are *usually* frowned upon. List which names you import from where, for the maintainer's sanity.
* But core-standard modules, and those which implement a well-known vocabulary (e.g. Elm's HTML module) often benefit from globular import.
* Explicit exports are usually desirable. Implicit transitive imports are usually *not* desirable.
* Resolve name clashes with namespace qualification.
* Provide *import-as* to deal with same-named (or long-named) modules.
* &#x200B;
* AutoComplete tends to work left-to-right, so qualified names usually have the namespace qualifiers on the left.

# Where shall I find the code to load?

* Maybe import-path from the environment, presumably with defaults relative to the running interpreter.
* Maybe look in an application configuration file for the path to some import-root? (Now where did I move those goalposts?)
* Often, package/subpackage/module maps to the filesystem. But some authors strongly oppose this.
* Within a package (i.e. a coherent and related set of modules) you probably want relative imports.
* Be careful with parent-path `../` imports: Do not let them escape the sand box.
* Some languages also allow you to completely replace the resolver / loader at run-time.
* JavaScript has an ""import map"" mechanism that looks overcaffeinated until you remember how the leftpad fiasco happened.
* Unison and ScrapTalk use a content-addressable networked repository, which is cute until log4j happens.
* Speaking of Java, what's up with Java's new module system?

# What about bundled resources, e.g. media assets?

* Holy-C lets you embed them directly in the source-code (apparently some sort of rich-text) as literal values.
* Python has a module for that. But internally, it's mainly a remix of the import machinery.
* Java gets this completely wrong. Or rather, Java does not bother to try. Clever build-tooling must fill in blanks.

# What about a Foreign Function Interface?

* Consensus seems to be that C-style `.h` files are *considered harmful.*
* Interest in interface-definition languages (IDLs) persists. *The great thing about standards is there are so many from which to choose!*
* You'll probably have to do something custom for your language to connect to an ecosystem.
* Mistake not the platform ABI for C, nor expect it to cater to anything more sophisticated than C. In particular, Windows apparently has multiple calling conventions to trip over.

# What about package managers, build systems, linkers, etc?

* *Configuration Management* is the name of the game. The game gets harder as you add components, especially with versioned deps.
* SemVer sounds good, but people \*\*\*\* it up periodically. Sometimes on purpose.
* Someone is bound to mention rust / cargo / crates. (In fact, please do explain! It's Greek to me.)
* Go uses GitHub, which is odd because Google now depends on Microsoft. But I digress.
* Python pretty much copied what Perl did.
* Java: Gradle? Maven? Ant? I give up.
* Don't even get me started on JavaScript.

# Meta-Topics:

* Niche languages can probably get away with less sophistication here.
* At what point are we overthinking the problem?",I’m curious to hear people’s thoughts on using dot notation for everything. This is the case in Python for example where attribute access of an object and using a method from a different namespace both are done with the dot operator. In Rust or C++ you use ‘::’ for the latter
could someone direct me to resources to make my own programming language?,zbgjtj,2022-12-03 21:40:31,I have an idea and i wanna make a language but i have no idea where to start. How do you start?,Crafting Interpreters is a good start. First half you learn how to write a tree walking interpreter and the second have a virtual machine.
Writing a Compiler - Part 1 - Defining The Language,yvcfir,2022-11-15 04:59:03,,Looking forward to read the whole series!
"I wrote a long-ish comment about bidirectional type checking. It was well received, so I posted it on my blog.",v7ddn0,2022-06-08 09:25:59,,"I think this is well explained! As someone unfamiliar, it'd be nice to see examples where one approach struggles but the other succeeds to really ""sell"" me on it. But I realize you're just transcribing an old comment."
Provably Space-Efficient Parallel Functional Programming,s37h5r,2022-01-14 03:29:04,,Can't wait until this kind of research makes it to GHC!
"A small, portable Scheme implementation with AOT and incremental compilers that fits in 4 kilobytes",r49hhz,2021-11-29 01:29:38,,"Paper: http://www.iro.umontreal.ca/~feeley/papers/YvonFeeleyVMIL21.pdf

Skimming over it, it has an architecture similar to tinypy, a 64 KB Python interpreter which I enjoyed playing with: http://www.tinypy.org/

That is, the parser and compiler is not written in native code; instead they are bootstrapped onto the VM via Scheme (or Python in the latter case).

The performance benchmarks are surprisingly good.  I think they must not be exercising the string data structure, or other Schemes also have extremely slow string performance as well.  Because the encoding of strings is a linked list of characters!  The Rib encoding is used uniformly for both the instructions and interpreter data structures like the stack.

Or maybe when you get to bigger programs, the benefits of being in cache are lessened.  Still seems like something fun to look into.

(The paper references sectorlisp at the end: https://justine.lol/sectorlisp/)"
Is it okay to compile down to C?,qvqa1i,2021-11-17 11:50:45,"I'm designing a safer systems programming language.

The code will be compiled down to c99, and then can be compiled by every standard c compiler to machine code. I chose to do this instead of compiling down to LLVM or compiling down to machine code directly (god forbid).

Aim would be to allow developers write safe code that's easy to audit, and maintain for a long time. It is inspired by Ada, C, C++ and python, but is optimized to be coded very fast on QWERTY keyboards, to improve developer productivity.


Others have tried to compile down to c code before. Even C++ started out like this.

Is this okay though? Do you see any issues with this approach?

It would be very helpful if you point out what future problems I might have, or things that I need to be careful about, so that I can be more careful with my design.","yes, though you do have to be careful to understand the C language -- it can be easy to accidentally depend on details of a particular compiler or platform.

btw, if you keep track of the line of your input source code corresponding to each line of generated C code, you can use the `#line` directive to allow gdb to step through source code written in your language directly!"
"Would a functional language like Haskell be worse off in developing a compiler as opposed to a systems language like Rust, C, etc?",odnfvo,2021-07-05 00:07:19,"Discussion/Help

I'm looking to explore compiler construction after having implemented an interpreter for my final year project in university. Since I've finished all of my studies and have a lot of free time until my job starts I have been looking at tutorials implementing a compiler in Haskell (the language in which I worked with for my fyp) but two problems have arisen. 

The first is that resources seem stark compared to say C. This does make sense given that a hell of a lot of language start off with C. The best link I've found has been Diehls [JIT Compiler using LLVM](https://www.stephendiehl.com/llvm/#the-basic-language) (in which I have spent the last half hour waiting for LLVM to install) and then a tutorial [here](https://bjbell.wordpress.com/2010/01/20/writing-a-compiler-in-haskell-compiler-series-part-i/) that doesn't seem to finished. 

Then the second reason and arguably most important is that wouldn't say C or Rust be much more suitable in the task of constructing a compiler? Wouldn't be as ""close the metal"" be better regarding efficiency in compiler construction overall? I've done a lot of the C portion of Crafting Interpeters and found it much easier to reason what was occuring than I think I would have if it was done in Haskell. Rust as well seems to be quite an interesting language to explore given the functional constructs in it.

 I know theoretically any language can do anything but I feel like perhaps I'm putting a lot of credit for the lack of a better word regarding this type of task on systems languages purely down to historical reasons.","""Close to metal"" part is only important for improving the compilation time. It's important, yeah. But so is development speed. If you can develop faster in Haskell, why not. In C++, writing double dispatch for visitor pattern is very tedious. I'd love to use ADTs and pattern matching for this. Haven't tried Rust yet.

In the end it's just the conversion of input language to output language."
Finding the Bottom Turtle,o4or5d,2021-06-21 14:20:42,,The importance of open source firmware can not be overstated.
Four Features That Justify a New Unix Shell,jgaho6,2020-10-23 06:45:43,,"(fixed this submission, the previous one linked to reddit itself!)"
Full Proof that C++ Grammar is Undecidable,ga27xn,2020-04-29 11:46:22,,"Just to be clear, I am not the author, I just found this interesting."
So... I developed an interpreted language in C++,cx4hea,2019-08-30 02:03:34,"Hey everyone. I have been developing a programming language in C++. If you are interested, see the medium post overviewing my journey building it, and some information about it here: [https://medium.com/@ElectruxR/so-i-created-a-programming-language-4d9c11038d22](https://medium.com/@ElectruxR/so-i-created-a-programming-language-4d9c11038d22)

The GitHub repo for it is: [https://github.com/Electrux/Ethereal](https://github.com/Electrux/Ethereal)

You can find multiple sample programs in **tests/** directory of the repository (plus, the **perform\_tests.et** in the main repo directory).

I hope you like it \^\_\^ . Feedback and contributions are, of course, greatly appreciated :D","Nice work :)

What is ""mfn"" in 

`mfn< str > print() { 	println( self ); }`

It doesn't seem to appear anywhere else. Also, is this method definition syntax drawn from somewhere? I don't think I've seen anything like it."
astmaker — A DSL in Rust for programming language designers,13k8d8k,2023-05-18 01:30:13,,"I know you!!!! You are the letlang guy, the one that also did an article on the Rust parsing ecosystem! I must say I really admire you and I enjoy your work! Thank you!"
"The Registers of Rust - Without boats, dreams dry up",11nzw0x,2023-03-11 04:28:01,,"This post is mostly about Rust, but I found it really interesting and gave me a new perspective on language design in general, so I thought it was worth sharing here."
Do you consider LLVM a complicated software? And are there any alternatives and how they compare to LLVM?,zy2rv5,2022-12-29 19:05:24,"In effect, what parameters should be important in making a decision of choice of LLVM / similar software for programming a language?

Also, how smart it is to avoid LLVM / similar software in programming a language?","Personally LLVM seems like a beast best avoided for hobby languages unless you already know it. I've spent the better part of a month getting into it just to stumble over an implementation I did at the beginning that wasn't *entirely* what LLVM wanted and which would've caused me to have to redesign almost everything. 

There are some alternatives, like asmjit, the jit from LuaJIT, the CLR from .NET or even the JVM. Personally most of them are either restricted (e.g. asmjit is written in C++ which makes interfacing in other languages harder) or don't have much or good documentation (e.g. LuaJIT or LLVM itself). That's why, like someone else wrote on here some time ago, I'd go the way of writing a KISS interpreter that consumes your AST (or whatever you'd use to generate actual assembly) just to test out whether your language feels ergonomic. Bonus points is that you could use it as a MIRI equivelant. 

Some also output C++ and have GCC handle it, but I found myself to be relying on it too much. I want my compiler to be great, not just a frontend for GCC."
"Temporal Programming, a new name for an old paradigm",z5vrlp,2022-11-27 16:24:56,,"Isn't this just FRP?  Take in a state, do some action, recurse with the modifications to enable the next state."
End of a language feature,wp1cy0,2022-08-15 22:39:44,,"Tangentially, if you're interested in Sobol sequences you might like this recent paper: https://diglib.eg.org/handle/10.2312/sr20211287"
Why roc does not have a Maybe type,wmzuxr,2022-08-13 07:57:32,,"> when...is instead of case...of

As a long-time programmer, this looks weird as heck, but you know, I could see myself coming around. It *does* read better.

(Of course, Inform 7 got there first.)"
"Ante: A safe, easy, low-level functional language for exploring refinement types, lifetime inference, and other fun features.",vkhfhm,2022-06-25 23:27:51,,"Add modules / namespaces.

Good Work, Good Luck !!! 👍🍀"
The appeal of bidirectional type-checking,v3z7r8,2022-06-03 21:17:46,,"Can anyone explain this in laymen terms, or ELI5? I am a software engineer by trade, understand what types are (like int, or tuple), but don't know what bidirectional type-checking is or how it is useful."
Type Theory's type universe: What is the type of `Type`?,tjev87,2022-03-21 23:49:35,,"I think this does a good job explaining the idea of a universe and what Russell's paradox is! I don't think this does a good job of explaining why Russell's paradox applies to type theory. Specifically I take issue with two things you say.

> The languages mentioned in the previous section (Idris, Agda, Coq) use a type theory called Typed λ (Lambda) Calculus, which is based on mathematical logic that heavily depends on Set theory. Set theory studies Sets, that are basically a collection of unique objects.


Type theory is very much *not* based on set theory. Type theory and set theory are both formal systems that can be used as a foundation for mathematics, neither is based on the other. Type theory is in some sense even more foundational than set theory, because the axioms of set theory are expressed in first order logic, while type theory is already a logic, its axioms are expressed directly. The fact that `Type` is called `Set` in Agda is an (imo unfortunate) artifact of history, it has nothing to do with set theory.


> We can define the type Boolean as a set of { true, false }. That means a variable that is the type of Boolean, can either be true or false, and nothing else.


The *type* Bool and the *set* `{true, false}` are not the same mathematical object. I'll refer you to [this fantastic stack exchange answer](https://cs.stackexchange.com/a/91345) by u/andrejbauer for an explanation as to why.

With this in mind, it's not at all obvious that Russell's paradox should apply to type theory. After all, types are not sets! Of course, it turns out that (an analogue of it) does apply, and it's called Girard's paradox. You mention Girard's paradox, but only in passing, in an expandable text snippet. 

Having said this, I still enjoyed reading your article, and I think it definitely has useful introductions to some tricky concepts, so thank you for writing it!"
"Infinite loops: a side-effect, or an implementation detail?",tdlff4,2022-03-14 09:05:45,"I'm writing a pure, functional (but not lazy) language.

It occurred to me while working on my MIR optimiser that several of its passes have the capacity to remove possibly-infinite loops if it determines that their result is unused.

This led me down the rather disturbing rabbit hole of considering wherever divergence should be considered a side-effect or not. I'm aware that a few languages have different takes on this:

- In C++, an infinite loop with no side-effects in the loop body is considered to be undefined behaviour and may be optimised away, with potentially catastrophic results

- In Rust, an infinite loop is considered sound and correct (otherwise, creating undefined behaviour in safe code would be trivial) and so will not be optimised away. Non-termination is considered to be a side-effect

- In Koka, divergence (I.e: looping infinitely, or at least an inability of the type system to prove that a function halts) is considered an algebraic effect

- In Haskell, infinite loops are lazily evaluated and so optimising them away is *always* fine if their result is unused

What approach do you take and do you have any particular thoughts about how this annoying and thorny issue should be tackled soundly and correctly?

I'd rather like to:

1) Be able to write recursive functions without needing to build a system that can prove termination such as through Walther recursion, or otherwise denote non-termination in the type system

2) Not need to add support for laziness

3) Not need to think too much about termination when writing optimisation passes

I realise that some or all of these things may be mutually exclusive though.","The broader question here is: what does it mean for a compiler to preserve semantics between the source code and compiled code? Does the compiled code have to do exactly everything that the source code does in order for semantics to be preserved, or does the compiler have wiggle room to change behavior slightly during optimizations? 

Xavier Leroy has written extensively on this topic, I would check out Section 2.1: Notions of Semantic Preservation in [A Formally Verified Compiler Back-end](https://www.cs.cmu.edu/~15811/papers/compcert-journal.pdf), a paper on CompCert. It's very close to the beginning of the paper, you shouldn't have to scroll too far to find it. He talks about some different logical definitions of preservation, and they have subtly different properties. 

My understanding is that what you're describing is considered a backward simulation: the compiled code is allowed to do *less* than the source code while still be seen to retain semantics. He even talks about almost this exact scenario, but uses a division by 0 in the source program as the example:

>Definitions 1 and 2 imply that if S always goes wrong, so does C. Several desirable  
optimizations violate this requirement. For instance, if S contains an integer division  
whose result is unused, and this division can cause S to go wrong because its second  
argument is zero, dead code elimination will result in a compiled program C that does  
not go wrong on this division.

The distinctions between simulations get subtle, and there is also mention of requiring a proof of ""safety"" for the source program to prevent situations like this from preventing preservation. I honestly keep re-reading this paper, and can't say that I fully grok it yet, but there is a wealth of information about this topic even in this one section.

To answer your question directly, divergence is a side effect. Your scenario is unfortunately a little philosophical - it's the same as, ""if a tree falls, and no one witnesses, did it really fall?"" Just because a side effect is avoided in the compiled code, it doesn't mean that the side effect isn't a side effect. It was just avoided. And avoiding behaviors is something that you may or may not want your compiler to do, depending on the definition of semantic preservation that you want to allow. I would choose one that allows this optimization."
Exotic Programming Ideas: Part 3 (Effect Systems),jz36fl,2020-11-23 04:47:15,,No one reminds of https://github.com/ghc-proposals/ghc-proposals/pull/313 so far? Hope that exciting proposal progresses faster!
Carpentry Compiler,eh1sfy,2019-12-29 12:57:18,,That's super cool
The Next 7000 Programming Languages,dpnmzl,2019-10-31 21:36:40,,I think they're missing a bit on webassembly. With the way things seem to be going I would not be surprised if more languages start targeting wasm and using that for ffi instead of only C bindings and libraries. Call me an optimist but I want the future where I can write code in any language and have it all interop with minimal effort.
A Bestiary of Single-File Implementations of Programming Languages,dn8v17,2019-10-26 12:05:40,,"The creator of the list included a few of my languages, but omitted my favorite, T3X, which compiles from source to ELF/386 using a single file of about 30K bytes. [http://t3x.org/t3x/t.t.html]"
A DSL for creating signed distance functions,12t1782,2023-04-20 22:41:01,"As a final project for the programming languages course at my university, we were tasked with designing and implementing a DSL.

I got the idea to combine my interest in programming languages and computer graphics, so I came up with a little language that lets us describe a 3d object, and then generates a signed distance function (SDF) to be used for rendering.

The general idea is very simple:

The DSL provides some primitive shapes (spheres, boxes, rounded boxes), and some operations on them (scaling, rotation, inflation, etc.), as well as some ways to combine them ((smooth) union and (smooth) intersection), and it can then produce a GLSL program fragment (representing the corresponding SDF) that can be dropped into other shader code, and used to render the resulting object in the GPU.

(Actually, spheres, boxes and rounded boxes can be built up from an even more basic primitive: the point. The DSL just offers these for convenience)

I also wrote an SDF renderer that is very helpful when designing objects. It should run on any modern WebGL-capable browser (though I've only tested it on Firefox).

Some example code:

    ball         = sphere 0.05
    stick        = box (0.01, 0.01, 0.3)
    ballOnAStick = union [ stick, translate (0, 0.3, 0) ball ]

The corresponding GLSL program fragment:

    vec3 v0 = pos;
    vec3 v1 = (v0 - clamp(v0, vec3(-1.0e-2, -0.3, -1.0e-2), vec3(1.0e-2, 0.3, 1.0e-2)));
    vec3 v2 = (abs(v0) - vec3(1.0e-2, 0.3, 1.0e-2));
    vec3 v3 = (v0 - vec3(0.0, 0.3, 0.0));
    return min((length(v1) + min(0.0, max(max(v2.x, v2.y), v2.z))), (length(v3) - 5.0e-2));

A screenshot from the SDF renderer:

[https://imgur.com/a/9zCs3Qq](https://imgur.com/a/9zCs3Qq)

The repository has a more extensive report that goes into the architectureand design decisions, but it's written in spanish. I don't think that should be a problem given the quality of machine translations we have nowadays. If anyone wants to translate it I'll gladly accept a PR, though.

[https://github.com/SebastianMestre/sdf-dsl](https://github.com/SebastianMestre/sdf-dsl)",This is so cool! Well done! What did you use to draw the diagrams https://github.com/SebastianMestre/sdf-dsl/blob/master/img/combinators.png ?
The Spinnaker Programming Language,125qpy1,2023-03-29 22:58:19,"Here we go at last! This has been a long time coming.
I've been working on an off on Spinnaker for more than a year now, and I've been lurking in this subreddit for far longer.

Spinnaker is my attempt to address the pet peeves I have in regards to the functional programming languages I've tried (mainly Haskell, Elm, OCaml, Roc...) and a way to create something fun and instructive. You can see in the README what the general idea is, along with a presentation of the language's features and roadmap.

I'm sharing the full language implementation, however, I don't recommend trying it out as error reporting and the compiler interface in general isn't user-friendly at all (don't get me wrong, it would be awesome if you tried it). You can find lots of (trivial) examples in the `examples/` directory (I'm against complex examples, they showcase programmer skill more than the language itself).

The compiler is meant to be minimal, so the whole standard library is implemented in Spinnaker itself, except operations on primitive types (e.g. addition), these are declared in Spinnaker and implemented in the target language through the FFI. You can look in the `stdlib/` directory to see what the langauge has to offer. The implementation of primitive operations is provided in the `runtime/` directory.

Being inspired by Roc, I decided to go with monomorphization and defunctionalization. My ultimate aim is to compile to C. Right now the available targets are JS, Scheme and an interpreter.

I appreciate any kind of feedback.

P.S.: Although I was able to implement the language, my code quality is abysmal. I also didn't know Haskell very well before starting this project. Tips on style and performance improvements are very welcome.","This is really impressive if this is your first attempt! Well done!

I see your type checker is using explicit substitutions as `Map`s. While this is not the end of the world, for performance reasons and to save your own sanity, you might want to represent unification variables as mutable references ([IORef](https://hackage.haskell.org/package/base-4.18.0.0/docs/Data-IORef.html#t:IORef)s or [STRef](https://hackage.haskell.org/package/base-4.18.0.0/docs/Data-STRef.html#t:STRef)s) instead.

Simon Peyton Jones et al's [Practical type inference for arbitrary rank types](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/putting.pdf) demonstrates pretty well how to do this. It's a long paper, but it's almost certainly worth a read, even if you don't care about higher-rank types.

The paper also implements bidirectional type inference, which is quite nice for both performance and error messages, so that might be relevant to you as well.

I was going to point out how you don't seem to have a way to restrict generalization (as described in [Efficient and Insightful Generalization](https://okmij.org/ftp/ML/generalization.html) for example), but I don't think your language even has local let bindings... at all? That's a bit of a strange choice. Is there a reason for this?

Also, the way you check against type signatures works for you right now, but will fall over once you introduce features that can be checked, but not inferred (e.g. higher-rank types or GADTs). If I understood your code correctly, you're currently inferring a type for the expression, unifying that with the expected type and checking that the unification did not bind any unification variables in the expected type.

A more robust and efficient way to do this would be to skolemize the expected type (i.e. replace every type variable with a unique skolem type constant that is only equal to itself) and then unify the two.

Finally, a few Haskell-specific nitpicks:

* `String` is a linked list of UTF-32 codepoints, which is just as bad as it sounds. Haskell programmers usually use `Text` from the [text](https://hackage.haskell.org/package/text) package instead.
* It's usually recommended to write type signatures for every top level definition. You have a few definitions that don't have one, so that might be something to improve. The fact you didn't do this also suggests that you're probably compiling without `-Wall` (since `-Wall` would complain about this), which you should probably turn on to save yourself from a few headaches.
* `e''''` is... uh... not great. I know Haskell programmers are pretty bad at using good names, but please try to come up with something more useful than that. 

But again, this is extremely well done! Certainly much better than my first attempt."
What would make you try a new language?,10oiqcc,2023-01-30 05:10:42,"Since we're all picky and working on our own languages, what would it take for you to be interested in another language and what would you want to see on their front page? (or one of the first pages of the site)","`what would you want to see on their front page?`

""We stole Rust's Cargo, C++'s zero-cost abstractions, gave it a Lisp (extensible syntax), and made it compile natively.""

I primarily work on low level game engine code... Suffice to say it would take a lot for me to consider switching from C++. That quote is probably only 20% of what I'd really want (and doesn't even mention the core language syntax), but that would 100% get my attention.

&#x200B;

The runner up would be ""GPU scripting using APL"" (or any array language, really). APL is almost literal magic sometimes, and I think it would find a natural home in GPU programming, if someone is crazy enough to make it work, even as a subscript in a shader programming language."
"APL was famously designed as notation for working out programming on blackboards first. Are there any other languages specifically designed for ""pen and paper"" programming?",1086pnq,2023-01-10 18:37:34,"I was just kind of wondering this, because I do like working my way through complex algorithms on paper to really get a feeling for them, and I'm sure I'm not the only on here who does. However, when I do the result tends to be messy pseudocode that feel neither here nor there. Either that or it becomes *really* tedious to write out what I mean in a correct reproduction of whatever language I'm working with at the moment.

Now, to be clear: I'm not asking ""which languages are good for writing out programming problems"" - there probably are lots of languages whose design choices *happen* to suit handwritten programming (feel free to discuss that though).

I'm wondering if anyone other than Kenneth E. Iverson ever spent time *specifically* designing a language for the sake of working out programming problems through handwriting? Alternatively, what kind of affordances do you think such a hypothetical language should keep in mind?

My first superficial thoughts would be that it needs to find a good sweet spot for terseness vs readability. The second, overlapping concern would be how to visually structure code in a way that ""agrees"" with handwriting.

The other thing is that it needs to align well with how human working memory works, even more so than normal languages should (but often don't because we abuse our IDEs as a crutch).","AFAIK Lisp was designed as a paper language, McCarthy was sure it wasn't actually feasible to implement it on that day's hardware."
Ownership vs full immutability,uxtcme,2022-05-26 06:51:36,"Mutation with multiple references scattered around the stack or across threads is a source of correctness and safety issues. 

To address this, Rust's ownership system, or more generally a linear type system, provides a way to track the number and type of references extant at any point and allows the programmer to enforce constraints on them. Added constraints make programming more challenging as ownership has to be manually propagated to and back from the called function(s). 

Alternatively, in a fully immutable language, a programmer is free from (type) constraints but has to manually ensure functions return all the values they modified. An immutable system is also inefficient in both space and time due to increased allocations. It is also extremely cumbersome to write --- if a function modifies an internal element of an object, it has to return the object and maybe even the element to the callee in case there are other future references to it. 

Given poor ergonomics with either, the difference between the two comes down to the runtime efficiency. It makes sense for a low-level language like Rust to prefer an ownership-based system. However, if one were making a higher-level language, does it make any sense to prefer ownership/linear types over immutability? Surely immutability is a whole lot easier to explain to someone than the borrow checker.","This is exactly what I'm doing with [Inko](https://inko-lang.org/): it uses single ownership, but doesn't come with a borrow checker. Instead, the model is based on [this paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.470.8030&rep=rep1&type=pdf). For concurrency I've adopted a somewhat simplified approach of the one used by Pony (which in turn is based on [this paper](https://www.microsoft.com/en-us/research/publication/uniqueness-and-reference-immutability-for-safe-parallelism/)). You can read a bit more about this [here](https://inko-lang.org/news/inko-progress-report-april-2022/#header-type-safe-concurrency).

Long story short: I think single ownership can absolutely work in a higher level language, it just hasn't really been done by any mainstream (or even close to it) language. I personally find the model of Inko very promising, but I may be biased :)"
Why is PHP like that?,uk0r4t,2022-05-07 07:56:04,"So u/wk_end linked us to [this thing](https://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/) about how PHP is terrible. I had to read it in stages, partly because it's kinda long and partly because I kept recoiling in horror. I didn’t realize how bad it was, I’ve only messed with it to write very short middleware to connect desktop apps to an online database. (Now I want to wash.) And yes, it was terrible when I used it, I got errors I didn’t understand because I had newlines at the end of my script. My own language is written with the primary use-case of annihilating PHP and I didn’t realize how bad PHP is.

But what are the historical reasons? I don’t see how it got so awful, and the better my own language works, the more I am puzzled by why PHP doesn’t.

I mean, consider for example PHP’s terrible, terrible implementation of what they claim are “closures”. But *I* have closures. But I’m a complete n00b at: (a) language design (b) language implementation (c) the language I’m writing my language in. But I have closures, they work perfectly, and once I had the syntax for ordinary lambdas I hacked together the semantics of closures in half an hour. So could anyone else who’d read the same book (singular) I’ve read. It’s not hard.

And again and again, when I look at the list of things that are terrible about PHP, I think: “But I didn’t have to shed blood and tears to avoid any of that stuff. It’s not harder to write things that *aren’t* broken.”

So … why is PHP like that? From the outside it doesn’t even seem like the devs always took the easy option. It’s like they wrote down all the possible options (no matter how difficult, cumbersome, and stupid) on separate slips of paper and then drew one at random out of a hat.","It's basically a combination of two factors:

First, Rasmus Lerdorf just wasn't a very good software engineer when he first wrote it. He made a lot of mistakes and he wasn't a stickler for consistency. He was cobbling together a little thing for his own personal use and he didn't put a lot of care into making sure it all hung together gracefully. He didn't know a lot about programming language implementation so he tended to do what he could get working quickly that was good enough for his needed.

There is no shame in any of this. If you're building a shed in your backyard, you are under no moral obligation to design it to NASA engineering specifications.

But then PHP got *massively* popular virtually overnight thanks to the success of shared web hosts with built in PHP support. Users started writing a *ton* of PHP code that was executed on servers they had little control over and no ability to control the version of PHP that it ran.

That meant that all of Lerdorf's hobby project mistakes got trapped in amber henceforth and forever.

Anyone could design a new language from scratch better than PHP, *even Rasmus.* But PHP doesn't have that luxury because there's billions of lines of code in it already written by people who are often barely maintaining it at all, running on tons of servers that are out of their control."
How Futhark represents values at runtime,owcyzc,2021-08-02 19:57:21,,"Thanks much for the post! Some key excerpts for me: ""our intermediate representation, for simplicity, does not have a C-like union type"" and ""various optimisation passes do not deal with tuples at all - just scalars and arrays of scalars. I think this simplicity is key to why we’ve been able to implement such aggressive optimisations."""
The new Oberon+ programming language – modern simplicity,olgfrt,2021-07-16 21:02:25,,"cool, does this have anything to do with https://en.wikipedia.org/wiki/Oberon_(programming_language)"
Which language to write a compiler in?,k3zgjy,2020-12-01 00:44:01,"I just finished my uni semester and I want to write a compiler as a side project (I'll follow https://craftinginterpreters.com/). I see many new languares written in Rust, Haskell seems to be popular to that application too. Which one of those is better to learn to write compilers? (I know C and have studied ML and CL).

I asking for this bacause I want to take this project as a way to learn a new language as well. I really liked ML, but it looks like it's kinda dead :(

EDIT:
Thanks for the feedback everyone, it was very enlightening. I'll go for Rust, tbh I choose it because I found better learning material for it. And your advice made me realise it is a good option to write compilers and interpreters in. In the future, when I create some interesting language on it I'll share it here. Thanks again :)","Haskell is awesome for writing compilers in, *but* your project might end up being more about learning Haskell than about writing a compiler, if it's the first thing you do with Haskell."
Why Typing Erlang is Hard: Standard Erlang,jpic88,2020-11-07 09:59:00,,And the HN discussion: https://news.ycombinator.com/item?id=25007045
basil: A Lightweight metaprogramming language,jfqdxg,2020-10-22 09:39:54,,So a Lisp with un-Lisp-y syntax with Futamura projections?
What was so fundamentally wrong about Flash and right about Javascript?,iksnct,2020-09-02 04:52:55,"UPDATE: Some people have been answering the question ""Why did Flash die?"" but my question really is ""Why couldn't Flash be fixed to be more like JS?""

I was watching [this](https://www.youtube.com/watch?v=HEdPX8pt_DQ) video about Java and it's beginnings as a browser language. In there they cite how Java on the browser is dead (and has been for quite a while), likening it to Flash. AFAIK both Java and Flash were sources of security vulnerabilities and overall clunkiness.

What I don't understand is, how is Javascript different? For me the main thing similarity is Java, Flash and Javascript are all interpreted/JITed in the browser; the main difference is Javascript comes baked into the browser and Java and Flash were provided as plugins by Oracle and Adobe (though many browsers used to come with flash reinstalled IIRC).

What was so fundamentally broken about Flash, that it never could have been reworked to be stable and ""safe"" the way Javascript is? I guess the answer will be ""CoMPatiBIliTY WiTH OLdeR VeRSiONs.""","- JavaScript was properly sandboxed, while Java and Flash were not (they tried to be, but ultimately failed).
- The JS engine was shipped by the browser, which meant that JS could evolve alongside web standards. Java/Flash were independent, so they could not.

These are the two main differences. The first was actually originally a *strength* of Java and Flash. For example, for a long time, JS could not access the clipboard. So you'd embed a tiny Flash applet whose sole purpose was to copy/paste things, since it had (mostly) unsandboxed permissions. The downside is, this meant that any Flash applet could steal/hijack your clipboard as soon as you loaded any website. The same situation happened for e.g. file storage, audio, etc. In each case, protecting users from bad or annoying websites required that the Java/Flash applets actually allowed the user to control these aspects, which was difficult, since they were just plugins to browsers and not part of browsers themselves.

The second reason I think is ultimately why they failed to survive. Users would put up with terrible security if they got functionality out of it (since this is usually what happens), but eventually, JS got new features, and Java/Flash could not. They typically owned a tiny box somewhere in the page (sometimes, the entire page). But their box was *not* built from web technologies. So, for example, when CSS added 3D transformations and animations, if you were building your site with Java or Flash... You couldn't use them. Because the applets did not embed web tech; they could only be embeded *in* web tech.

The result was that in order to be competitive, Java and Flash needed to have a version of *every single feature of the web* that managed to be better than the JS version. In the beginning, this was easy because of all the things they could do that JS could not. But once browser vendors decided ""well, there's no *reason* you can't do those things with JS"" they lost almost all of the ground they had covered immediately.

Eventually, JS got the ability to do pretty much everything that Java and Flash did (e.g. audio, webcams, localstorage, location, ...) but safer for users (real permissions prompts so random websites can't secretly spy on you!). Combined with the general security nightmare caused by their poor sandboxing, it's not surprising they died.

The last reason that JS won was because it couldn't die. It's used everywhere by everything, so it cannot be killed, no matter good of an idea that might be (similar to C). It has millions of horcruxes in the form of every website to every exist, which no one can afford to give up. Because of this, there was never a question of whether Java/Flash would *replace* JS- just whether they would survive alongside it indefinitely."
why differentiate between functions and lambdas ?,i0ks1k,2020-07-30 19:58:50,"Example in Kotlin:

    fun printAndReturnSum(a: Int, b: Int): Int {
    	val sum = a + b
    	println(sum)
    	return sum
    }

Alternatively we could do:

    val printAndReturnSum = { a: Int, b: Int ->
    	val sum = a + b
    	println(sum)
    	sum
    }

either way we would invoke via `printAndReturnSum(1, 2)` and get the same behavior, so my question is: why do we even need the `fun` keyword ? I feel like understanding higher order functions would have been much easier for me if the latter syntax was standard.  


I know that a function in a class is only loaded once and re-used by each instance of that class while a value is duplicated for each object, but couldn't that be detected and optimized by the compiler ?","Most likely for the convenience of recursion, I would assume. Either lambdas need to support late binding (I think that is the term) of `self` or a name or an explicit `fix` instrumentation.

If you're asking for Kotlin specific differences, I don't know because I've never used it."
An Introduction to Efficient and Safe Implementations of Dynamic Languages,hgiuyb,2020-06-27 07:44:51,,Is there a video of the lecture somewhere? This looks extremely interesting!
"Atto, an insanely tiny self-hosted functional programming language",avvma5,2019-03-01 04:15:05,,"It doesn't really have a purpose, but it's fun to play with."
Compiler Development: Rust or OCaml?,15jpmxe,2023-08-06 21:26:44,,"I feel like this is a well reasoned, but incomplete take on ocaml vs rust. It covered the benefits of using ocaml, but only glossed over the issues. It would be more complete if it compared performance, tooling, platform support, cross compilation, and ecosystem. (It mentions the ecosystem being a weakness on the ocaml side, but it doesn't talk about rust's ecosystem.) I think the conclusion might be more nuanced if it did cover those topics."
Forth as an intermediate language,11tsjq6,2023-03-17 22:25:15,"A few months (?) ago, there was a discussion here about using Forth as an intermediate language. I think that, as it often happens, this may have been a ""sub-discussion"" within a relevant question. Unfortunately, I cannot find it right now :/

So, more generally, just as C is sometimes used as an intermediate language, what are the arguments (or counter-arguments) for using Forth as an intermediate language?

Personally, I would be happy even with pointers to articles or concepts that demonstrate either a pro or con to using Forth for this purpose....but a bit of description around just posting a URL is always welcome :)

I have to say, I have made some very simple translations in terms of mapping basic language constructs (e.g. various assignments, iterations, function calls) and...they seem to work. Recursion might be a bit more challenging as it might be possible to do it without creating the usual stack frame.

The thing is that Forth can run on anything....so using it as an intermediate language would make the re-use of existing, robust, debugged code (but written in another language), very easy to re-use in all sorts of targets.

Ofcourse, I may be missing something huge here, but, would there be a formal or standard way to ""translate"" compilation between a register and stack machine? If a language like C, Pascal or Java was to be compiled down to Forth, would it lose anything?",Following. I think Forth would make a good stack vm.
"Lambdas, Nested Functions, and Blocks, oh my! (On the lambdas proposal for C23)",omau26,2021-07-18 03:22:40,,"Why not use a lambda syntax based on a cast-block-to-function-pointer syntax (analogous with compound literals) and just use the existing struct mechanism for captured environment variables? The proposal seems to add more weirdness without a substantial benefit (the concision of the capture list seems like a small benefit with a big cost).

Using a struct type for objects representing environments lets you easily do things like create an array of environments and use those environments together with function pointers determined elsewhere. And it introduces no dependency on exotic things like `auto` and `typeof`.

Using an inferred return type also seems like an awkward fit for C.

    #include <stdio.h>
    #include <stdlib.h>

    typedef int compute_func(int i, void *user_data);

    int
    compute_with(int i, compute_func *user_func, void *user_data) {
        return user_func((i + 2) * 3, user_data);
    }

    int
    main(int argc, char **argv) {
        struct env {
            int i;
        };
        int i;
        if (argc != 2) {
            fputs(""usage: lambda i"", stderr);
            exit(1);
        }
        i = atoi(argv[1]);
        i = compute_with(
            1,
            (int (*)(int i, void *untyped_env)) {
                struct env *env = untyped_env;
                return env->i + i;
            },
            &(struct env){i});
        printf(""%d\n"", i);
        return 0;
    }"
CORE - My Proof Assistant,od9lfw,2021-07-04 08:21:02,"Back in March I started my fourth attempt at making a proof assistant, and to my surprise I actually succeeded. A proof assistant is a programming language for mathematical logic and proofs. I wanted to make my own simple proof assistant for set theory, which is a logic in which the objects are ""sets,"" i.e. collections of other objects. Since the language's creation, I've been able to prove some basic facts about the natural numbers and construct addition and multiplication from scratch (using the ZFC axioms). I also made a [website](https://been-jamming.github.io/CORE) where you can look at all of the results I've proven in CORE. In the website you can recurse through results by clicking on the bold words in the code for the proofs. I have a repository for the language [here](https://github.com/been-jamming/CORE). For the people who are interested, I will describe the language in more detail below.

The main philosophy of the language is simplicity, and if it wasn't the language would be far from finished. Thus there are only three things you can do in CORE: create axioms, make definitions, and construct proofs. Of course, axioms, definitions, and proofs can refer to previous definitions, and proofs can refer to previous axioms and proofs. Proofs are written in a constructive style inspired by the [BHK Interpretation](https://en.wikipedia.org/wiki/Brouwer%E2%80%93Heyting%E2%80%93Kolmogorov_interpretation). Proofs are named and scoped. Inside any proof and even at the global scope, objects (sets) can be constructed using existentials. For example, if you have an axiom which says there exists a set with no members, then you can *construct* a set with this property. In the process you give it a name and henceforth any result can refer to that object by name. In my example proofs, I construct the `NATURALS` and the `INTEGERS` so I can prove the results required for arithmetic. Inside of proofs, variables store true or proven statements and new true statements can be produced and stored using a few simple deduction rules. 

For anyone wondering how to read the proofs on the website I linked, here's a quick key that might help:

`*` for all

`^` there exists

`->` implies

`&` and

`|` or (also used around an object name when it is first constructed)","Always happy to see new developments in proof assistants. :) May I ask why you chose set theory over type theory? This seems like an uncommon choice for modern proof assistants. I think this means you can't leverage computation in proofs (e.g., for proofs by reflection), and you can't use modern innovations like univalence (I assume?). But it's closer to mainstream mathematics, so maybe that's a benefit? What kind of logic do you have? First-order?"
Metalang99: Full-blown preprocessor metaprogramming for pure C,medyz8,2021-03-27 21:39:06,,Today is the first stable release -- v1.0.0. Look forward to your feedback!
C Is Not a Low-level Language,kiqr05,2020-12-23 18:30:02,,"So thinking & reasoning in parallel is the evident ""low-level"" characteristic that modern machines poses, while humans are unable to do that well enough (may due to [the magical number seven](https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two)), so we humans have to stay at high-level. Then any programming language meant for code in it to be written by humans, can only be high-level."
The unreasonable effectiveness of the Julia programming language,j8jk0c,2020-10-10 20:22:48,,"Julia is great, but it suffers from the same problem Python does (shipping code to end-users) - not much of a problem for scientific usage, but a problem in general programming and gamedev."
Is there a fundamental reason why dependently typed Vectors can't compile to O(1) access vectors?,j7gjbd,2020-10-09 00:43:57,"(note: all code in this post is in a Agda-like pseudo-code)

E.g. `Vec T n` in Agda: https://github.com/agda/agda-stdlib/blob/master/src/Data/Vec/Base.agda

Since we know the size in compile type, it should be possible to compile them to something similar to `std::vector` (or maybe `std::array<N>`) in C++ right?

As a comparison: natural numbers are defined inductively in Agda and Idris, like this:

    data Nat : Type where
      zero : Nat
      suc : Nat -> Nat

Therefore, syntactically `500` is a list of 500 `suc`s. But both Idris and Agda optimize this and compiles `(suc ... (suc N))` to GMP integer `500`. This way arithmetic operations are fast. E.g.

    + : Nat -> Nat -> Nat
    n + zero = n
    n + (suc m) = suc (n + m)

^^^ this + operation syntactically involves O(M) recursions (M is the size of rhs). But under the hood it compiles to a single `gmp_add(lhs, rhs)` operation.

I'm curious why they (Idris, Agda and similar dep typed languages) don't apply a similar optimization to vectors. This way:

    access : Vec T n -> (k : Nat) -> (k < n) -> T
    access (x :: xs) zero _ = x
    access (x :: xs) (suc n) proof = access xs n proof

Although this involves O(n) recursion (n is the size of first param) it could compile it to an O(1) `vec[k]`. Which is safe because a dep typed language can have the proof `k < len(vec)` at compile time. Or otherwise we can return `Maybe T` which will have value `nothing` at runtime if `k >= len(vec)`.

So I could ask this question in r/Idris r/Agda r/depent_types etc but I was wondering if there is a trivial or fundamental reason why this optimization will not work in general (unlike Nat optimization).

Context: I'm writing a dependently typed language *as an exercise* (it's not a serious effort, I'm just a learner). I've been thinking about this for a while, but didn't attempt to implement it. I'm really sorry if this is a stupid, basic question or if I'm missing something obvious.

Further context: I'm fully sold to the idea of purely functional programming and appreciate the lack of side-effects. But my only reservation is the ""necessary"" O(logN) inefficiency (tree-search instead of array-access). It's not a big deal, but I'm wondering if it's possible to solve it...","There's a dependently typed language ATS which has real arrays. I don't know the details, but they work out constant-time access [here](http://ats-lang.github.io/DOCUMENT/INT2PROGINATS/HTML/x3618.html).

I wouldn't be so sure about this, though:

> ""necessary"" O(logN) inefficiency (tree-search instead of array-access). It's not a big deal,

It *is* a big deal performance-wise. Not the log N, but the lack of contigiousness. Cache misses galore, my friend. This is why the Mutable Array is and always will be a data structure unsurpassed."
"An Incremental Approach to Compiler Construction - Jaseem Abid [video, Papers We Love London 2019]",ferjxa,2020-03-07 14:49:50,,"Thanks for sharing the talk here u/cutculus! I'm the speaker, AMA and I'll try my best to help.

Edit: Slides are here: [https://blog.jabid.in/talks/inc/pwl](https://blog.jabid.in/talks/inc/pwl)"
"Parse, don’t validate",dszj7b,2019-11-07 23:46:45,,"This seems like a bad idea overall, because it doesn't compose. For example, imagine that I have more than one condition I would like to check. Let's say that I want my list to be non-empty _and_ sorted. Should I create three new types, one for non-empty lists, one for sorted lists, and one for lists that are both non-empty and sorted? What if I have three conditions (say: non-empty, sorted, and unique)? The number of new types that I'd need to potentially create will be _exponential_ in the number of conditions that I'd like to check.

It also doesn't compose well with functions. For example, suppose that the `NonEmpty` type did not exist in the standard library, and I wanted to build one to use the method described in the blog post. For whatever reason, I have a `NonEmpty` list, and now I want to sort it. I can't use the built-in `sort` function to do this, since this will cause me to ""lose"" the check that the list is non-empty. I need to _redo_ the check. Sure, we can hide this check inside a `sort` function that is specialized for the `NonEmpty` type, but the check remains: it even exists inside the standard library function for sorting `NonEmpty` lists. And if we were implementing a `NonEmpty` type for ourselves, we would need to implement sort or any other function that needs to be called on `NonEmpty` ourselves, rather than using the standard ones for list directly.

In comparison, the method that just does the checks seems to compose better and require much less programmer effort. I don't buy the rationale described in the post as to why this method is bad."
The Fascinating Influence of Cyclone,dbh4jn,2019-10-01 03:48:36,,"Superb write-up, I learned a lot."
Steady Typing,c1899u,2019-06-16 17:45:39,,"Interesting video and concept.

&#x200B;

I will say that ""Run despite of errors"" can be done in inferred type languages, e.g. \`defer-type-errors\` in Haskell. I also think it's unfair to put dynamic types and inferred types on equal footing for propagating API changes, even if the type errors are sometimes a bit weird, inferred types are still a dream compared to dynamic types when propagating changes, just follow the compiler errors."
Gleam v0.25 released with a new approach to fixing callback hell,z3lmew,2022-11-24 22:39:59,,"> This feature is largely inspired by Python’s `with`

That's what I was thinking of as I read the examples. I see now that it's closer to Koka's [`with` statements](https://koka-lang.github.io/koka/doc/book.html#sec-with), where the documentation concludes with:

> Using the `with` statement this way may look a bit strange at first but is very convenient in practice – it helps thinking of `with` as a closure over the rest of the lexical scope.

I think I prefer Gleam's keyword `use`, but it still looks a bit strange to me. I wonder, is there maybe a syntax that could make it more intuitive? And are there any other downsides to doing this, apart from familiarity?"
I started working on a speakable programming language: Have a look at the initial prototype,uuv7u8,2022-05-22 04:23:48,"For some years already I have some minimalistic conlang in mind.

This conlang should only have very few grammatical elements, be very expressive, and should basically be unambiguous.

These properties, which are similar to Lisp, would also be suitable for a programming language. So I started to create one yesterday.

Here you can try the initial prototype and read more about it: [Tyr](https://gitlab.com/Tyre/tyr)

Just read it if your interested.

But anyway, these are the most important features:

- currently it only supports basic math
- it's a real conlang with phonetics, phonotactics, syntax and grammar and so it doesn't use the typical terms and keywords
- the most important idea is infinite nesting without relying on syntax or any explicit words to represent parenteses (like lojban)

Some simple examples:

```
junivan: -(1 + 1) 
nujuzvuv: -2  - 1
an'juflij'zvuv: 2 + -3
```","Read through the doc out of intrigue. I have some experience learning lojban and have been working on a natural programming language environment.

Not much to add here, but I am curious how you imagine editing existing programs that have been spoken.

Does spoken language get written to file as evaluated version? Or must you edit word by word?"
Introductory resources to type theory for language implementers,uhgl4o,2022-05-03 21:52:09,,"I'm slightly cranky that they cited Pierce and Harper, basically the two canonical intro-to-type-theory books, and then said they didnt read them. They are very good books! And Harper, at least, is free online."
Is there nuance to 'multiple return values' that makes it innovative?,syjl2m,2022-02-22 17:41:11,"This feature seems to be put in the light of an ""innovative"", ""modern language feature"", often marketed as one of the strengths of a programming language.. which puzzles me to no end.

Multiple returns have existed for the longest time as records, structs, objects, tuples, outparams, and whatnot.. With outparams, no optimizations are needed, otherwise there are [optimizations](https://blog.janestreet.com/ocaml-is-smarter-than-i-thought/) that [ensure](https://stackoverflow.com/q/42411819/10831589) unboxing/passing on registers at function boundary, it seems that the ""multiple returns"" problem has been solved long ago.

So is there something new langs like Go add to this formula? It seems tuples in general are second-class in Go, and only the special case of multiple-returns is first-class.","> Multiple returns have existed for the longest time as records, structs, objects, tuples, outparams, and whatnot.. With outparams, no optimizations are needed, otherwise there are optimizations that ensure unboxing/passing on registers at function boundary, it seems that the ""multiple returns"" problem has been solved long ago.

The bare multiple return values are also pretty old. E.g. they are in [Common Lisp](http://www.lispworks.com/documentation/lw60/CLHS/Body/03_ag.htm)."
"On the design of APLs, LISPs, and FORTHs: a short essay",pt3mtl,2021-09-22 17:12:38,"Syntax is ever a contentious subject. Nominally it doesn't matter, and for the most part our IDEs pick up the slack.

LISPs and FORTHs (i.e. concatenative languages) tout the property of homoiconicity. Like the 'everything is an object'-ness of big OO languages, except at a syntactical level.

Homoiconicity is not about simplicity of metaprogramming, but the simplicity of the parser. In FORTH in particular, the parser can be written in a few hundred assembly instructions, and can be re-bound at runtime, allowing the language to bootstrap itself to more complex syntax. Meanwhile, parsing S-expressions is little more than balancing parentheses and CommonLisp allows hooking macros up inside the parser to change the syntax directly.

While homoiconicity does have this desirable property, and meta-programming to boot, the tradeoff is that code is distinctly harder to read.

Most concatenative programming languages in the FORTH family, such as Factor, has extensive batteries-included meta-programming to allow e.g. the capture of local variables. The well known quadratic root formula is a nightmare to do purely stack-based.

CommonLisp similarly has a lot of heterogenous syntax for quotations, literals of both scalar and compound data structures, and loop constructs.

Which brings me to APLs. APLs are hard to read. APL itself, J, and even innovative newcomers like BQN (which does some great work, look into it if you are at all interested in array programming.)

The thing APLs bring to the table in exchange for this obscure syntax is extremely high code density. Very complex (not complicated) operations on data can be expressed simply as compositions more base operations. In terms of pure data-manipulation they are hard to beat.

However, the syntax is an expensive price to pay. Array programing can these days be done in easier-to-read languages with mere libraries (Numpy comes to mind in particular) to virtually identical effect.

At the same time the compositional nature of the array syntax is, at a most basic level, a fairly restricted form of higher-order functional programming.

Which brings me to the conclusion:

Is weird syntax bad? _No!_ By all means, push the envelope. Learn these languages, design even weirder ones. This is how language innovation is made.

But in doing so, be aware — not wary, just aware: most people have encountered infix notation is elementary school. Most people are accustomed to words forming sentences. The more you deviate from the two, the higher a price you pay in cognitive load, the need for code presentation tools, and barrier-to-entry for newcomers.

Everything is a trade-off.","I spent literal decades worrying about syntax, thinking that better, more natural syntax would lead to better tools, would make it easier to solve problems. To some degree I still think that's true. You can't do complex arithmetic on paper with roman numerals; notation can be a powerful tool.

But these days I think the thing you are always fighting - with a new language, a new tool, an unfamiliar library, code you wrote 6 months ago - is learning curve. How much is familiar, how much is new, how many new concepts do I have to internalize in order to make progress. IMO syntax is one of the smaller aspects of this. The much bigger challenge is to convince people to approach solving their problems in an unfamiliar way.

As an example, Chuck Moore, inventor of Forth, famously says that local variables are harmful. People who are familiar with programming languages with local variables find this absurd; you're supposed to do everything on the stack, with stack-manipulation words? Doesn't that get insanely complicated??

> The well known quadratic root formula is a nightmare to do purely stack-based.

But Chuck Moore never said ""manage all of your function's inputs on the stack"". That's an assumption that people make coming from languages with local variables; that your function should be a black box, and all of its internal state must be kept inside.

How you solve this problem in a Forth without local variables is actually very simple: you use global variables. The quadratic root formula is a closed system; it doesn't have to be re-entrant*. There's no reason why global variables don't work, except that it is considered unthinkably bad style in other languages.

And putting your parameters into global variables actually brings benefits, because now it becomes trivial to factor a complex calculation into many different tiny words that all use the same internal parameters to co-operate with each other. When you have local variables, it's hard to break out pieces of the work into their own self-contained units; if the local state can be shared between them, suddenly it's trivial. And now you can more easily debug each tiny piece, and interactively inspect the current state of a partial calculation, without any kind of special tooling - just the regular old interactive Forth terminal. Maybe you can even re-use some of the pieces for related calculations down the road. Chuck Moore says that Forth is writing short words; local variables get in the way of that.

Forth's design trade-offs assume that the person implementing the language is the same person implementing the program, and so why make more work for that person by implementing complex language features that they don't need? The syntax follows from that way of thinking, not the other way around, and teaching that way of thinking is _hard_. The only way I learned it was to write my own Forth on a resource-constrained computer.

I'm currently implementing a lispy language that compiles to 65816 machine code, targeting the Apple IIgs. It's not ""functional"" at all; semantically it's kind of like C if C had unsigned int and long as its only data types. No lists, no symbols, no structs, just function calls and math. But the syntax is prefix notation with parentheses, and you can define special forms to extend the compiler. (It's actually implemented as a Fennel library, so I don't have to write a parser.) I showed it to some IIgs hackers, folks generally happiest with low-level assembly code, and one piece of feedback stuck with me - ""I'm actually fascinated by the Lisp code you use, it looks like something I can understand.""

Syntax - especially syntax that's as simple and consistent as Lisp's or Forth's - isn't the hard part of learning. It's just the first, most visible step.

* (If you've got a real fancy Forth with pre-emptive multithreading you might have to define a thread-local variable instead, but IMO pre-emptive multithreading with a fully shared state is a complexity disaster and Forth is much better suited to simple co-operative multitasking. If you _did_ need to make your thing re-entrant, you can do that too - you just need to be explicit about it, and define helper words to manage it.)"
Leaning toothpick syndrome,ogay2s,2021-07-09 00:36:42,,"I hacked up lambda calculus in shell once using return (don't ask), eval, and string interpolation.  For a (curried) function of n parameters, you need ~2^n backslashes.  Fun times...

EDIT: of course you can write auto-curry, but where's the fun in that?"
CoqPL: Verifying a compiler through equational means,lxzgj9,2021-03-05 08:20:16,,"Oh thanks for posting this :-) 

I'm the speaker in this talk, happy to answer any question!"
Cakelisp: a programming language for games,kh6gh2,2020-12-21 08:54:35,,"[See also x-post on /r/gamedev](https://www.reddit.com/r/gamedev/comments/kh1p0a/cakelisp_a_programming_language_for_games/)

After years of dealing with points of frustration in C++ land, I've created my own programming language. It emphasizes compile-time code generation, seamless C/C++ interoperability, and easier 3rd-party dependency integration. It's like ""C in S-expressions"", but offers much more than just that.

I had a hard time trimming this post down because of how excited I am about the language. Feel free to skim and read more in the sections that pique your interest :).

I don't expect everyone to love it and adopt it. I do hope that some of the ideas are interesting to fellow programmers. I found it eye-opening to realize how much better my development environment could become once I opened this door."
Which lambda syntax do you prefer?,jljxja,2020-10-31 23:08:00,"

[View Poll](https://www.reddit.com/poll/jljxja)",`x => x + 2`
Resources for the working programmer to learn more about the fundamentals and theory of programming languages.,giaeht,2020-05-12 20:36:45,,"You might like ""Zen Code"" at [wiki.hackerspaces.org](https://wiki.hackerspaces.org).  If they've buried it for not being content-relevant, try adding User:average/Hacking\_with\_the\_Tao."
Do you know any interesting talks about PL design/compilers/interpreters?,fxc504,2020-04-09 02:42:33,"Hi folks! 

Let’s share your favorite talks about whatever related to designing and creating programming languages! 

I’ll start first: 

William Byrd on “The most beautiful programm ever written” - https://youtu.be/OyfBQmvr2Hc

EDIT: putted title to the talk","This is an iOS note, so it might not be formatted well, but:


Philip Wadler - Linear types can change the world!

https://pdfs.semanticscholar.org/24c8/50390fba27fc6f3241cb34ce7bc6f3765627.pdf

Gilad Bracha - Pluggable Type Systems

https://bracha.org/pluggableTypesPosition.pdf

Gilad Bracha - A Slice Through the History of Programming Languages

https://www.youtube.com/watch?v=91fjAbsJdUo

Guy Steele - Growing a Language (I actually saw this many months ago)
https://www.youtube.com/watch?v=_ahvzDzKdB0

Trevor Jim - What are principal typings and what are they good for?
https://dl.acm.org/doi/pdf/10.1145/237721.237728?download=true

Eeve - Let’s stop copying C
https://eev.ee/blog/2016/12/01/lets-stop-copying-c/"
"Single Ownership and Memory Safety without Borrow Checking, Reference Counting, or Garbage Collection",14a79va,2023-06-16 00:47:20,,"> Any struct that stores a non-owning pointer would instead store an index (or ID) into some central data structure.

That’s just reinventing pointers. There can be dangling indices the same way as dangling pointers."
great works in programming languages,144ekk3,2023-06-09 00:55:26,"hello you all,

&#x200B;

heres a list by Dr. Pierce, the professor who wrote Types and Programming Languages and Software Foundations, about great papers in the PL field.

[https://www.cis.upenn.edu/\~bcpierce/courses/670Fall04/GreatWorksInPL.shtml](https://www.cis.upenn.edu/~bcpierce/courses/670Fall04/GreatWorksInPL.shtml)

&#x200B;

enjoy! thanks","Nice list. Personally, I'd add [""Finally Tagless, Partially Evaluated""](https://okmij.org/ftp/tagless-final/JFP.pdf) and [""Typed Tagless Final Interpreters""](https://www.okmij.org/ftp/tagless-final/course/lecture.pdf) by Oleg Kiselyov et al. The papers introduce the Tagless Final (TF) approach to defining and manipulating languages, and generally any kind of data. In comparison with ADTs, TF is more extensible (solves the expression problem), type-safe, and exhibits no overhead of run-time tags.

What is particularly interesting to me is that if we look at the usual syntax of GADTs (Generalized Algebraic Data Types), as found in OCaml or Haskell for example, it resembles TF a lot."
"Do you know any books about low-level (C/C++, Rust level) programming language design?",xktl8i,2022-09-22 14:26:30,"(First time poster here!) I’ve become curious about their design ideologies, and how they work under the proverbial hood. Thanks!","The [rustc dev guide](https://rustc-dev-guide.rust-lang.org/index.html) contains an enormous amount of useful tidbits that I found to be really handy when I was implementing my own trait solver. It's also got information about tonnes of other quirky, often-unexplained aspects of compiler design and type systems."
Crafting Interpreters,uiyhae,2022-05-05 22:18:31,"Hi all, I have a quick question regarding the online resource Crafting Interpreters by Robert Nystrom.

I just finished my college compilers class and felt that I didn’t get everything out of the course that I wish I did. I’m particular, virtual machines. By the time we arrived at that unit I was bombarded by other course work and didn’t get to truly understand that phase of compilation.

That being said, I want to work my way through crafting interpreters to get real experience using what I learned (and learning what I didn’t).

For those who worked through this book, about how long did it take you to complete it? Roughly how long was each section?

Thanks all!","I recommend reading all as fast as you can first, THEN reading it slowly again.

This is a strategy of mine with dense, useful, wide-range knowledge. Your second time will be better.

---

Also, do not worry much about the actual speed of learning. Doing it slowly with focus is much more efficient than trying to to do a run on it..."
How do you choose your language's filename extension?,uejopc,2022-04-29 19:22:31,"Basically what the title says. I've seen several practices regarding filename extensions in big existing languages, for example:

* The full name of the programming language (.java, .php). This obviously works better with short programming languages, imagining having a .haskell file.
* An abbreviation of the name (.cpp, .cs, .py).
* Seemingly random letters coming from the name (.rs, .hx, .pl).

The language I'm currently working on is called Dauw, short and probably memorable, so I'm tempted to use .dauw for my script extension, but as far as aesthetics go I dislike the full name in the extension, so I'm also opting for something like .dw or .dau.

How have you all selected a proper file extension for your language?","I vote .dauw

My reasoning is that:
- There’s no ambiguity - it’s the name of the language.

- Every 2 or 3 letter file extension is already taken, so let it go. (.dw and .dau are both already gone). 

- 4 letters really isn’t hard to type."
Alternatives to LISP,pcuhfq,2021-08-28 03:40:32,"I love how simple the basis of a LISP is and how well you can extend it. However, for my next project I don't want to write yet another LISP again.

What are some other extremely customizable languages that are not LISPs? Some more ""obscure"" languages would be interesting to (like stack based languages).

What I am looking for are languages that

* Have a minimal core syntax
* Have powerful meta programming capabilities
* Allow for super readable code
* Preferable are not just a LISP in disguise

EDIT

To clarify: I am looking for inspiration to design (and implement) a new language. The use-context will be similar to emacs lisp in that it will be used as the configuration language for an editor.
It does not have to be super useable (since no one will use it anyways), but rather it should be an interesting project.
That's also why I want to look into some more ""obscure"" paradigms.","[Red (remake of Rebol)](https://www.red-lang.org/) fits the bill. Or you could look at [Factor](https://factorcode.org/). If you want to try logic programming, you could also check out [Mercury](https://mercurylang.org/). I once had a colleague who swore it was the best programming language ever - not sure I agree but if you're into logic programming it's worth giving it a try. Or, try [Haskell](https://www.haskell.org/), though I wouldn't call this obscure."
Static Integer Types,o5k2dw,2021-06-22 18:41:40,,"This is a good article.  It's unlikely that getting integers right will earn your language much praise, but I can guarantee that you will receive complaints if you get them wrong - and getting them wrong is quite easy.

Unless you're designing a (relatively) low-level language, the distinction between data and address widths can probably be safely disregarded, and you can stick with only fixed-size integers.  The compiler will then use whichever pointer and offset sizes are needed for the specific machine, without any impact on the surface language semantics.  Although if you expose things such as counting the number of elements in an array, you better not pick an integer type with too few bits.

The section about converting (casting) between different integer sizes is useful for every language with fixed-size integer types.  I agree with the author that implicit conversions are dangerous.  Particularly, implicit conversions between integers of different signedness can lead to terrible bugs.  I have become a believer in having explicit size-extend/zero-extend functions for these conversions, just to make it completely clear what is going on.  I recently had to write some code in Standard ML which does provide various fixed-size integer types, but the only way to convert between them is to pass through the standard `LargeInt` type (an arbitrary-size integer).  The standard recommends that compilers optimise conversions between fixed-size types that go through this type, but I still dread the potential performance impact.

As an aside, I think that unsigned integers are useful even in high-level languages, not just systems languages.  They are very handy whenever you need to do modular arithmetic (e.g. for cryptography), or use integers for bit-level tricks.  I don't think this is only useful for ""low-level"" languages - my own language is very high level (purely functional, parallel), but it's not unusual to use unsigned operations to encode data or implement algorithms.  I agree with the article that using unsigned integers to encode things that are ""semantically"" never negative (like counts in a shopping cart) is probably a bad idea.  Unsigned integers are for representing bit vectors, not for encoding natural numbers.

Regarding overflow, I think a fairly safe default is to raise an error (panic, exception, whatever) by default on signed overflow, but not for unsigned overflow.  That's what SML does as well, I think.  This also fits with my suggestion of only using unsigned integers when you specifically need bit vectors or modular arithmetic, *never* just to express that you want natural numbers (I'd suggest a separate arbitrary-size `nat` type for that)."
My own LLVM based compiler,lexix7,2021-02-08 06:38:32,"Just to learn more about compilers magic, I've been implementing a programming language (all hand written) since 16/09/2019. In these days I'm finally generating some LLVM code.

Some test's screenshot: [Test](https://ibb.co/QvphQMM)  

For more information about the project:  [GitHub](https://github.com/Carpall/mug)",amazing!
A Novel Approach to Generate Correctly Rounded Math Libraries for New Floating Point Representations,j3vdqb,2020-10-02 22:22:46,,"As soon as I saw ""correctly rounded"" and ""new floating point representations"" in the same sentence I knew John Gustafson was involved.

I have to say though, his work on posits makes thinking about floating point numbers interesting.

EDIT: Ah, this is follow-up work to Gustafson's Minefield approach! Cool stuff :)

EDIT2: As I understand it, this is the ""core"" of the algorithm in a nutshell, minus all the smart tricks to solve the problem with less work like range reduction:

- first identify the correct outputs *after rounding* for each input (or if that's too big of a domain, start with random sampling on a subset)
- based on the rounded outputs, determine the acceptable *pre-rounding* output ranges for each input
- use linear programming to find the right coefficients for a polynomial satisfying these constraints

This as opposed to the existing methods, that (again, if I read the paper correctly) essentially try to optimize the polynomial coefficients for the least-error relative to the correct results, then determines how much precision is needed to get the polynomial to round correctly for all inputs.

It kind of intuitively makes sense to me that the new method would optimize better, since it gives the polynomial more wiggle room while also optimizing for the desired output directly, instead of hoping that we have enough precision that truncation goes right"
Program Logics for Certified Compilers (this textbook is now free),iocas5,2020-09-08 01:51:39,,"Hi folks, Andrew Appel just announced (on the [coq-club@inria.fr](mailto:coq-club@inria.fr) mailing list) that he is making this textbook available for free. This book is mostly about separation logic, so it might be a bit too theoretical for this subreddit (based on an informal poll I conducted in a separate post). But this looks like a great resource that collects a lot of knowledge about separation logic in one place, so I thought I'd share it in case anyone here is interested in the formal verification of pointer-manipulating programs (and the development of languages that are conducive to such verification?).

Here's the original announcement:

>Announcing:  free author-page access to   
>  
>[*Program Logics for Certified Compilers*](https://www.cs.princeton.edu/~appel/papers/plcc.pdf), by Andrew W. Appel with Robert Dockins, Aquinas Hobor, Lennart Beringer, Josiah Dodds, Gordon Stewart, Sandrine Blazy, and Xavier Leroy. [Cambridge University Press](https://www.cambridge.org/us/academic/subjects/computer-science/programming-languages-and-applied-logic/program-logics-certified-compilers), 2014.  
>  
>Our publishing contract allows the authors to make a complete PDF available on the authors' personal web sites.   Initially I did not do so, because the publisher's e-book price was less than $15.  Now that the e-book price is an unreasonable $72, I am happy to [make it available for free](https://www.cs.princeton.edu/~appel/papers/plcc.pdf).  
>  
>Some parts of the book have been superseded by more recent work in the Verifiable Software Toolchain.   The part that may be most useful is the core explanation of how VST's step-indexed semantic model permits the soundness proof (with respect to the CompCert C semantics) of a higher-order separation logic for a language with function pointers.  
>  
>Sincerely,  
>  
>Andrew Appel  
>  
>Princeton University"
The Art of Code by Dylan Beattie (Rockstar PL author),g6unqj,2020-04-24 04:50:46,,"Thank you for sharing, it was beautiful."
A formally verified proof of the soundness of System F,avnjlk,2019-02-28 13:50:55,"Hi r/ProgrammingLanguages! For those of us who are interested in programming language theory/research, I have completed a Coq proof of the soundness of System F (the polymorphic lambda calculus) that may be useful in your own developments. [Here](https://github.com/stepchowfun/proofs/tree/master/coq/SystemF)'s the proof! It took me about two months. Of course this isn't the first proof of this result, but I think it's one of the easiest to extend. It's fully self-contained, assumes no external axioms, is relatively short (1579 lines), and is broken up into modules (for some reason, academics have a bad habit of putting all the code in one file). It should be fairly easy to extend with new features (e.g., tuples, existential types, etc.) if you have the relevant background (Coq, System F).

The repo also contains a (much smaller!) [soundness proof](https://github.com/stepchowfun/proofs/tree/master/coq/STLC) for the simply-typed lambda calculus, if that's all you need.

If you want to publish research in the field of programming language theory, having a machine-checked proof greatly helps your chances of being accepted. It can be helpful to extend an existing proof rather than starting from scratch. So I hope this gives you a good starting point!

Happy to answer any questions.",">	for some reason, academics have a bad habit of putting all the code in one file

Ugh this is so true and it's the worst haha. My advisor claims ""that way I can have everything open in a single window and I don't have to look around to see all the code"" but like... I dunno, I far prefer switching tabs/windows to see everything than scrolling through one gigantic file. Sigh.

Anyway, this is a really neat contribution! Thanks for posting it here!"
The Language Design Meta-Problem,ao5hy7,2019-02-08 00:30:46,,"Thanks for writing about this challenge. Largely, I agree with you, but I don't really believe this is accurate: ""I know of almost no one interested in addressing this meta-problem."" 

This hits the mark better for me: ""I don’t have any solutions."" Design always involves some painful trade-offs, and your meta-problem is no exception. The enemies of future-proof language designs are formidable:  ecosystem/backward compatibility, development cost/complexity, and the need for a timely ROI. From my perspective, languages have definitely improved over time, and I believe they will continue to do so. But this will likely continue to unfold in a messy Darwinian, survival-of-the-fittest way.

I agree with you that more powerful PL development tools would help a lot, especially given that languages are only going to get more complicated over time. LLVM is a godsend for me, but it is the only such aid I have found worth using. As you say, a similar toolkit that simplifies IR handling and semantic analysis would go a long, long way. Ditto for tools/libraries that made it easier to plug a PL compiler into editors/IDEs, linters, language servers, package managers, debuggers, etc. Making such tools is difficult, expensive, and likely not profitable. It would have to emerge as a labor of someone's passion, much like LLVM.

On your list of language aspects that require future proofing, #7 and #11 are (to my eyes) aspects of the same issue. I would add ""structured concurrency"" to #5/#6. I would add bullet items for variations on polymorphism and metaprogramming, both of which are massive. Memory management is another. Even after those improvements the list is far from complete, as I am sure you would agree.

I share your hope that, as a community, we will collaborate more towards solutions to this important problem. I would like to think the way we help each other with our projects here and on Discord, is helping set the stage for deeper and more strategic collaborations."
"I suggest every language designers to read about this, this article was suggested by @snidane, and I thought that it has really great insights on how to handle error in a programming language.",9h17di,2018-09-19 10:17:41,,"This was seriously an impressive read - I actually just finished it up half an hour ago from /u/sidane 's [original post](https://www.reddit.com/r/ProgrammingLanguages/comments/9gqgfz/is_trycatchthrow_necessary/e66hn2j/).

One thing that left me interested but was unexplored was Joe's statement:

> Notice that the invariant is marked private. An invariant’s accessibility modifier controlled which members the invariant was required to hold for. For example, a public invariant only had to hold at the entry and exit of functions with public accessibility; this allowed for the common pattern of private functions temporarily violating invariants, so long as public entrypoints preserved them. Of course, as in the above example, a class was free to declare a private invariant too, which was required to hold at all function entries and exits.

This bit confused me - especially the statement that the pattern of private functions frequently enough violating the 'invariant' bits to be called a 'common pattern.' Would anyone have any insight on what reasoning one would have for violating the invariant conditions? Before this post, I was considering emailing him for his insight.

Also, as a fun aside, in the section `Sytactic Sugar` - is `Result` behaving as a Monad? It feels like it is because it's a data structure that wraps a type, and that sort of 'piggybacks' on the type, adding its own behavior - in this case, making it so that all operations on a 'failed' result end up with another 'failed' result. However, I still haven't quite wrapped my head around them yet."
Recent trend towards more UB (?),13usqwq,2023-05-29 19:54:11,"I first noticed it in Rust that the language designers are very comfortable with just declaring ""X is UB"" (undefined behaviour) instead of providing sensible behaviour for edge-cases. Currently, I'm refreshing my knowledge of C and found that newer C standards introduced UB where previously it was up to the implementation to define behaviour.

I get that it's easier for language designers and compiler engineers to declare UB and let the world burn, but I don't like it. Especially if I can't opt-in to less UB for the cost of maybe less optimisation. Am I missing something?

Take the example of allocating zero memory. Why burn down everything if you could instead just return the null pointer? I know that in this example I can just write my own wrapper and hope that the compiler/linker will inline it, but not all UB is so easily detected.

In other words: UB can encompass a wide range of things, from returning random (meaningless) data to calling ""rm -rf /"". One is clearly better than the other. So, why are people ok with languages declaring that they might just randomly execute (dangerous) syscalls whenever the programmer misbehaves?

I hope this doesn't fall under ""excessive negativity"" or ""rants"". While I'm personally frustrated by this, I also want to understand the language design choices.","UB largely exists to enable optimization. A lot of the optimizations come from eliminating dead code after inlining functions or macros.

This article has a pretty useful explanation of how the compiler uses UB:  
http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html

In general, something like ""do a dangerous syscall"" is not a reasonable way to handle UB and we'd get angry if a compiler really did that. However, compilers do some very unexpected things that are hard to explain unless you get into the details of how optimization is done.

(I'm talking about C, I'm not really familiar with how Rust does or uses UB.)"
Type checking if expressions,10ctoel,2023-01-16 04:16:16,,"I’m surprised that print returns a value — if it didn’t,    you could make statements have type unit and then you wouldn’t need special rules for conditionals because print and assignment would have the same unit type (similar to OCaml)."
The Generics Problem,zng0wo,2022-12-16 22:32:53,,">I don't think there's any real reason \[modules\] can't just be treated like normal structs and manipulated via the same language constructs as any other value

Well they *can*, but unlike structs, modules can contain types, so you need some form of dependent types for this. If you remove types, modules are really just regular records/structs.

The solution the author came up with is honestly much more similar to the classic [Scrap Your Typeclasses](https://www.haskellforall.com/2012/05/scrap-your-type-classes.html), than ML-style modules."
When should we *not* move a check to compile time?,xnv5es,2022-09-26 02:33:07,"Hi all, a languages/architecture musing here.

Generally, it's better to move a check to compile time. When we can harness the type system to enforce something, that's usually a win.

However, I suspect there's some nuance to the stance. Take the following examples:

 * We could prevent overflow at compile time, with refinement typing, but we don't.
 * We could prevent out-of-bounds access at compile time the same way, but we often opt not to. Plenty of code just relies on a language's built-in in-bounds assertion.
 * Assertions in general are ways to check something at run-time. Plenty of times, I could lift those guarantees into the type system, but it would lead to complexity that spreads a little further than it should.
 * We could use LCell and static-rc in a lot of places, but we often don't, because it puts too much burden on the type system, so to speak.
 * Most uses of the borrow checker are great, but it can sometimes have problems when a function's guarantees cannot be propagated/upheld by its caller (such as if the caller is overriding an unchangeable trait method signature).
 * AFAIK, we could use Coq to have much stronger compile-time guarantees than a lot more things than today's languages... but we don't, presumably because it's way, way more difficult.

Compile-time checks are almost always superior than run-time checks, but I suspect that's not an absolute rule. So how do we know when a compile-time check is good or bad?

Some guesses:

 * Perhaps a compile-time check is good as long as it doesn't introduce too much *artificial complexity*, complexity that is not required by the problem itself.
 * Perhaps a compile-time check is good as long as it can be reasonably contained. In other words, its infectious spread can be reasonably contained. For example, if a library expects interface T, but we have type X, we can make a typeclass or adapter so nobody outside our library needs to know about T.
 * Perhaps a run-time check is good enough when we know it's very rare, or the cost of triggering it is low.
 * Perhaps it depends on the domain? Car steering software should probably pay any cost to move more checks to compile-time. A game, app, or a CRUD server, perhaps not.
 * Perhaps a run-time check is good enough when we can easily recover from it?

What do you think?

A related question: what are the *costs* of moving checks to compile time? (Besides one-time costs like learning curve, that is)","The original inspiration for this question was [this comment](https://news.ycombinator.com/item?id=31936621) said that Rust will catch most, if not all, bugs, and then someone [replied this](https://news.ycombinator.com/item?id=31936621):

> ... The flipside is that too much of it and code becomes so complicated, it's very hard to work with --- you're falling into a Turing tarpit[2]. It becomes easier to just write simple code without bugs, without using all that type system wizardry. But a judicious use of this pattern, where it's appropriate, may be very beneficial.

It got me wondering where the limit to static checking is, how to know when we've reached it, and in what situations."
Terry Tao on Desirable Properties of Math Notation,u812me,2022-04-21 00:57:41,,"I stumbled across this and I think it may provoke some thought for language designers (at least mathematically inclined ones)

Discussed here by programmers: https://news.ycombinator.com/item?id=23911903"
"req, an HTTP scripting language",t231hh,2022-02-27 02:01:12,,"I like it. I write scripts all the time that interact with web APIs, and it’s surprisingly cumbersome. I’d still use a ‘real’ language for doing this at work, but I could see using something like this on personal scripts.

My first question is - what does ‘decide json’ do? I don’t see any way to build structs / static types or anything like that. Does it just give you the equivalent of a JS object / map or something like that?"
Minerva - My Programming Language,pqix10,2021-09-18 16:52:34,"I've recently written an article about the programming language I'm working on: [https://sayedhajaj.com/posts/Introducing-Minerva](https://sayedhajaj.com/posts/Introducing-Minerva)

Any feedback would be appreciated","The basic structure of Minerva reminds me a bit of [Julia](https://syl1.gitbook.io/julia-language-a-concise-tutorial/language-core/functions), which is really a Lisp in imperative clothing.

IIUC you are mainly (beyond just having fun) wanting to explore possibilities of more ""readable"" control structures. I certainly always thought the ""repeat...until <cond>"" of Pascal was easier to understand than the ""do...while <cond>"" of C. But I see you've noticed that there's also a lot more to a programming language :-)

I don't know if you can get any value from an ""opposite"", but a completely opposite approach is to attempt to go beyond structural programming as [Bosque intends to do](https://fossbytes.com/microsofts-new-programming-language-bosque-keeps-your-code-simple/). I'm not sure I ""get it"" completely, but in some way I suppose my language Tailspin is going in that direction with no control structures, just [streams of values and recursible matcher-switches](https://tobega.blogspot.com/2020/05/a-little-tailspin.html)

Have fun!"
Quantleaf Language: A programming language for ambigious (natural language) programs.,nefj8y,2021-05-17 20:56:42,"In this post I will share to you, a preview of a “big” programming language project I have been working on. You can run all examples below at [quantleaf.com](https://quantleaf.com)

I have for a long time had the idea that it should be possible to create far “simpler” programming languages if we allow the programs to have uncertainties just like natural languages have. What this technically means is that for one sequence of characters there should be many possible programs that could arise with different probabilities, rather than one program with 100% probability.

The first consequence of this, is that it is possible to make a language that both could “absorb” Java and Python syntax for example. Here are a few, acceptable ways you can calculate fibonacci numbers.

(Compact)

    fib(n) = if n <= 1 n else fib(n-1) + fib(n-2) 
    print fib(8)

(Python like)

    fib(n) 
       if n <= 1 
           return n 
       fib(n-1) + fib(n-2)
    print fib(8)

(Java like)

    fib(n) 
    {
       if (n <= 1) 
       {
           return n
       }
       return fib(n-1) + fib(n-2)
    }
    print(fib(8))

(Swedish syntax + Python Like)

    fib(n) 
       om n <= 1
           returnera n
       annars
           fib(n-1) + fib(n-2)
    skriv ut fib(8)

In the last example, you can see that we use Swedish syntax. The language can today be written in both English and Swedish, but can/will in the future support many more simultaneously.

Another consequence of the idea of an ambiguous programming language is that variable and function names can contain spaces (!) and special symbols. Strings does not have to have quotations symbols if the content of the string is ""meaningless""

See this regression example.

    ""The data to fit our line to""
    x = [1,2,3,4,5,6,7]
    y = [3,5,10,5,9,14,18]
    
    ""Defining the line""
    f(x,k,m) = x*k + m
    
    ""Define the distance between the line and data points as a function of k and m""
    distance from data(k,m) = (f(x,k,m) - y)^2
    
    ""Find k and m that minimizes this distance""
    result = minimize distance from data
    
    ""Show the result from the optimization""
    print result
    
    ""Visualize data and the line""
    estimated k = result.parameters.k
    estimated m = result.parameters.m
    scatter plot(x,y, label = Observations) 
    and plot(x,f(x,estimated k,estimated m), label = The line)

Some things to consider from the example above: The langauge have a built in optimizer (which also can handle constraints), in the last two lines, you see that we combine two plots by using ""and"", the label of the line is ""The line"" but have just written it without quotations.

The last example I am going to share with you is this

    a list of todos contains do laundry, cleaning and call grandma
    print a list of todos

You can learn more about the language here [https://github.com/quantleaf/quantleaf-language-documentation](https://github.com/quantleaf/quantleaf-language-documentation). The documentation needs some work, but you will get an idea where the project is today.

As a side note, I have also used the same language technology/idea to create a natural language like query language. You can read more about it here [https://query.quantleaf.com](https://query.quantleaf.com).

Sadly, this project is not open source yet, as I have yet not figured out a way to sustain a living by working on it. This might change in the future!

&#x200B;

BR

Marcus Pousette","A possible issue here is that you have multiple small domain languages which may collide and interact in ways that result in nonsense. The more alternative ways you have to express a construct the more ways you can end up with bad program behavior, not just non-determinism."
"""If you can compile it, there's probably no bugs"" languages other than Haskell?",n96c9h,2021-05-10 22:39:24,"Hi, I hope everyone is having a great new year so far.

There is a bit of a half-joke in the Haskell community which goes something like this: given the constraints of the language, if you can compile your code, there's probably no bugs in it. (Obviously, that's not *entirely* true but that's not exactly the point.) Are there any other languages about which One could reasonably make such a claim? A colleague of mine once said something similar about Rust, I think, but I am hard pressed to think of any other languages.

Thanks in advance.","There are languages with even more expressive type systems than Haskell that are made for proving things, like Coq or Idris, but I'm not sure if thats what you are looking for."
Rust's Most Unrecognized Contributor,n3jnih,2021-05-03 08:21:06,,"I was always struck by Rust's community and approach to language design back when I was getting involved with it in 2011. But even then Dave Herman wasn't super visible publicly. As somebody interested in language design, I think it's really cool to learn from this history, noticing how very early decisions ripple down into the language we know today."
What code generators are there?,jhaand,2020-10-24 22:57:20,"While one might be quick to pick the biggest name around, LLVM, surely there are a bunch of other, less known compiler backends out there that one could use for their own language. Most are probably unmaintained, feature-less hobby projects, but surely there are at least a few that could actually be used with good results. I know of a couple.

Of course, there's [LLVM](https://llvm.org/). Tons of features, many optimization passas, and an impressive array of supported targets. A big dependency though, and can feel almost monstrously complex at times.

I've also read a little about [QBE](https://c9x.me/compile/), which impressively claims to provide ""70% of the performance of advanced compilers in 10% of the code"". I have no experience using it however, and I am not knowledgeable about its stability and maintenance status.

[Cranelift](https://github.com/bytecodealliance/wasmtime/tree/main/cranelift) is a newish code generator written in Rust. I'm not very familiar with this one either. I assume it's meant to be called from Rust code, and maybe not as convenient to embed in a C (FFI) application.

Do you know of any other code generators, or do you have anything to add about the ones I listed? I'm curious to know what more could be out there, and if I'm missing out on something that would fit my use-cases better than LLVM.","[GNU Lightning](https://www.gnu.org/software/lightning/)

It's used in GNU's smalltalk, scheme, and common lisp implementations for JIT-ing."
Pure: a language for playing with pure type systems,i4mo7c,2020-08-06 14:33:55,,"This is so cool! A few random questions for my curiosity:

1. What evaluation strategy does the interpreter use?
2. Internally, does it use names, De Bruijn indices, or something else for variable binders?
3. If I pick some sorts, axioms, and rules that lead to a weakly normalizing system, is it guaranteed to be strongly normalizing as well? Just kidding, haha."
Facebook’s TransCoder AI converts code from one programming language into another,gz6i82,2020-06-09 03:51:31,,"This is an fun idea, but I think translating syntax is the easy part. What about memory management, runtime differences, library differences, etc? I'm concerned a layman would look at this article and think that this tool would make converting between languages simple. To me, this syntax translator only solves about 5% of the problem."
"Books/Resources to learn more about category theory, programming theory, and the foundations of computing in general.",ggooe4,2020-05-10 06:00:08,"I apologise if this isn't the right subreddit to ask for this but it seems more appropriate than learnprogramming.

I'm trying to find more resources relating to more of the theoretical side of computer science particularly to my degrees focus on software engineering rather than computer science...which is the name of the degree. Nonetheless my interest in the foundation aspects of programming for example automatons and Turing machines and in the design of programs given by Dijkstra's has piqued my interest in what this area has to offer. (Mainly due to wanting to do my final year thesis in this area).

Throughout my recent studies in making my own language in Haskell, I have come across category theory but not in any great detail. It seems like a fascinating that I would love to sink my teeth into, given that there's a textbook or something of that nature on it.

With regards to Type Theory, I haven't found much on it bar a paper from CS Kent, ""Type Theory and Functional Programming"". I really enjoy FP but is there any resource that teaches you Type Theory?

Now I know this subreddit is dedicated to the design of programming languages more than anything. I would say that one of my interests in general is language, natural and programming so I think language design and theory suits this. From reading through past posts I have found a few books that may be useful to my goals in learning more and perhaps to anyone who is in the same situation as myself! The books are:

* Structure and Interpretation of Computer Programs,
* Crafting Interpreters,
* Build Your Own Lisp
* Dragon Book
* The Implementation of Functional Programming Languages
* Implementing Functional Languages: A Tutorial

Now I know there definitely a lot more but would you recommend these books and what other books would you recommend?

tldr; What books would you recommend for someone wanting to learn more about category theory, type theory, and programming theory?","For category theory: [Steve Awodey, ""Category Theory""](https://www.amazon.com/Category-Theory-Oxford-Logic-Guides/dp/0199237182).

For type theory: [Benjamin Pierce, ""Types and Programming Languages""](https://www.cis.upenn.edu/~bcpierce/tapl/)."
Classes and Instances · Crafting Interpreters,ei7oqw,2020-01-01 03:37:13,,"A New Year treat for us! (Including me who still can't figure out how assignment works; expressions are a piece of cake, but can't wrap my head around assignments yet)"
Oxidizing OCaml: Locality,13tpuel,2023-05-28 11:07:06,,"TLDR: They built a very limited borrow checker in Ocaml which has only three lifetime values (local, caller, and global) and only shared references (i.e. no non-copyable types and hence no uniqueness checking). The idea is that this still allows some compiler optimizations without the complexity of a full lifetime system."
Static and dynamic challenges of size types,13fm2q5,2023-05-12 22:01:18,,"I'm curious, for the commutativity problem you mention in the *unification* section, had you looked at Abelian unification? I would think this would get you close to where you want, although your pair example still has an ambiguous computable interpretation. But it should get you type inference for `concat`, `replicate`, `take`, `drop`, and type-safe indexing like `at [n][m] 't : [n+m]t [n]i64 -> t`.

In some ways I would find it surprising if Futhark allowed the pair example, and I can't figure how it would generalize nicely. How would `a+b+c` unify with `n+m` in a way that is useful? It seems possible to accomplish though, if there was a single fixed method for pseudo-syntactically unifying, just a bit complicated to reason about for the programmer in order to avoid ambiguous results."
Why don't more languages implement LISP-style interactive REPLs?,10u74ts,2023-02-05 15:48:23,"To be clear, I'm taking about the kind of ""interactive"" REPLs where you can edit code while it's running. As far as I'm aware, this is only found in Lisp based languages (and maybe Smalltalk in the past). 

Why is this feature not common outside Lisp languages? Is it because of a technical limitation? Lisp specific limitation? Or are people simply not interested in such a feature?

Admittedly, I personally never cared for it that much to switch to e.g. Common Lisp which supports this feature (I prefer Scheme). I have codded in common lisp, and for the things I do, it's just not really that useful. However, it does seem like a neat feature on paper.

EDIT: Some resources that might explain lisp's interactive repl: 

https://news.ycombinator.com/item?id=28475647

https://mikelevins.github.io/posts/2020-12-18-repl-driven/","I think it's because of the repercussions that such a REPL has on the design of the programming language.

Common Lisp's REPL (with something like SLIME) works amazingly well because:

1. CL has built-in, language-integrated debugging facilities without the need for external tools
2. CL has interactive error handling built-in (errors don't crowbar your process; lots of choices can be made about how to recover from errors)
3. CL allows functions, classes, and methods to be redefined at runtime with coherent semantics—as an ANSI standardized behavior
4. CL has automatic memory management
5. CL has built-in introspection facilities: types, class fields, etc. can all be inspected at runtime
6. CL is dynamically typed and won't signal errors when types don't match at compile time
7. CL allows for definitions to be deleted
8. though not required, CL most commonly is understood as a ""living image"": a program is the living state of code and data in memory, not a read-only executable section of a binary

Many of these go completely against the design goals of other languages. For instance, redefining a function in Haskell can wreak lots of havoc when modules are separately compiled, so a Haskell REPL in almost any serious implementation will not hold a candle to a Lisp REPL. 

Common Lisp's REPL really shines when you have large programs that you're on the hook for modifying, especially in small ways. It's extremely useful in situations where you build large amounts of state (think compiler ASTs) that you need to drill into when you encounter some kind of bug. The fact the language allows on-the-fly re-definition of buggy code through the conduit of the REPL allows incredibly rapid understanding and consequent solving of problems."
"A Conversation with the Creators Behind Python, Java, TypeScript, and Perl",zz30pm,2022-12-30 23:20:18,,Think I found the video recording https://www.youtube.com/watch?v=csL8DLXGNlU
Anyone aware of interesting studies into the ergonomics of programming language features?,w7xtfg,2022-07-26 03:26:47,"For the most part, I think it's fair to say that a lot of programming languages are designed from empirical experience and tacit knowledge of the community.   


I'm really interested in studies like Justin Lubin and SarahChasins work (['How Statically-Typed Functional Programmers Write Code'](https://schasins.com/assets/papers/howStaticallyTypedFunctionalProgrammersWriteCode.pdf) for example) and wonder if anyone is aware of similar work that studies the interactions and features of a language/s (or paradigm)?  


Also, thoughts on this kind of methodology and theories around how we design languages more than welcome!","Andreas Stefik has put together a nice page on his Quorum language website on this topic:   
[https://quorumlanguage.com/evidence.html](https://quorumlanguage.com/evidence.html) 

One of my personal favorites in this genre is his own ""An Empirical Investigation into Programming Language Syntax""

[https://dl.acm.org/doi/abs/10.1145/2534973](https://dl.acm.org/doi/abs/10.1145/2534973)"
Efficient Compilation of Algebraic Effect Handlers - Ningning Xie,vq86w1,2022-07-03 11:21:23,,I found the discussion in 25 min - 42 min particularly clear in explaining the implementation difference between OCaml (segmented stacks for efficient one-shot resumptions) vs Effekt (capability-passing style and how lexically scoped handlers work) vs Koka (evidence-passing).
The 3 languages question,vltbsi,2022-06-27 19:28:46,"I was recently asked the following question and thought it was quite interesting. 

1. A future-proof language.
2. A “get-shit-done” language.
3. An enjoyable language.


For me the answer is something like:

1. Julia
2. Python
3. Haskell/Rust


How about y’all? 

P.S Yes, it is indeed a subjective question - but that doesn’t make it less interesting.","I agree, this is an interesting question to answer.

Future proof 1:Rust  (Yes I'm one of those people) [https://www.youtube.com/watch?v=IA4q0lzmyfM](https://www.youtube.com/watch?v=IA4q0lzmyfM)If you want future proofing, the obsession with correctness that Rust gives you is what you want. Java is a weakly typed language compared to Rust, Rust's type system does not allow for nulls.Haskell is similar, but Haskell is doing other things to run your code and one day you're going to have to deal with that...

Get shit done 2:Javascript. Javascript on the browser, Javascript on the server, Javascript running database queries...

I hate Javascript, but I've done a thousand times more work in it than anything else. Javascript is the blitzscaling language to write your startup in, dump your crappy code on the next generation and exit with 10 billion dollars before the technical debt rot sets in and the new owners realize they've been scammed.

Not Rust, because writing code without a garbage collector takes longer. Rust ironically taught me that garbage collection is a very acceptable compromise for actually delivering value.

Leisure 3:Rust again. If I want to still work on a project in a few years...or actually if I still want to work on a project next week, I'll write it in Rust."
How has learning about PL design helped you in industry?,v0kjbr,2022-05-30 05:12:11,"I'm just looking for anecdata.

Have you ever had the chance to do PL work in your day job? Be that DSL design or custom PLs? Do you feel thinking about programming languages helps you think about other kinds of software design?

Have you ever had any interesting reactions when mentioning your PL work in interviews?

In short, what kinds of things can someone gain career-wise by thinking about programming languages?","We have a bunch of linters at work that operate on the AST of the code. It’s much more powerful than text search. That’s probably the most practical thing I’ve personally done.

Of course there’s the intangible benefit of simply knowing how your tools work at an even deeper level. Where someone else thinks something is magic, you can actually understand how it works."
"HVM: a massively parallel, optimal functional runtime in Rust",shh8i3,2022-02-01 08:39:55,,"the HOW.md is excellent, and gives me a much better idea of the rewriting process than the last time I looked into this.

I guess I'm going to have to give Kind another look, too."
On the merits of low hanging fruit.,nqm6rf,2021-06-02 21:47:43,"I've been reading this subreddit for several years now, and so often it bums me out. It seems like almost everyone is obsessed with reaching as high up the fruit tree as they can go, and everyone is trying to grab the exact same fruit. If that were the only fruit left, then I would understand that, but it's really, really not. There's still loads of low hanging fruit just begging to be picked. It would be nice to see a little more variety here, y'know? So here's my contribution:

The past couple of days I've been experimenting with a new global allocator for my personal library for our language. I call it the vspace allocator because its job is to track cells of virtual memory. Right now I'm using 4GB wide cells, and I have roughly 32k of them. The idea here is that when you make an allocation you reserve a cell and mmap some portion of it. Because the nearest mmap will be at least 4GB away you have a strong guarantee that you will be able to mremap up to 4GB of memory in a cell without having to ever unmap and move your data. This means you can build growable data structures without worrying about invalidating ptrs, copying data, or messing around with buckets.

My motivation for building this isn't because I'm expecting people to do all of their allocations through the vspace allocator. No, the use I have in mind is for this to be an allocator allocator. Perhaps ""meta-allocator"" is a good term for it? Anyway, let me tell you about my ring allocator, which has a fairly unique design:

So in case you're not familiar with them, ring allocators are used as temporary or scratch allocators. They're backed by ring buffers, so once you reach the end of the buffer you will wrap around and start allocating from the beginning of the buffer. Anything that was already there will become clobbered. In theory this is fine because you're not supposed to put any long-lived data into a scratch allocator. In practice this makes calling functions a scary prospect because there may be an arbitrary number of scratch allocations made. My solution to this is to put a pin into the ring allocator with the idea being that any allocation that crosses the pin's index will cause a panic. This way you will be notified that you ran out of scratch memory instead of the program continuing in invalid state. To avoid conflicts over pins multiple ring allocators can be used.

The way pinning works is there can only ever be a single pin, which is fine because a second pin would necessarily be protected by the first pin. When you attempt to make a pin you will receive a boolean that you will use as a key to try to remove the pin later. If the boolean is true, then you are the owner of the pin, and may remove it. If it is false you are not the owner of the pin, and it will not be removed. In this way every function that's interested in pinning some memory that it's using can be a good citizen and attempt to use its key to unpin its memory when it's finished doing its work. A language with macros and defer can create convenient macros that ensure the user will never forget to unpin the ring alllcator.

Now let's combine my ring allocator with my vspace allocator: Instead of panicking when an allocation would cross the pin, the ring allocator can move the pin to the 0 index, grow larger, and then move the allocating head past the old pinned memory and into the newly allocated memory. If excess memory usage is a concern, then an absolute max size can be set, and successfully upinning the ring allocator can shrink it to its original size.

In this way a ring allocator can be made safe and reliable to use. This is notable low hanging fruit because it automates memory management in many of the same ways that a GC does, but with barely any overhead. Of course I'm not suggesting that my ring allocator is sufficient by itself to handle everything about memory management, but it might be an attractive alternative to garbage collection for some tasks.

There are lots of simple ways to attack useful subsets of hard problems, and sometimes that simplicity is so valuable that it's worth settling for an 80% solution instead of holding fast for a 100% solution. I believe being aware of designs like this should inform the way we design our languages.",I'm curious what you mean by low hanging (compared to high up) fruit. Like high level programming problems? Or like attempting more than what is realistic
Will the traditional while-loop disappear?,lr7nlw,2021-02-24 16:28:10,"I just searched through our application’s codebase to find out how often we use loops. I found 267 uses of the for-loop, or a variety thereof, and 1 use of the while loop. And after looking at the code containing that while-loop, I found a better way to do it with a map + filter, so even that last while-loop is now gone from our code. This led me to wonder: is the traditional while-loop disappearing?

There are several reasons why I think while loops are being used less and less. Often, there are better and quicker options, such as a for(-in)-loop, or functions such as map, filter, zip, etc., more of which are added to programming languages all the time. Functions like map and filter also provide an extra ‘cushion’ for the developer: you no longer have to worry about index out of range when traversing a list or accidentally triggering an infinite loop. And functional programming languages like Haskell don’t have loops in the first place. Languages like Python and JavaScript are including more and more functional aspects into their syntax, so what do you think: will the while-loop disappear?",The `while running { if stop_condition { running = false } }` is still a very widely used pattern
Odin Programming Language: An Introduction - 2020-11-26,k9qb3l,2020-12-09 19:33:22,,"On odin-lang.org you have the following. The second line is self-referential.

    uintptr     an unsigned integer large enough...
    uint        same size as uint
    int         same size as uint"
"First alpha release of my programming language, Jasper",jpym7a,2020-11-08 05:21:42,,"Hello, dev here.

Thanks for crossposting, I hadn't thought of posting here"
Advantages of currying in a programming language?,jde9xp,2020-10-18 18:47:47,"With so many functional languages using currying, I really don't see the practical advantage. I would argue that SML's ""tupling"" syntax augmented with partial application is better.

So the main thing currying is good for seems to be partial application. But currying is terrible for that because it imposes an arbitrary order of parameters. I've seen a discussion and a blog post about ""data-first"" vs ""data-last"" parameter orders. This is a problem created entirely by currying, and can be solved by easy syntax like

    foldMyList = foldl _ _ myList

which desugars into

    foldMyList = \f z - > foldl f z myList

So currying doesn't help partial application. But it also hurts function composition. Consider SML's scheme (sadly, rejected by the OCamlers) where every function has, technically, exactly one (possibly tupled) parameter and one return value. Then functions with more than one (real, non-tupled) parameter may be freely composed:

    f :: Int - > (Double, String)

    g :: (Double, String) - > Foo

    composed = g . f

In the curried form, these functions are non-composable:

    f :: Int -> (Double, String) 

    g :: Double -> String -> Foo

    composed = g . f  - - Type error! 

So currying hurts composition because of its inherent asymmetry: functions can *return* multiple values but cannot *take* multiple parameters. And it hurts partial application too. And its ""every function has only one parameter"" elegance is inferior to SML's scheme. So what are, if any, practical advantages of currying, and why do programming language authors like it so much?","SML supports multiple parameters in the Haskell style just fine:

    fun plus x y = x + y;

The real reason for prevalence of tuple arguments in SML's standard library is actually as a concession to efficiency, as explained to me by multiple SML compiler maintainers I asked about it.  In SML, operators are evaluated before operands.  This means that in an application `f x (g y)`, which with explicit parentheses is `(f x) (g y)`, the subexpression `(f x)` must be evaluated before `(g y)`, which typically would require heap-allocation of a closure value, while we then evaluate `(g y)`.  If we instead define `f` to take a tuple as an argument, then a naive compilation of `f (x, g y)` works just fine, without any extraneous heap allocation of a closure.  OCaml worked around this by instead doing right-to-left evaluation of function operands, and this is why you tend to see curried functions in the OCaml standard library.  (Incidentally, Moscow ML, which is based on the Caml Light runtime, inherits this evaluation order, and so doesn't strictly implement SML correctly.)

An optimising compiler can of course work around this entirely, but I think SML was designed such that a reasonably simple implementation would still generate decent code."
PL Notation is a Barrier to Entry,j20ku9,2020-09-29 22:45:21,,"Yes, please! I'm so happy to hear that it's not just me. I'm a student who is just getting into the field, so the notation I encounter in research is often quite intimidating to me, especially since it's often different to what I learn in classes (and sometimes, it even differs from one class to the next on the same subject)."
Constraint Solvers for the Working PL Researcher,iqcioh,2020-09-11 05:16:22,,Are the slides available somewhere? (I generally don't watch videos.)
Smalltalk Based Programming Language Syntax,i8ohxu,2020-08-13 07:10:42,"Hi everyone!

I've finished my bachelor's degree this year with a programming language thesis. It's pretty simple and the idea was to find a syntax that could look as close to English as possible while preserving the ""technical"" stuff (so it doesn't look like HyperTalk).

The interpreter for the language is written in Python for the sake of simplicity and prototyping speed, and the source code is hosted on [GitHub](https://github.com/tale-lang/tale).

Here is a short version of the syntax:

    -- Comment.
    
    -- Simple binding.
    x = 1
    
    
    -- Unary function binding.
    (x) squared = x * x
    
    -- Unary function call.
    x = 2 squared
    
    -- You can also play with unary functions to create fluent sentences.
    (x) as = ...
    (x) JSON = ...
    
    x = 10 as JSON
    
    
    -- Binary function binding.
    (x) * (y) = ...
    
    -- Binary function call.
    x = 2 * 2
    
    
    -- Prefix function binding.
    !(y) = ...
    
    -- Prefix function call.
    x = !2
    
    
    -- Keyword function binding.
    just: (x) = ...
    
    -- Keyword function call.
    x = just: 1
    
    
    -- Complex keyword function binding.
    add: (item) to: (list) at: (index) = ...
    
    -- Complex keyword function call.
    add: 1 to: numbers at: 0
    
    
    -- Pattern matching on types.
    (x: Int) / (y: Int) = ...
    
    -- Pattern matching on value.
    (x: Int) / 0 = Error
    
    
    -- Data.
    data Point =
        x: Int
        y: Int
    
    -- Types.
    type Eq =
        (a) == (b): Bool
    
    -- Implementations.
    Point as Eq =
        (a) == (b) =
            (a x) == (b x) and:
            (a y) == (b y)        

I know that the semantic part of the language is very important, however, the initial idea here is to first play with the syntax and make it English-like, and then find a way to make the language as safe and performant as possible. Right now I'm studying Rust for low-level game dev stuff, but it's also a nice language from the design perspective.

Just wanted to share this little piece. If you have any comments or feedback, would be glad to answer. Thanks for your attention!","As a hobbyist compiler/interpreter writer I love this. Love this. This could be fun to mesh with a Forth like dictionary to create an interactive language of sorts. Then throw in some logo/turtle stuff and create mind melting images. Honestly I dig this syntax as it lets you make it as english-like as desired or just keep it straight forward. Could be great as a learning language that starts off more english like then becomes more syntax driven over time as they learn.

Question 1: What does the #include/framework/library syntax look like?

As for Rust I am becoming a cargo cult nut, I love the syntax and design based around a lack of a garbage collector. About time! However, it seems early for a games language but if you want to help push it along I know it needs some support.

Question 2: Have you looked into the Gai language by Jonathan Blow? I haven't checked into in for a while but he is developing it purely as a low-level simple syntax game programming language.

Question 3: How would you feel about someone using this syntax as a base to implement extensions? Giving full credit for the source syntax, of course.

Thanks."
/r/programminglanguages hit 30k subscribers yesterday,e4ulqc,2019-12-02 13:53:00,,"Pretty cool.  It doesn't seem that long ago that we marked reaching 10k.  And looking at the stats, indeed it wasn't.  Extrapolating unreasonably, the subreddit seems to be doubling in size about every year now."
Unix50 - Unix Today and Tomorrow: The Languages,dy3gj4,2019-11-18 21:53:51,,"Brain Kernighan's new book about Unix history and his experience at Bell Labs is a great read. It's really interesting to hear what it's like inside the offices that created so many of the tools we use everyday, especially hearing about other legends like Doug  McIlroy, Ken Thompson, and Dennis Ritchie."
How Swift Achieved Dynamic Linking Where Rust Couldn't,dtqv11,2019-11-09 11:51:59,,"How is the performance of such a ""smart"" ABI compared to the C++/Rust way?"
Immutability is better but why?,134b775,2023-05-01 11:16:20,"My understanding is the following:   


1. In multithread programs immutable objects don't have to synchronize.
2. Immutable code is easy to reason about; you have some input and you get a result, there's nothing aside to think about.
3. Immutable code is safer, some other ""parts"" of the system won't modify your data inadvertently. 

&#x200B;

Those are the three main things I can think about. 

&#x200B;

Questions about each point: 

&#x200B;

1. If my program is single threaded then mutability is not a concern right? Because there will be always only one writer.  
2. Controlling side effects and simpler code is very important specially when code grows. But if the code is small and/or the style followed is free of side effects, is immutability still important?
3. For #3 I can only think about plugins where a 3rd party can access your data and modify it behind your back, but in a system that is under your control, why would you modify your own data inadvertently? Maybe because the code base is too large? 

&#x200B;

I use immutable data in my day to day work but now that I'm designing my PL I'm don't want to blindly make everything immutable nor make everything mutable just because.  


I thinking my PL will be for small single thread (albeit concurrent) programs with very little 3rd libraries / interaction.   


Is there something else I'm missing.   


I think FP is slightly different in this regard because since is modeled after mathematics and there is no mutability in mathematics there's no need  to justify it ( and yet, needed in some cases like Monads) .","> If my program is single threaded then mutability is not a concern right? Because there will be always only one writer.

Does your language have pointers? A classic example is two pointers in different parts of a program to the same dynamic array. Then more items are added beyond its capacity, so the array data is reallocated. In one place you'd know about the new pointer, but in the other place you have a dangling pointer now. In a single-threaded environment, the danger isn't in mutability, it's in _shared mutability_.

Some further reading and additional points in: [The Problem With Single-threaded Shared Mutability](https://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/)"
A Block-Based Functional Programming Language,131s0kg,2023-04-28 21:37:40,"Hi All, 

Im a Univeristy Student currently studying Computer Science at the University of Southampton, and as apart of my Dissertation / Thesis i have created a proof-of-concept tool that combines ideas from the Functional Language Haskell with the Block-Based visual paradigm.

Ive spent alot of time and effort on this project, and whilst i havent managed to achieve all my goals, im still proud of its outcome and am here to share it!

You can find the project at [https://blockell.netlify.app/](https://blockell.netlify.app/), where you can create small programs using blocks to declare functions and generate haskell code. Unfortionately, there are no tutorials for the tool, and to run the code you have to copy it into a haskell file and run it locally on your machine. :(  


Anyways, if you like the tool, or have any comments, suggestions or improvements feel free to comment them below or answer this anonymous feedback form here: [https://forms.office.com/e/nd6sJ3Gq0U](https://forms.office.com/e/nd6sJ3Gq0U)  


Any and all commments with be tremendous help to my project, but also Im just proud of this language and want others to see it :)","1. It should be prefilled with some code. Especially code I can make trivial tweaks to to see that it's working before I dig into the menus.

2. Replit supports Haskell. If possible you should created a button that launches replit or something similar with your code in it. Or ideally embed an existing solution."
Future of high-level languages,12x46f5,2023-04-24 13:12:47,"Lately we’ve seen a lot of progress in low-level programming with Rust being the canonical example. The other trend has been the growing adoption of static typing, for example with the success of TypeScript. We have been going down the route of writing down more and more information for the compiler or interpreter to work with. 

What are some promising directions that are looking into writing less but expressing more? Of course this comes at a cost of safety or performance, but these are not the top constraints always.","I think the future is going to be ever more elaborate static type systems.  It's an obvious place to improve.  It's much less clear how to systematically improve dynamically typed languages - they're already plenty expressive and powerful.

I don't think the future will be more *powerful* type systems - academia is full of very powerful, expressive, and unapproachable type systems.  I expect broader adoption will involve making e.g. effect types ergonomic and accessible to programmers who don't want to learn the intricacies of type systems.  Epic's Verse language is an example of this idea (although it is not yet proven that its quite restricted effect types are going to be widely usable).

Gradual typing has been an area of major work, but mostly because it allowed type systems to be retrofitted to existing popular languages such as Python and JavaScript.  It is not clear to me that gradual typing is very meritorious beyond such retrofitting work, and I don't think freshly designed languages will be based on gradual typing. (Counterpoint: gradual typing can be seen as anything that involves you to not fully utilize the available type system; e.g. I saw a presentation on how to use ""gradual types"" to bridge Haskell and a full dependent type system.)

One thing that powerful type systems makes available is the idea of type-based code generation.  Sometimes the types are so precise that there is really only one possible term of the expected type - and then the compiler can infer it for you.  Dependently typed languages such as Agda or Idris exploit this a lot.  It is not clear to me whether this requires too much of the average programmer however, as it only works if your types are completely precise."
Pictures of a Working Garbage Collector,109tcqy,2023-01-12 15:16:48,,"Awesome and interesting post!

Also I read ""pictures of working garbage...collector"" and I thought it was funny"
The Golang Design Errors,100724x,2023-01-01 07:57:25,,"TLDR:

Gopher thinks that Go is mostly great, but has three major flaws:

1) lack of operator overloading, or even a generic sorting interface, makes basic sorting tasks gratuitously painful

2) having to write if err != nil all the time is horrible

3) threadbare and difficult to use standard library (e.g. writing a priority queue using the heap module requires 100 lines of example code)."
Sigils are an underappreciated programming technology,zqc39n,2022-12-20 10:48:15,,"I have mixed feelings on sigils. I'm not as familiar with Raku, but perl is a popular example of them where `$foo`  is a scalar, `@foo` is an array, and `%foo` is a hash.

However, referential versions of these arrays and hashs are very popular (and analogous to the only type of collection in languages like python). So then you end up with $foo which could be a string, number, array reference, hash reference, or an object. Really all `$` tells you is that it's a variable, which is exactly what semantic syntax highlighting already tells you in most languages (i.e. variables are a different color). Which means they often offer no additional meaning beyond the highlighting color already present. 

Perl also has the goofy side effect of needing to do @$foo or %$foo to access them. Which means that sigils are less of an identifier of the variable, but rather a technique for accessing them. You need to treat something like a hash if you want to use it like one. Personally I'd rather just leave those symbols off. If I'm trying to calls `keys` on a variable, clearly I'm treating it like a hash already. Therefore `keys %$foo` (where foo is already correctly highlighted) contains two symbols that provide no extra meaning to the expression."
The tooling is the language?,vvhk20,2022-07-10 10:37:31,"C with it's incredibly flexible GCC and powerful GDB (not to mention Valgrind et al). Lisp with its REPL. Java with its IDEs and build systems. Go with its rapid compiles. Rust with its error feedback.

I used to think Rich Hickey was right when he said targeting the JVM was the best decision for Clojure because of the Java libraries, but now I believe it was leveraging the tooling (of both Java and Lisp).

Not sure what my point is, maybe if you're designing a new language, consider what your tooling is going to bring to the party? The only way your language can be used is through its tooling after all. It's where the rubber meets the road.","Language and tooling complement each other. The language should be designed with tooling in mind, and the tooling should take advantage of the language.

Best example of a language \*not\* designed with tooling in mind is SQL. At the time predictive input (""intellisense"" and the likes) was not considered. Today, writing SQL seems antiquated because you don't get the assistance from predictive input, simply because you need to write the ""select"" list before you indicate the sources. 

How the language is designed affects (front end) intellisense, debugging, syntax highlighting, error messages, refactorings etc. On the back end the language's module system and compilation/interpretation model may affect linking, optimization, etc.

Consider how editors for OO languages can predict possible methods from the type of the object when you hit `.` to delimit at method/property. If the language has been designed so that the type is not known at that time, it will inhibit what the tooling can do."
Programmable type systems?,vjmxuy,2022-06-24 20:07:42,"This is hard to describe, but I'm trying to find examples of type systems where the programmer can in some way write logic based, programmable types.

As one basic example, TypeScript allows the user to create complex types by manipulating them using a system of conditional types and things like literal types. Here are some examples of what can be achieved with it:

https://twitter.com/diegohaz/status/1309489079378219009?t=a5-vZ2C3R7uEXimq9bL1lA&s=19

https://twitter.com/buildsghost/status/1301976526603206657?t=yVnJMbp_CWkOVGfhLauzTQ&s=19

What I'm looking for are full-fledged sub-languages used to manipulate the types like that. This probably mostly applies to languages with structural type systems. Any ideas?","In dependently typed languages, types are first class expressions, just like any other language construct. You can put them in data structures and create them using functions. A ""conditional type"" is literally just a regular conditional expression that returns a type. Check out Agda, Idris, Lean, and Coq."
"I finally finished my never-ending side project! It is a language written in JSON that doesn't use any reserved words or predefined structure. The end result resembles standard code as close as I was able to replicate. Next, I plan to use this language to drive a visual UI for learning/debugging.",uo1pqp,2022-05-12 21:58:11,,"I don't have strong opinion about the language, it seems to be too  esoteric but I think the site is amazing! I really like the interactive tutorial style."
Abstraction: Not What You Think It Is,tqtxfx,2022-03-29 14:59:47,,"I think there's a difference between worrying about people using a word to mean something other than its well-defined meaning (which is confusing) and trying to force a poorly-defined word to fit some imposed well-defined meaning (which is unhelpful). To me this article feels like it thinks it's doing the former when it's actually doing the latter.

Abstraction in a software engineering context doesn't have a well-defined meaning which is exactly why, as the article says, it's so hotly debated. Things which are well-defined tend not to require such arguments, since the arguments can be settled by just reading the definition. The solution [isn't to show everybody some formalization for what abstraction means and tell them to use that](https://xkcd.com/927/), but to just accept that ""abstraction"" has a vague, inconsistent meaning that everyone still basically understands the gist of. If you want a stricter meaning, use a different term."
"Parsing Layout, or: Haskell's Syntax is a Mess",pho0ab,2021-09-04 16:51:49,,"For language designers, has anyone written a survey of the different ways indentational syntax can be done? E.g. is there a Haskell-like design which wouldn't prompt ""X's Syntax is a Mess"" with a long detailed post about how to implement it?"
Survey: dumbest programming language feature ever?,pc5m39,2021-08-27 02:37:08,"Let's form a draft list for the *Dumbest Programming Language Feature Ever.* Maybe we can vote on the candidates after we collect a thorough list.

For example, overloading ""+"" to be both string concatenation and math addition in JavaScript. It's error-prone and confusing. Good dynamic languages have a different operator for each. Arguably it's bad in compiled languages also due to ambiguity for readers, but is less error-prone there.

Please include how your issue *should* have been done in your complaint.","The [COMEFROM](https://en.m.wikipedia.org/wiki/COMEFROM) instruction.

Born as a GOTO parody in response to Dijkstra’s letter against spaghetti code, it works basically as a time-reversed jump between statements.

Just. imagine. the. possibilities."
A better name for Monad?,ojgr01,2021-07-13 22:29:21,"Monad is an amazing concept with a name only mathematicians understand. How would you call it to make it (Edit: the name not the concept) more intuitive for developers? Do you think this could make the concept easier to understand?

While we are at it:

What are better names for the operations `pure/return` and `bind`?

Edit: This is not a proposal, just an exercise in language design. It is not about the question if we should change the name. It is about *how* you would change the name if you would do it.","`return` is a terrible name because of its usual meaning in programming languages. `pure` is a better name, and I think `wrap` would be a better name still.  
`bind` makes sense to be called `then` for monads that can easily be thought of as a computation, e.g. option types, result types, promises, parsers, etc."
"Any usability studies on ""name determines visibility""?",nn3c35,2021-05-29 01:45:48,"Go has its ""capitalized vs lower case determines public/private"", Dart uses an initial `_` to control visibility.

So far the argument *for* that I've seen is that it's short and it's obvious at the calling site. The argument *against* is that changing visibility involves updating all calling sites.

I know that the decision to use _ in Dart has been challenged, I don't know so much about Go. Has there been any actual usability studies of how it turned out in practice?","I have a somewhat relevant anecdote (which obviously doesn't rise to the level of a usability study, but I wouldn't be surprised if this hasn't been tested yet)

----

I was a teaching-assistant for a distributed systems class that had programming projects in Go. All other classes used C or C++, so the students were mostly new to Go. 

One of the _most common_ issues (probably at least 1 in 4 questions, if not more) that students with their code was fields inexplicably remaining their zero-values, despite being assigned a value. The cause was always that they forgot to capitalize a field; the problem is that in Go, a struct can _look_ like it's only used within one package (so private access seems sufficient and all your literal `.field` references compile fine), but if you interact with the struct using reflection (in our case, it was for sending request/response messages across the network), the reflection doesn't have access to private members.

I don't know how this would generalize to professional Go programmers; I'm sure they're more aware of it and it would bite them _less_ often, and the fix is usually not so hard. 

The fundamental problem is that Go uses reflection _pervasively_, and reflecting on a type with private members is generally wrong, yet there's no way for reflection to warn you about it (because there's no efficient way to _detect_ private members).

While I think this is made _worse_ by the intuition that names shouldn't matter to reflection (`Dog` is the same as `Cat` -- so why would `capitol` be different from `Capitol`?), I also doubt this would be greatly improved by using a sigil or keyword for visibility, since the problem would only shift from ""reflection cares about names"" to ""reflection cares about visibility"", which is still easy to slip up on.

On the other hand, the pervasive use of reflection also makes automatic refactoring to change field names or field visibility not risk-free; this also has lead to friction such as the need to use [field tags](https://golang.org/pkg/encoding/json/#:~:text=field%20tags) (which are by no means a bad feature) because some capitalizations are illegal.

-----

Another quirk is the use of Unicode identifiers in Go.

Some languages (e.g., CJK) don't have uppercase/lowercase variants, yet are valid identifier names. This makes the ""uppercase for public, lowercase for private"" not make sense if you also want to enable ergonomic use of non-English identifiers. This has even lead to proposals like [this one](https://github.com/golang/go/issues/5763) to _change_ the rule for which words are exported in a future version of Go! An alternative proposed by rsc is to _also add a sigil_ for distinguishing public/private.

Using Unicode features in general in a language can put it in a weird place, since whether or not a scalar-value is a ""letter"" depends on the version of Unicode; though at least Unicode promises to never add/take away a character class like ""uppercase"" from an already assigned character.

----

Also, while I'm here, personal subjective opinions: using capitalization on public/private is a waste. When I'm using a library, I _literally cannot see your private members_, so you lose the ability communicate anything with capitalization (unless you're demanding users of your code inspect its source because you don't provide good docs...). Other languages use capitalization to better effect, in my opinion:

* Java, by (unenforced) convention, uses capitalization to distinguish types from packages/variables. This is especially useful for quickly scanning against Java's (inadvisable) `<type> <name> =` syntax for defining variables, and for distinguish `Type.staticMethod()` vs `variable.method()`.
* C#, by (unenforced) convention, uses capitalization to distinguish members from local variables, so `Field = 1;` looks different from `variable = 1;`.
* Haskell enforces a `ConcreteType`/`typeVariable` convention, which allows for very terse parametric signatures. (It also allows you to escape the Java convention of _all type variables are single letters_ to distinguish from defined/imported names)"
I designed a small stack-based scripting language,nixpeb,2021-05-23 10:07:52,"Edit: [Implementation](https://github.com/mwerezak/stackscript) of an interpreter. The interpreter is written in Python (lol). It's fairly incomplete but you can run it on some script text and see the contents of the stack after execution.

Edit: You can now find an operator reference at the end of the README in the GitHub repo linked above.

Edit: Rewrote a significant part of the post to keep it up to date (the design is under active development!) and improve clarity.

---------

While I've created a few small DSLs in the past, usually for work-related things, this is the first time I've created a general purpose language just for the sake of it.

I'm not sure what to flair this. Criticism is welcome but I'm not sure if `Requesting Criticism` is the best fit. I guess this reads a lot like a blog post so I'm applying that.

The inspiration for this language comes from another small esolang called [GolfScript](http://www.golfscript.com).

Being designed for code golfing, it makes some trade-offs that I don't particularly want for my own language. However it really got me thinking.

I wanted to see how far I could get with trying to make an expressive and easy to use stack-based scripting language.

Also, this being my first step into the world of programming language design, I wanted something easy to start with and stack based languages are really easy to parse.

Other inspirations for this language come from Python, Lua, and a bit of Scheme/LISP.

The implementation is still incomplete, I've only started working on it this past week. But I've made a lot of progress so far and I really like the direction it's going.

Anyways, more about the language itself (still yet to be named):

Naturally since it is stack-based all expressions are RPN.

    >>> 3 2 * 4 +
    10

You can assign values to identifiers using the assignment operator `:`.

    [1 2 3 'a' 'b' 'c']: mylist

Right now the available data types are booleans, integers, floats, strings, arrays, tuples (immutable arrays), and blocks. The block type is really important so I will get back to that later.

I also want to add a Lua-inspired ""table"" mapping type that also supports Lua-style metatables. I think this will add a lot of *oomph* to the language without needing to add the weight of a fully-fledged object system.

Like Lua I plan to have a small closed set of types. I think you can do a lot with just lists, tables, and primitives.

Now, back to the ""block"" type. Blocks are containers that contain unexecuted code. These are very similar to ""quotations"" in Factor. Blocks are just values, so you can put them in lists, pass them when invoking another block, etc.

Blocks can be applied using either the invoke operator `%`, or the evaluate operator `!`. The evaluate `!` operator is the simplest, it just applies the contents of the block to the stack.

    >>> 4 { 1+ }!  // adds 1 to the argument
    5
    >>> {.. *}: sqr;  // duplicate and multiply
    >>> 5 sqr!        // blocks can be assigned to names, like any other value
    25
    >>> '2 2 +'!  // can also evaluate strings
    4

Unlike `!`, the invoke operator `%` executes the block in a new ""scope"". This means that only the top item on the parent stack is visible to the inside of the block. As well, any names that are assigned inside the block remain local to the block. 

While only one argument is accepted, all the results of invoking a block are added back to the parent stack.

An example, calculating the factorial:

    >>> {
    ...     .. 0 > { .. 1- factorial% * } { ;1 } if  // ternary if
    ... }: factorial;
    >>> 5 factorial%
    120

To invoke a block with more than one value, an argument list or tuple can be used.

    >>> (1 2) twoargs%

To pass multiple results from one block directly into another, the composition operator `|` can be used. This operator actually functions just like `!`, 
except that the result of invoking the block are collected into a single tuple.

    >>> (x y) somefunc | anotherfunc%

I imagine named arguments could be accomodated Lua-style by passing a single table as the argument, once I implement a table data type.

Since using the `!` may necessitate working with lists and tuples, some support is built in for that.

The unpack operator `~` will take a list or tuple and push its contents onto the stack. The pack operator `<<` will take an integer and collect that many items from the stack into a new tuple.

    >>> 'a' 'b' 'c' 3<<
    ('a' 'b' 'c')

The indexing operator `$` replaces a list and an integer with the n-th item in the list. Indices start at 1.

    >>> ['a' 'b' 'c' 'd' 'e'] 2$
    'b'

As well, there is a multiple assignment syntax specifically intended to make handling argument lists more convenient.

    >>> [ 'aaa' 'bbb' 'ccc' ]: stuff;
    >>> stuff: {thing1 thing2 thing3};
    >>> thing3
    'ccc'

    >>> {
    ...     :{arg1 arg2 arg3};
    ...     arg2 arg1 - arg3 *
    ... }: do_something_with_3_args;


Blocks are very much like anonymous functions, it seems natural to do things like map and fold on them. I haven't yet implemented built-in ""blocks"", but I plan to support at least `map` and `fold`.

`map` will invoke a block on every element of a list and produces a new list from the results.

    >>> [2 3 4 5] {.*} map!
    [4 9 16 25]

`fold` can work by pushing each item onto the stack and then evaluate the block.

    >>> 0 [2 3 1 5] {+} fold!  // sum a list of values
    11

Note that since `map` and `fold` must operate on more than a single argument value (and using argument lists for such basic operations would be annoying), they use `!` syntax instead of `%`. 

This general rule helps distinguish calls that could potentially consume an arbitrary number of stack items. I'm inclined to call blocks intended to be used with `!` something like ""macro blocks"" and blocks intended to be used with `%` ""function blocks."" Not sure how much of an abuse of terminology that is.

That's all for now! I've already written quite a bit! If you've stuck with me so far thank you for reading and I hope you found it interesting.",Interesting. Concatenative languages are interesting but you didn't reference any of the existing ones in your post. Have you looked at Joy or Factor? Obviously there is a long lineage here going back to Forth and includes Postscript.
Can a language ever be faster than its parent language?,mn4ive,2021-04-09 07:35:25,"I was thinking about this today and couldn’t find much by searching. 

If a language were written in Python and interpreted in Python, could it ever be faster than Python? What if it was compiled with something like Cython?","Depends on what you mean by faster.

Say you have a host language H and a child language C, where the interpretor for C is written in H.


I don't believe that C can execute faster than H in the normal case.  I can think of some edge cases though.

1. Maybe, for a specific pattern of code, the optimal way to implement a feature is very difficult or annoying in H.  In C, the programmer has easy access to that feature and can use the optimal form of that code all the time.

2. Statically reasoning about code in H may be wrong given real usage, but a runtime for C (written in H) would be able to dynamically analyze how the program is being run and:
- spend more time optimizing the important parts of the program
- generate better code after understanding/having data on likely branches and commonly chosen code-paths
see: runtime optimization

If you extend the definition of interpreter and you allow for JIT compilation in general, you open a big can of worms for optimization because then the interpreter is generating and executing assembly, the lowest level interface to the computer itself, so theoretically the fastest thing you can make.  Just need a good JIT."
Compiler with support for formal verification and mathematical proofs of code,jysev8,2020-11-22 16:33:13,"Hi! 

Some time ago I made a basic compiler similar to [Whiley](http://whiley.org/). I've been working on this tutorial series that explains the details behind its implementation.

https://www.youtube.com/playlist?list=PLih6dHuLK743S0yg7sqyPEJ7nbNfKQCYe

The plan is as follows:

  - explain and introduce basics with help of OpenJML
  - explain inner working of SMT solvers with Alt-Ergo
  - explain the theory behind Hoare logic and show fully rigorous proofs of code done manually
  - show the actual implementation of compiler
  - explain Floyd's extensions for Hoare-Floyd logic
  - introduce Weakest Precondition Calculus
  - extend the compiler with additional things such as arrays, algebraic data types, pointers, etc

I am curious how many people are interested in this topic. Let's see how much attention this post can get. Also, any feedback and suggestions are greatly appreciated. I'm constantly trying to improve on the quality of my videos.","Low karma accounts are often used for spam, and your account has less than 100 combined karma. As a result, this post has been removed. If you
feel this is incorrect, please send a message to the moderators (and include a link to this post).

Please note that ***this subreddit is about programming language design, not programming per se***
and is not the place to ask generic questions about programming, such as ""What language should I use"" or ""How do I write a program using X"".


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ProgrammingLanguages) if you have any questions or concerns.*"
Rescript: Safer and faster alternative than Typescript,ilqmx7,2020-09-03 18:05:54,,"ReScript is renamed BuckleScript if anyone was wondering.

Edit: They have also changed syntax a bit."
"Joe Duffy: ""The Error Model""",iaffh7,2020-08-16 05:05:07,,"Long and very interesting read. The point that stuck with me the most is this simple realization: ""Bugs aren't recoverable errors!"", so language design that makes this explicit is going to make life easier for the programmer.

An index-out-of-bounds is a programmer's mistake. The code has to be changed to fix the bug; failing fast and dropping the whole program on the floor seems legitimate, as there is no way of telling what else will go wrong afterwards (e.g. this is part of Erlang's recipe for reliability).

On the other hand, a network timeout or a file-not-found are situations that a program should be able to anticipate and react to, so the language should provide a mechanism to handle them in stride."
"The ""unattainable"" Programming Languages research problems",g335xu,2020-04-17 22:35:33,"Mathematics has the Millennial problems. 

Physics has the Theory of Everything. 

What would one consider some of the ""ultimate""(most impactful if achieved) ideas in Programming Languages research?

(Disclaimer: This is a very subjective question with different people having different opinions, but I would love to hear them all. Also, I understand my claims about the Mathematics and Physics problems might be controversial but I just wanted to give an example)","Fast, easy, pauseless, low-overhead, multi-threaded memory management.

(EDIT: Despite the upvotes, I think my answer is really poor here, and just piling on conditions does not do it justice.
I may have still not qualified this very well; there is a tremendous amount of research here. But the thing that I want to point out is a lack of a ""perfect"" GC which achieves every single property inherently, you can get really far with many good properties. I believe it to be the case that no solution can even be both 100% pauseless (in having constant worst case allocation time), and have no more than a constant factor overhead)."
Dogelang – A Python with Haskell Syntax,f710m5,2020-02-21 06:28:20,,"Awesome, the things I dislike about both languages!"
I'm writing free a book on designing and implementing programming languages. The first few chapters are up now. I'd love to know what you think!,5o7y4z,2017-01-16 09:41:44,,"It seems a shame to leave this sitting here with no comments, but at the same time I have to shake off a strong sense of not really being this book's target audience.

Frankly whenever I read a language design (and/or implementation) text these days, I spend a little bit of effort trying to follow along and then ultimately give up in exasperation *because the author isn't designing the exact same language I would design*. I state this as nothing less than a confession of my own severe tunnel vision, and certainly not a critique of any particular work - least of all yours.

I read up to the part on scanners before cashing in, so for what it's worth, I made it much further in your book than I have in any other PL text in several years :-)

Overall I found the style light and easy to read. I fear I have lost my ability to read with beginner's eyes, so I have no idea how approachable it is for Joe Programmer. It did seem to hit a nice balance between exploring the implementation details of things and the theoretical underpinnings; that is to say, I feel like a motivated reader should have no difficulty pulling out `$search_engine` and digging deeper into any of the concepts that are lightly introduced in the book.

At the risk of belaboring my own self-deprecation a bit much, I don't think I'd have the patience to tackle a project like this personally, so I admire your efforts accordingly. Maybe it'd be more accurate to say I would spend more time second-guessing myself than writing.

That's probably why I spend more time implementing languages than writing about it ;-)"
"Hurl, a terrible (but cute) idea for a language",14hs70w,2023-06-24 20:46:23,,"This is algebraic effects, right?"
"Even if you can't write assembly like a poet, you can read disassembly like a hunter",12bjhf1,2023-04-04 21:39:02,,xor eax eax is one of those hilarious x86 family idioms that will always make me chuckle
What are the issues with algebraic effects?,11ti9sc,2023-03-17 13:36:26,"It seems this sub has a liking for algebraic effects. You can’t go a week without seeing a post here where someone wants a language with X only to find that X is already solved by algebraic effects. After playing around in Koka I’m slowly getting the idea of algebraic effects and so far I like them. It makes me wish that more mainstream languages would adopt algorithmic effects, but at the same time I’m skeptical. What are some of the problems with algebraic effects? What hurtles do language designers still need to overcome with implementing algebraic effects? Are there any problems that would be easier and/or safer to implement without algebraic effects than with?","The context of your questions is not clear to me. I am going to assume you are asking your questions with research in mind.

> What are some of the problems with algebraic effects?

There are many open problems in both the theory and practice of programming with effect handlers. I will list some focused on the practical side of things here:

**Multiple resumptions and linear resources**

In their full generality, the continuations arising from effectful operations may be invoked more than once. Mixing multiple resumptions of continuations with linear resources poses an interesting challenge: how to maintain modularity and composability of effectful programs, whilst ensuring the integrity of the said linear resources?

**Scalable effect systems**

A scalable type-and-effect system design for programming with effect handlers remains an open question. There are a lot of different type-and-effect systems out there, but they all have their shortcomings that come to light when you program with effect handlers in anger. You can consult Hillerström and Lindley (2016), Lindley et al. (2017), Leijen (2017), Brachthäuser et al. (2020), and Convent et al. (2020) to see some different points in the design space.

**Efficient implementation techniques**

A substantial part of the literature is dedicated to studying efficient implementation techniques for effect handlers. Nevertheless, there is still scope for improvement. I believe new techniques will continue to emerge as different languages has different runtime constraints.

**Use-cases / applications**

Effect handlers are just now making their way into the mainstream programming languages (either via native support or library support). It is now becoming feasible to use effect handlers for ""real-world"" use-cases. As an example see the OCaml library eio, which makes crucial use of effect handlers to implement efficient I/O in direct-style. We have only begun to scratch the surface here. It will be exciting to see what kind of applications people come up with.

> What hurtles do language designers still need to overcome with implementing algebraic effects?

For an existing programming language I think the one of main challenges is how to retrofit effect handlers onto the language without penalising legacy programs. Sivaramakrishnan et al. (2021) identify and discuss solutions to this challenge in the context of OCaml. Another challenge is interacting with a language's FFI, e.g. how do you handle effects across language boundaries? I don't think anyone has solved this challenge yet.

> Are there any problems that would be easier and/or safer to implement without algebraic effects than with?

Currently, the big selling point of effect handlers is that they let you implement structured concurrency abstractions in a modular and composable way as a user-definable library. The key ingredients that makes this possible are direct-style programming and effect forwarding. The former lets you use compose effect handler-code with non-effect handler-code seamlessly. The latter lets you compose effect handlers seamlessly to implement ever more complex effect behaviour."
Build Your Own Programming Language,zo7aog,2022-12-17 22:22:27,,"I assume the book is about a stack VM interpreter, rather than registers based? (Like most of the resources on internet)"
"Via lambda-the-ultimate.org: Computer History Museum releases source code for PostScript, with Adobe's permission and support",zkjztp,2022-12-13 10:43:07,,I wonder if they cleaned it up before releasing it :)
"I ""coded"" an insertion sort algorithm in my own esoteric programming language!",x1p5kq,2022-08-31 01:32:32,"**\[Video below\]**

Pikt is an image-centric esoteric programming language which I love developing and using!  
In this example a list of integers is sorted via an insertion sort algorithm, and a custom color scheme gives it an aesthetically pleasant and familiar look (you can basically customize every ""keyword"" of the language).

The initial output you see is the generated Kotlin code, which is then compiled and/or interpreted.

If you wish to see more:

* Repo: [https://github.com/iAmGio/pikt](https://github.com/iAmGio/pikt)
* Explanation: [https://github.com/iamgio/pikt/wiki/Insertion-sort-breakdown](https://github.com/iamgio/pikt/wiki/Insertion-sort-breakdown)

&#x200B;

https://reddit.com/link/x1p5kq/video/d57v1amjyvk91/player",Looks cool! I still don’t get how you go from the bridge to the other layout. I am also curious why did you decide to make your language imperative? Using pixels for keywords seems more suited for something similar to APL.
Peridot MVP,wjsxwh,2022-08-09 11:37:08,"Hey all! I've been working on my programming language [Peridot](https://github.com/eashanhatti/peridot) for about six months, and it's finally at the point where I can call it an MVP! Peridot is a language in which the compiler backend is implemented in userspace via metaprogramming.

At this point, the implementation is reasonably mature. This includes

* Functional programming with full-spectrum dependent types
   * Dependent function types
   * Dependent record types
* Singleton types + struct patching
* Typed logic programming with higher-order hereditary Harrop formulas and higher-order abstract syntax (HOAS), akin to [λProlog](https://www.lix.polytechnique.fr/~dale/lProlog/)
   * Typesafe and scopesafe metaprogramming
   * [Pattern unification](https://github.com/AndrasKovacs/elaboration-zoo/tree/master/03-holes)
* Imports
* A query-based compiler
* [Normalization-by-evaluation](https://davidchristiansen.dk/tutorials/nbe/) for efficient compile-time evaluation
* [Glued evaluation](https://gist.github.com/AndrasKovacs/a0e0938113b193d6b9c1c0620d853784) for small error messages and metavariable solutions

Known issues:

* Some mutually recursive types cause the typechecker to loop
* My implementation of LP is rather slow (it's good enough for demonstration purposes though)
* There's a few edge cases where the typechecker permits ill-typed function calls
* Queries sometimes produce duplicate solutions

&#x200B;

Check out [the README](https://github.com/eashanhatti/peridot/blob/master/README.md) for more information on the language's design. Example code can be found [in this folder](https://github.com/eashanhatti/peridot/tree/master/examples) and [in this folder](https://github.com/eashanhatti/peridot/tree/master/test). Currently no docs exist, so you'll just have to get your questions answered here ;-)

The userspace backend - that is the entire point of the language after all, hah - is currently being implemented. I'll probably make another post when it's finished!","Impressive work!

The link for ""Glued evaluation"" appears to not be the intended one (it's a link about NbE), but for example [this gist](https://gist.github.com/AndrasKovacs/a0e0938113b193d6b9c1c0620d853784) could be a reasonable reference."
Ebel - Programming language designed for genetic programming and file editing,txtw2g,2022-04-07 03:19:09,"Hello, I would like to showcase here a little bit from my interpreted programming language on which I've been working for the past almost a year.

So long story short I'm working on a compiler ([Ebe - Edit by example](https://github.com/mark-sed/ebe)) for file editing based on user examples (instead of code) and it uses genetic programming to accomplish this. It is also meant also for non-programmers since the user does not write any code. So you feed 2 files into the compiler - a snippet of the file you want to edit (e.g. a first line from csv) and then the same file, but edited by hand (so perhaps one value deleted one switched with other). Ebe then takes this and uses it for its fitness function to find a fitting algorithm in Ebel.

So the requirements for Ebel were that it needs to be easy to generate, mutate and crossover for genetic programming and also it is specialized for file editing (so technically transforming lists of lexemes). For this reason I chose the approach of having so called ""passes"", where a pass determines how the file is parsed through (by words or entire lines) and then this pass parses its lexemes (line, text, number, float... but later on other file types might be added, so it could be even more specific) and these lexemes are fed into instructions, which can be imagined as a pipeline, where for one lexeme is one instruction (and then also control instructions and possibly a loop). Here's a simple **example** to describe this in a better way:

    PASS Words
      NOP
      DEL 
      DEL 
      PASS number Expression
        SUB $1, $0, 32
        MUL $2, $1, 5
        DIV $0, $2, 9
        RETURN NOP 
      PASS derived Expression
        RETURN DEL
    PASS Lines
      SWAP 1
      LOOP

**This example does the following:**

* PASS Words - File will be interpreted word by word and for each line it will:
   * NOP - 1st object will left as is.
   * DEL - 2nd object will be deleted.
   * DEL - 3rd object will be deleted.
   * PASS number Expression - 4th object, if it is a number will be:
      * SUB $1, $0, 32 - Subtract 32 from its value.
      * MUL $2, $1, 5 - Multiply the new result by 5.
      * DIV $0, $2, 9 - Divide the result by 9 and save it as the new value for the object.
      * RETURN NOP - Don't modify the new result.
   * PASS derived Expression - If 4th object was not number, then use the following without regarding its type (derived = any type).
      * RETURN DEL - Delete the object.
* PASS Lines - File will be interpreted line by line and for each line:
   * SWAP 1 - Swap current line with the following one.
   * LOOP - Repeat until all lines were processed.

As you can see it looks similar to a bytecode, which was an intention to make it work nicely with GP and also quick for interpretation both of which are true, but to be completely frank, there are still some flaws, which need to be handled, such as nested passes to allow for really specific edits and so currently not all imaginable edits are possible.

Also to sum up a little bit how the GP performs, it does really great on simple edits which are periodical through the file, so stuff like deleting whole columns and then swapping some columns. For example I used it to extract values from some column in a markdown tables or using it to modify numeric values in structured files (gtf file - modifying offsets of genes). But some harder tasks still take longer than I would like to compile, which will hopefully get better in the future, but that does not concern the language, so won't get more into that.

I also have a question **if** **someone knows about something similar** (language or compiler that writes the code for you)? Also any feedback on the design is appreciated, but bear in mind that it's not a language to be written by people (although it can and it contains some syntactic sugar).

And a second question,  if you were to design **a programming language just to edit text files, how would you do it?**","That looks super cool and like a really challenging project, I have no idea how I’d go about implementing thay though lol.

> if you were to design a programming language just to edit text files, how would you do it?

If I wanted to write something more robust than a command tool but still strictly for file editing It would probably look like a weird mixture of Haskell and Awk. It’d be like Haskell but way stripped down so it’s simpler to use and implement but regular expressions would be first class. I’d also want to include an IO type like in Haskell. I also like how Awk tokenizes each row into columns and abstracts away having to iterate through each line. I’d probably use a main function for entering the program rather than being loaded immediately like in Awk."
"λ-2D: An Exploration of Drawing as Programming Language, Featuring Ideas from Lambda Calculus",twfqkc,2022-04-05 06:57:17,,"This is the most innovative new programming language I've seen. Incredible. Keep up the good work!

**EDIT** Would it be possible to do pattern matching graphically?"
What programming language features would have prevented or ameliorated Log4Shell?,rfml37,2021-12-14 02:33:58,"Information on the vulnerability:

* https://jfrog.com/blog/log4shell-0-day-vulnerability-all-you-need-to-know/
* https://www.veracode.com/blog/research/exploiting-jndi-injections-java

My personal opinion is that this isn't a ""Java sucks"" situation, but rather a matter of ""a large and complex project contained a bug"". All the same, I've been thinking about whether this would have been avoided with certain language features.

Would capability-based security have removed the ambient authority needed for deserialization attacks? Would a modification to how namespaces work have prevented attacks that search for vulnerable factories on the classpath? Would stronger types that separate strings indicating remote resources from those indicating local resources make the use of JDNI safer? Are there static analysis tools that would have detected the presence of an exploitable bug here? What else?

I'm very curious as to people's thoughts. I'm especially interested in hearing about programming languages which could enable some of Log4J's dynamic power in safe ways. (Not because I think the JDNI lookup feature was a good idea, but as a demonstration of how powerful language-based security might be.)

Thanks!","The root problem is that programmers are unwilling to say no to features.  The social reason is fairly simple, I think: a feature makes your users happy, and if they even show up with a patch, it even seems free!  Of course the true price will be paid later, over time, and is probably not even known immediately.  It's like taking a variable-interest loan with infinite running time.  The most obvious solution is for maintainers to say ""no"" to new and complex features, unless it really is a feature that is critical to the majority of users.  Of course, this may just result in the project being forked and people switching to the fork that includes every feature for everyone.

As a social problem, it probably doesn't have a simple technical solution.  But language features might help make it easier to gauge the true complexity cost of a feature.  You mention capability systems, and they are indeed a good way to make at least some of the complexity more evident.  If a patch for your logging library requires giving the logger the ability to load code over the network, then it may seem more obviously suspect.  Of course, that doesn't mean you won't accept the patch to please the user.

If you really want ""safe"" dynamic code loading, then sandboxing might work, but I really think it's better to think more carefully about why we end up with such complex features in code that isn't really supposed to be solving a very difficult problem."
"Cwerg - an opinionated, light-weight compiler backend",oe9qr2,2021-07-05 23:30:31,"[https://github.com/robertmuth/Cwerg](https://github.com/robertmuth/Cwerg)

Cwerg has been my ""covid project"" for the last year+.  
It can directly generate arm32 an aarch64 elf executables and has essentially no dependencies.   
It is still a bit rough around the edges and so far it only has a proof-of-concept subset-of-c frontend but I am happy to work with anybody adventurous enough to  give it a try with their frontend.","Very interesting, would definitely be interested in discussing this with you as the backend for my language Flogram.

Any idea performance wise how good it is? Both compilation speed and performance of output code vs say LLVM?"
Kalyn: the Lispy Haskell (not by me),nv2kxo,2021-06-08 20:08:24,,I too clicked on this guys GitHub page after reading the hn article xD
Domain Specific Languages for Computational Law,lg5qwa,2021-02-09 23:30:38,,"Very interesting! It's good to see work on computational law gaining traction again: Automated reasoning is needed in many government organizations and also private companies, and declarative formalisms such as Prolog and logic programming which are mentioned in this repository can help to formalize laws so that they can be executed and reasoned about with computers to a quite significant extent."
Perceus: Garbage Free Reference Counting with Reuse,k9ixz2,2020-12-09 10:28:55,,"Hey, I'm on that paper! Happy to answer questions about it.

ETA: No one should hesitate to send me an email with questions about my research if they don't want to ask publicly here. My email is at the top of the PDF."
Blog post: Three Architectures for a Responsive IDE,hul5a8,2020-07-20 21:22:25,,"Talks about different language designs and ways to do code completion etc in them.

Might be interesting to some of you."
One-pass Compiler Primer,go64us,2020-05-22 05:51:54,,Here is a free book about a one-pass compiler by Prof. Wirth himself: [https://inf.ethz.ch/personal/wirth/CompilerConstruction](https://inf.ethz.ch/personal/wirth/CompilerConstruction).
C-Reduce - it’s a kind of magic!,f6ef98,2020-02-20 01:53:12,,"Something interesting is that creduce was specifically built to handle csmith's output.

csmith (for those who don't know) was a project designed to identify errors in C compilers. It works by generating random programs that are guaranteed to be semantically valid, and then compiling those programs with a number of compilers and comparing the results. If the compilers do not all agree on one result, there must exist a bug in one of the compilers.

For some reason, csmith had the best luck finding bugs when it generated programs that were some 63kB in size. But apparently the compiler developers were not fond of receiving example files of this size when the csmith team tried to submit bugs.

So creduce was born in an effort to enable bug reporting that wouldn't be ignored, and then it turned into a research project all of its own!"
The Lobster Programming Language,b949a8,2019-04-04 05:52:14,,"I found this link to be a better description:

[Lobster Design Philosophy, History, and Future
](https://htmlpreview.github.io/?https://raw.githubusercontent.com/aardappel/lobster/master/lobster/docs/philosophy.html)

I like it!  The thing that gives this credibility IMO is that the design acknowledges a lot of tradeoffs (in addition to knowing something about the author's previous projects).

Probably 75% of the language description pages I see are too close to ""this is going be the best of all worlds"", which inevitably leads to a letdown.

If anyone manages to run it, let us know :)


"
How .Net JIT compiles generic functions so fast?,13lzxgd,2023-05-20 00:09:27,"I'm making a lot of assumptions in this post, so don't blast me if i'am wrong about something.

I know that JITs must be fast to compile bytecode to native code, otherwise they would introduce too much overhead on program startup.
I also know that .Net CIL supports generics.
I also also know that generics slow down a lot compilation (for example as from as i know c++ compilers are slowed down especially by templates, or am i wrong?)

My question now is, is .Net JIT fast at compiling generic functions? If so, how can it be so fast?","This is a great question with a really cool answer!

First of all, generics don't have to be slow to compile. Generics in Rust and especially C++ are slow, because they are implemented via momomorphization, i.e. these compilers compile a new copy of the function for every possible type that it is used with. C++ is especially bad in this regard, since it needs to re-typecheck every single instantiation.

But Generics in Java, OCaml, Haskell, or most high level langauges for that matter don't slow down compilation. Instead of monomorphization, these languages use a uniform representation for values, i.e. under the hood, everything is just a heap-allocated pointer. In these languages, generic functions are treated just like any other function, because every type can use the same implementation.

The downside of this approach is that, well, everything needs to be represented by a single heap-allocated pointer.

What does .NET do then? Many types in .NET, namely classes, *are* heap allocated. For these, the runtime just compiles a single implementation.

But .NET also supports value types! Value types are not heap alllocated, so these are compiled via monomorphization like in C++ and Rust, with the caveat, that they are monomorphized *lazily at runtime*. This means that the first time you call a generic function at a given value type, a version of that function specialized to that type is compiled.

This gives .NET the throughput benefits of monomorphization, while keeping the latency of uniform representation."
"Anyone use ""pretty"" name mangling in their language implementation?",12m738n,2023-04-15 02:18:26,"I've been having some fun playing about with libgccjit!

I noticed the other day that it won't allow you to generate a function with a name that is not a valid C identifier... Turns out this is because when libgccjit was first built in 2014, the GNU assembler could not yet support symbol names beyond that. This has since changed in 2014, from then on GNU `as` supports arbitrary symbol names as long as they don't contain NUL and are double-quoted.

This has given me an idea to use ""pretty"" name mangling for symbols in my languages, where say for instance a C++-like declaration such as:

```
class MyClass {
  int some_method(
    char x,
    int y,
    float z
  );
}
```

gets mangled as:

`""int MyClass.some_method(char, int, float)""`

Yes, you read me correctly: name-mangling in this scheme is just the whitespace-normalised source for the function's signature!

I'm currently hacking on libgccjit to implement support for arbitrary function names in the JIT compiler, I've proved it's possible with an initial successful test case today and it just needs some further work to implement it in a cleaner and tidier way.

I'm just wondering, does anyone else mangle symbols in their langs by deviating from the typical norm of C-friendly identifiers?

Edit: I've just realised my test case doesn't completely prove that it's possible to generate such identifiers with the JIT (I remember seeing some code deep in its library implementation that replaces all invalid C identifier characters with underscores), but given the backend support in the GNU assembler, it should still be technically possible to achieve. I may just need to verify it more thoroughly...",i use ‘module_name:identifier’
Context: The Missing Feature of Programming Languages,11j73jg,2023-03-06 02:37:53,,"Welcome to the world of effect systems! As you observe in this post, there's plenty of assumptions about the contexts that functions are called in today, which effect systems are an effort to encode. Koka (my favourite effect-based language for now) has an almost direct equivalent for your `with` statements :)

    with locking_scheduler()
    
    myFunction1()

I also think eff has some very fun demonstations over here: https://www.eff-lang.org/try/"
"""Discussions about programming languages often resemble medieval debates about the number of angels that can dance on the head of a pin instead of exciting contests between fundamentally differing concepts.""",yp07hz,2022-11-08 05:00:16,"This is just a really fun paper, in case you didn't read it.

> Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs

https://dl.acm.org/doi/pdf/10.1145/359576.359579 (PDF)","Great paper, though I don’t agree with that quote. A lot good conversation goes on in this subreddit, about all different kinds of language ideas."
We Need Simpler Types (speculations on what can be improved in future type systems and on erasing the boundaries between types and values),xd69z6,2022-09-13 20:31:03,,"I definitely agree with this post.

I think the ideal is some sort of dynamic/uni-typed language with compile time refinement checking. And refinement checks should be written the same if they're compile time or run time, and the compiler should figure out when they should be run."
Share a niche programming language you have tinkered with before,wps82e,2022-08-16 19:48:52,"I'll start. During my early years, and the popularity of Warcraft 3 I tinkered with this language. 

[JASS (Just Another Scripting Syntax)](http://jass.sourceforge.net/doc/) is an event driven scripting language used in Blizzard Entertainment's Warcraft III game. Map creators can use it in the World Editor to create triggers and AI scripts.

It looks like this:

    function Trig_JASS_test_Actions takes player p returns nothing 
      local integer i=0
      loop
        exitwhen i==90 
      call DisplayTextToPlayer(p, 0,0, ""Hello, world! ""+I2S(i))
        set i=i+
      endloop
    endfunction","Well, at the risk of sounding like a broken record I have to bring up [Céu](http://ceu-lang.org/) again. A structured synchronous reactive programming language in the tradition of Esterel, targets embedded devices and essentially compiles everything to a FSM in C (which it uses as its ""host language"").

    input int KEY;  
    par/or do  
      every 1s do  
        _printf(""Hello World!\n"");  
      end  
    with  
      await KEY;  
    end

The above example prints the “Hello World!” message every second, and terminates on a key press.

It does single-threaded concurrency really elegantly and with very low memory and energy overhead, at the cost of certain assumptions about relatively low CPU intensity per event. It also has a built-in concept of time.

It's really *really* nice to program Arduino boards in, in my limited experience. Especially for somewhat complex interactions.

The papers section of the website is full of good stuff:
http://ceu-lang.org/publications.html"
How do languages like rust determine exhaustive patterns? Can this be extended to include compile time constraints?,w1uco8,2022-07-18 16:43:10,"In particular you may have match statements like

    match x {
         -2147483648 ..= 100  => ""less than hundred"",
         102..=2147483647 => ""more""
    }; 

The compiler is detail oriented enough to understand that 101 isn't covered. How does this work?

The reason why I'm interested is if there is a way to understand other constraints as well. For example, can we subtype int32 to only contain numbers less than 10. For example the syntax could be something like

    x: i32 for x < 10 

Then this could be used to have compile time constraints. So if there is a function that requires an argument of an int less than 20, we can be assured that we can pass x to that function.

Is this doable? Or is there something more complicated? One point of concern is that we can't necessarily verify that if x < 10 then x < 20 unless we define that as an invariant.","[Here](http://moscova.inria.fr/~maranget/papers/warn/index.html) is an article about checking for exhaustiveness.  Handling ranges like in Rust is basically the equivalent of an *Or*-pattern, which the article covers. However, the approach is based on enumerating constructors, which is probably a bad idea when you have four billion of them.  I think the approach works fine if you use a better representation for integer ""contructors"".  I would use a list of (begin,end)-pairs.  Since the list is bounded by the number of cases in the original program, this should never grow too large, and it is easy to check whether the final list covers the entire possible space.

Handling arbitrary constraints is not so easy.  While the compiler might be able to deduce that `x < 10` describes a well-defined range of possible values, that will not scale to any user-defined function.  And even if you only allow primitive operations, what about a constraint such as `x % 2 == 0`?  It is trivial to do these checks dynamically, but it is very difficult to verify exhaustiveness statically.  I suggest looking at [refinement types](https://en.wikipedia.org/wiki/Refinement_type) for an approach that tries to solve the problem you describe.  I think Liquid Haskell is the most mature implementation of refinement types in a real programming language."
Your language's favorite MINOR feature?,tm13nj,2022-03-24 16:24:19,"u/Uploft asked about your favorite feature of your language and a bunch of people I bet didn't answer because it would start with: ""Welcome to my TED talk. First, let me define monadic hylomorphism"", and even I trying to write a scripting language for virtual babies and dogs would have to bang on for a bit to convince you it was a good idea. But y'all had some *little* ideas, didn't you?

Mine ... I've tried to copy my syntax wherever possible, usually from Python, from Go where that fails me, but I have always been irked by continuations so in Charm (a syntactic-whitespace language) they have to be marked. A continued line ends with `..` or , because I'm feeling generous and it suits the language, with `,` in situations where it's syntactic. And the continuation, either way, has to begin with `..` so that, darn it, you can tell that that's what it is.

It's a very minor point but I felt strongly enough to spend one of my strangeness dollars on it, this is the molehill I'll die on.

What's yours?","In [Star](https://github.com/ALANVF/star), commas and newlines are analogous everywhere, even inside array literals. This actually solves the issue of trailing commas by not needing commas at all"
First release of okta,sasfr8,2022-01-23 20:16:43,"Hi! Today, I release the first version of *okta*, a programming language I have been working on for half a year now. I started okta as a summer project, but as I had a lot of fun developing it, I decided to continue the project. Nowadays, I consider okta quite usable, so here I am, releasing the 0.1.0 version!

Link to the [webpage](https://okta-lang.org/).

You can find some examples [here](https://git.sr.ht/~mikelma/oktac/tree/main/item/examples).

This is my first attempt to create a programming language, so help and feedback is very appreciated!","Do you really want to get into a trademark fight with a company worth billions of dollars?

EDIT: because of how trademark law in the US works they have no choice but to send you a c&d and then sue you if they ever hear of you."
"I made a Turing complete, open source programming language that compiles to executable from generated assembly",rxkpq1,2022-01-07 02:01:57,,"It's kind of impressive how toxic the comments over on r/programming are. Don't listen to any of them. It might not be the most impressive project, but it's still a good step and something that will certainly be worth doing in the long run."
Made a interpreter for an Argentinian educational 1980's language,pk07ao,2021-09-08 09:01:06,"Hi everyone, I've made an interpreter for an old language called [TIMBA](http://dirinfo.unsl.edu.ar/servicios/abm/assets/uploads/materiales/23005-timba.pdf) made in Argentina during the 1980's. You can check it specification [here](https://github.com/qequ/retruco). 

It was originally a teaching language and didn't have a compiler, actually there were used real Spanish cards of Truco (a popular card game here in Argentina), it was more cs unplugged oriented, like  [CARDIAC](https://en.wikipedia.org/wiki/CARDboard_Illustrative_Aid_to_Computation) or [Little Man Computer](https://en.wikipedia.org/wiki/Little_man_computer).

I made a virtual machine and a compiler from the TIMBA language to the opcodes of my VM. I wrote it in Python because the performance is not central to the language. The compiler is heavily based on the [Teeny Tiny Compiler](http://web.eecs.utk.edu/~azh/blog/teenytinycompiler1.html) by Austin Henley.

It's my first interpreter, any constructive criticism/ pull requests is highly appreciated :)","I don't see any example timba programs in your repo. That might help some for understanding the language. Overall, sounds like a fun project!"
"BQN: finally, an APL for your flying saucer",p830gs,2021-08-20 19:06:18,,"Author here! Not too much to say that isn't on the website, but I'll be sticking around to answer questions.

A few recent happenings in the BQN world. I was recently a guest on [this episode](https://www.arraycast.com/episodes/episode-07-marshall-lochbaum-and-the-bqn-array-language) of the Array Cast, an array oriented podcast. Since then activity on [the chat forum](https://mlochbaum.github.io/BQN/#where-can-i-find-bqn-users) has really picked up, with somewhere around 20 active users discussing all sorts of things and complaining about how they don't have time to read it all. Come contribute! Finally, just Wednesday I finished up [header](https://mlochbaum.github.io/BQN/doc/block.html#block-headers) handling in the compiler, which was the last really big syntax feature left. dzaima's still working to get some aspects of this running in CBQN, but now it feels like I can say the compiler implements BQN instead of just most of it."
Into the Core - Squeezing Haskell into Nine Constructors by Simon Peyton Jones,nb6xj6,2021-05-13 11:03:40,,Link to the slides: http://www.erlang-factory.com/static/upload/media/1488806820775921euc2016intothecoresimonpeytonjones.pdf
Code on paper,n18k3n,2021-04-30 00:45:37,"Hello PL designers,

I am doing programming on paper a reality. I like to think on paper and I always felt the lack of the actual computation limit the thoughts I can have, so I decided to make a programming language suited for paper and pencil.

I am using iPad with Pencil as input devices. So far I have a grid paper where glyphs are written in the squares and it can highlight and evaluate math expressions. It works well for a crude prototype.

My aim is to bring the original vision of APL to life. APL started on blackboard long before it could be run on computer. It was always meant as a notation for expressing algorithms, i.e. “runnable math”. I am not very versed in APL family of languages, but they fascinate me because I know their origin and their attributes are well founded.

Today, I will not show you any demos since what I’ve got is not yet a programming language, but I would like to hear your ideas. APL feels weird for everyday coder, but it entails great concepts. It’s brevity is fascinating, but unnatural. I would like to fix this. Also, array-orientation is the way to go in a world where machine learning is prevalent. Ironically, APLs weird symbols/glyphs are really natural when you have pencil in your hand… 

Have you ever thought how would programming look like without a keyboard? What would be the ideal form for a hand-written programming language? How should work the IDE/REPL? Have you ever coded on paper? What was your experience?","Program flow is something that has concerned me as I try to design my own low-level ASM-like language.  For awhile I worked on stuff that I thought was going to result in ""shorter pipelining"" of commands.  I found that a number of my ideas were invalid because the convergence or divergence of operators and arguments was more than I could express in a 1-dimensional line of text.

Part of me wondered, ""What do the engineers who draw circuit diagrams do?""  But I rapidly concluded, they probably just have all kinds of 2D layout problems, and not any inherent additional clarity.  2D might be better for expressing program flows than 1D, but it's still a dimensional limit.  You're gonna end up drawing a line somewhere, where you need another line to cross over it, and that's uncomfortable.

We're not really capable of visualizing N-dimensional stuff.  We pretty much have to rely on our minds for such models.  From a program flow standpoint, we don't necessarily have to have a lot of dimensions.  A bunch of networked attachments at some 3D node, might be ok.  But... at some point it is gonna look like a mess.  I've crawled around inside my car plenty, to fix stuff.  I'm glad I've not really had a wiring problem, because trying to map wiring diagrams to what's going on in the car, sure is a pain.

Recently I made a [4 strut tensegrity prism](https://www.reddit.com/r/humblewoodworking/comments/m58x2n/tensegrity_4_strut_prism_from_branches/) and that was difficult to visualize and understand, when all the parts were flat on the floor.  Had a lot of trouble trying to decide which string to put in what hole.  This is like the problem of something that needs to be 3D when you only have 2D paper to represent what you're talking about.  Once I'd gone at it ""hands on"" in 3D for awhile, I finally understood the basic tensegrity principles, of what was happening in such a prism.  I don't think any amount of looking at pictures of such things, would have gotten me to such understanding.

An animation might have, though.  I think of the prism as basically a spiral, twist, or helix shape that wants to collapse downwards.  The side tendons, pull the main struts up and backwards to keep them from collapsing."
"Goodbye, JavaScript: Formality is now implemented in itself and released as a Haskell project and library!",kdrkvn,2020-12-16 02:42:34,,"That's some remarkable achievement, great job, looking forward for more progress toward Formality

&#x200B;

Congratulations dude"
Are there any programming languages where most data flow is in the left-to-right direction?,ja9mmn,2020-10-13 16:53:47,"Most programming languages do simple variable assignment like this:

    three_variable = 1 + 2 

The values are flowing from the right, over to the variable name on the left.

I've recently heard about how some languages have [(or might be getting) a pipeline operator, e.g. `|>`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Pipeline_operator) 

And I kind of like the left-to-right flow.  Even though I guess it wouldn't be practical for everything.

So it just got me curious if there are any regular programming languages where this is the normal way they operate overall?  Even for simple variable assignment like:

    1 + 2 = three_variable","UNIX shell of course:

    expr 1 '+' 2 | (read three_variable ; echo $three_variable)

Haskell with do-notation desugared:

    λ> pure (1+2) >>= \ three_variable -> print three_variable
    3

And variable assignment may not be the most interesting, pipelined processing is semantically more close to function application, UNIX shell again:

    cat /etc/passwd | sed s/you/me/g

Haskell:

    λ> import Data.Function
    λ> 1+2 & print
    3
    λ> 

Edh:

    (repl)Đ: 1+2 | console.print
    3
    (repl)Đ: 1+2 & console.print
    3
    (repl)Đ: desc(3)
    It is a `DecimalType` value `3`
    (repl)Đ: 3 | desc
    It is a `DecimalType` value `3`
    (repl)Đ: 3 & desc
    It is a `DecimalType` value `3`
    (repl)Đ: desc $ 3
    It is a `DecimalType` value `3`
    (repl)Đ:"
"Indentation-based syntax in Python, Nim, Haskell, F#, & CoffeeScript",hi2pwc,2020-06-30 00:11:11,,"Context free is such a good, underrated programming youtuber"
"Making C++ Memory-Safe Without Borrow Checking, RC, or Tracing GC",14g8uyu,2023-06-23 01:06:57,,"Hey all! Would love your feedback on this. It's basically an attempt to find methods of memory safety that are simple enough to retrofit onto C++.

I'll be submitting a proposal to talk about this at CppCon, though it's a very minuscule chance that that would actually happen. Still, be brutal and tell me everything that makes these a terrible idea!

I'm also thinking about adding a section to the end, briefly going over all the other methods of memory safety that I know, in case someone else can think of a method for making them fit C++. Thoughts?"
"Fixing the Next 10,000 Aliasing Bugs",11jpxqo,2023-03-06 13:47:39,,"So, nice way to explain borrow-checking... but I claim the culprit is mutation, not aliasing. If nobody can mutate, then a copy is semantically identical to an alias.

Maybe there are other applications of this concept, though?"
An idea for a language with both Functions and Procedures,10ni7if,2023-01-28 23:52:58,"This is an idea I’ve been toying around with for a while, and I wanted to see if it already exists somewhere or if there’s something I’m missing and it’s a bad idea.

The main idea is to have functions, which are actual pure functions, and procedures, where you interact with the outside world. A procedure can call a function or a procedure, but a function can only be defined in terms of other functions.

These functions being pure and not having monads or other constructs to force side effects into them should be easy to aggressively optimize. 

Procedures are where you interact with the outside world, and would contain most of the control flow, so they would be harder to optimize but the main idea is that procedures would usually be IO bound, so they would need less optimization. 

I think that this design would allow make such a language well-suited to data processing and other math heavy tasks, while still being reasonable to use for other applications. Essentially, make the “hot loop” very well optimized. 

If I were to build this, it would probably borrow a lot from Rust (who borrowed from ML) since I think not having a GC and getting native performance would help this a lot if its main use case is number crunching.","One challenge you'll run into with this—or any other scheme where the type systems functions into different kinds (think sync/async, gc/no-gc, etc.) is higher-order functions.

If you have a function that takes a callback and invokes it, is that function pure or not? It depends on the purity of the callback. You either need two copies of every higher order function, a pure one and a non-pure one (and thus two copies of every function that *calls* one of these), or you need to be able to express functions that are ""polymorphic over purity"" where the type system can figure out that the function is pure if it's callback is and not otherwise.

The latter is probably what you want, but it's a lot of complexity."
"I’m making a new language for fun. Should it use single “=“ sign for comparisons since I can do that, or keep two “==“?",10l4536,2023-01-26 01:26:54,Title,"= comparison 
<- for assignment

Lol doesn't matter have fun with it? You could use an emoji if you wanted."
"Cranelift's Instruction Selector DSL, ISLE: Term-Rewriting Made Practical",10h4znx,2023-01-21 02:55:07,,"I wonder how much of ISLE could've been implemented directly in the host language if it had [pattern synonyms](https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/pattern_synonyms.html) and [view patterns](https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/view_patterns.html), or perhaps using [Prisms](https://stackoverflow.com/questions/50915526/what-are-prisms). There seems to be a bit of conceptual overlap, in any case."
Building an interpreter for my own programming language in ChatGPT (and solving AoC 2022 with it!),zcti53,2022-12-05 10:03:10,,"Out of curiosity, are you able to share the language spec that you fed into ChatGPT? 

I'd like to try to reproduce & play around with it myself."
Language-agnostic source code query engine,xleion,2022-09-23 05:51:29,,Ive actually been looking for something like this for quite some time. I think there’s a huge space for tools in between grep and ‘find all references’
What constitutes a programming language?,uz4ulp,2022-05-28 02:42:26,"As I explore breaking free from the confines of purely text-based programming languages and general purpose languages, I find myself blurring the lines between the editors and tools vs the language.

When a programming language is not general purpose, at what point is it no longer a programming language?

What rule or rules can we use to decide if it's a programming language?

The best I can figure is that the tool simply needs to give the user the ability to create a program that executes on a machine. If so, the tool is a programming language.",[Relevant meme.](https://cdn.discordapp.com/attachments/530622307383771156/979850762102382632/compiler_alignment_chart.png)
"Rock v0.2.1, a little native toy language I've made with Rust and LLVM.",u7xwjx,2022-04-20 22:33:34,"This is nothing really new or fancy (or complete), just a toy language that I'm still a little shy to share here due to its early nature.

[https://github.com/Champii/Rock](https://github.com/Champii/Rock)

However I feel it's ready enough to receive its first criticism

You can use the [stdlib](https://github.com/Champii/Rock/tree/master/std/src) implementation as an example folder

Here is a sample from the readme to show what it looks like:

    fact a =
        if a <= 1
        then 1
        else a * fact (a - 1)
    
    main = print fact 4","There's already a Roc language, in case you want to avoid confusion.

https://www.roc-lang.org/

Maybe you can play Roc paper scissors to see who gets to keep the name."
Does Rust prevent more logic bugs than statically-typed pure FP languages?,teskx2,2022-03-16 00:13:15,"~~--Rust has a reputation of--~~ Rust users sometimes express that it feels like ""if it compiles, it works"" more than other languages.

Some guesses about why, off the top of my head:

 * In most languages, we can find ourselves in situations where we assume that some existing data doesn't change. When we then break that assumption, we can cause bugs. However, when we have a borrow in Rust, it means nobody can accidentally change that data, so nobody can accidentally violate our assumptions.
 * In Rust, if we want to mutate an object, we need an &mut to it, which often means we first need a &mut reference to its parent. This forces us to consider the parent object (and all of its other children) in any mutation, and so reminds us to maintain invariants between them. We could call this a ""top down"" style of coding.
 * In Rust, we generally have to refactor more and think more about how we're doing things before we can successfully compile (because there are extra constraints at compile time compared to Java, specifically aliasability-xor-mutability), which happens to give us more time to think of possible bugs.
 * (any other reasons?)

So now, my main question: are these points also present in (statically typed) pure functional languages like Haskell? Respectively:

 * Pure FP languages also enforce that existing data doesn't change.
 * In pure FP languages, our data is still organized hierarchically, and to ""modify"" a piece of data, we have to recreate it, and its parent, and *its* parent, and so on, to get a new entire world. So, we have to ""go through"" its parent anyway.
 * We probably have to refactor more in pure FP languages too (because it also has extra constraints compared to Java, specifically that everything's immutable).

Would love to hear your thoughts! Are there any logic bugs that Rust prevents that e.g. Haskell does not?

(Note that I'm specifically talking about the quality of reducing logic problems, let's keep away from comparing the languages overall, or their other aspects like performance.)

Edit: Clarified the ""if it compiles, it works"" notion.","Haskell allows you to restrict IO operations that can be performed by a given function and its dependencies. This is something Rust doesn't try to prevent, at least in a normal environment. Any function can open a file and write to it, and that is another shared mutable state. The same holds for GPU and other work. I don't mean to be one-sided, there are other logic error Rust is good at preventing too, especially if you want performance.

That said, there are some embedded operating systems that use Rust to restrict io as well. But that's pretty far from the normal environment"
Do you prefer doing GUI with a markup language and a programming language or just the programming language?,ru8u9e,2022-01-02 20:39:28,"e.g. C#/XAML, Java/XML, JS/HTML, Objective-C/Storyboard XML.. or Kotlin (Jetpack Compose), Dart (Flutter), JSX (React), Swift (SwiftUI)

I, for one, much more prefer working with a programming language alone. Microsoft tries too hard to push the C#/XAML with Windows GUI programming or cross-platform Xamarin.Forms. I prefer it because I don't have to switch tabs and also I don't need to do weird-finding-the-widget code like in Java (`#getViewById`). Jetpack Compose is a godsend for Android programming. Dart/Flutter is as well. Everything fits so nicely. Not so with markup language programming.","I find that it's easier to reason about GUIs when you can declare them rather than work with them imperatively. With markup you always get that, with code, it depends on the framework or library."
Learning resources for type inference?,rk4qnp,2021-12-20 04:16:15,"Hi there, I am a CS student and one of my personal side projects is to design and build my own programming language. I've had a lot of fun so far learning about parsing and the LLVM framework.

Right now however I'm becoming increasingly interested in type inference, I would love to learn how to implement some form of global type inference (similar to Haskell or Julia) and I was wondering if anyone had anyone could recommend some learning resources.

From what I could gather from my own research, both Haskell and Julia implement some version of Hindley-Milner type inference. Unfortunately, I couldn't find much on it online. 

I learned a bit about natural deduction and discrete mathematics at uni. If there are any mathematics that I should also learn to gain further understanding I am all ears!","This is what got me started, if anyone knows who the actual author is please tell me because I'd like to credit them.

https://pfudke.wordpress.com/2014/11/20/hindley-milner-type-inference-a-practical-example-2/"
JavaScripth: a lispy JSON evaluator,p0ieec,2021-08-09 01:01:43,,"I wrote this yesterday because I thought the concept might be somewhat useful but I was wrong.

Here's the most interesting example:

    [
        {""def"": {""range"": {""fn"": [[""n""],
            {""if"": {
                ""cond"": {""eq"": [""n"", 0]},
                ""then"": [],
                ""else"": {""concat"": [[""n""], {""range"": [{""-"": [""n"", 1]}]}]}
            }}
        ]}}},
        {""def"": {""filter"": {""fn"": [[""ls"", ""f""],
            {""if"": {
                ""cond"": {""eq"": [""ls"", []]},
                ""then"": [],
                ""else"": {
                    ""if"": {
                        ""cond"": {""f"": [{""head"": ""ls""}]},
                        ""then"": {""concat"": [[{""head"": ""ls""}], {""filter"": [{""tail"": ""ls""}, ""f""]}]},
                        ""else"": {""filter"": [{""tail"": ""ls""}, ""f""]}
                    }
                }
            }}
        ]}}},
        {""def"": {""ls"": {""range"": [9]}}},
        {""def"": {""pred"": {""fn"": [[""n""], {
            ""or"": [
                {""eq"": [0, {""mod"": [""n"", 3]}]},
                {""eq"": [0, {""mod"": [""n"", 5]}]}
            ]
        }]}}},
        {""print"": {""+"": {""filter"": [""ls"", ""pred""]}}}
    ]


It seems the recursion limit is super low since {""range"": [1000]} overflows the stack, so all in all this language is even less useful than my previous meme language, RustScript.

It would probably work reasonably OK if I used a stack VM instead of evaluating it treewalk, but probably the only good thing about this language is that its implementation is only about  a hundred lines and it's easier to understand than brainfuck at the very least"
"Implementing ""defer""",ls4h8j,2021-02-25 19:26:19,,"For anyone interested, there is a great design document about how this is implemented in Go. They end up using a bitmask to record which defers should be run for easy cases, and falling back to a linked list to handle things like deferring in a loop.  


[https://github.com/golang/proposal/blob/master/design/34481-opencoded-defers.md](https://github.com/golang/proposal/blob/master/design/34481-opencoded-defers.md)"
Making video series on compiler development. Any cool ideas?,jm8vln,2020-11-02 04:00:15,"Hi
Recently I made this series
Part1:(Setup) https://youtu.be/XefoXqYuipQ
Part2(Parser) : https://youtu.be/09fTc_n_58k
Part3(Abstract Syntax Tree): https://youtu.be/LIud_Ak5YiQ
Part4(Compiling to C): https://youtu.be/zQOqfU86GQc

By the end of this series I show how to build a minimal working compiler. Now I plan to extend this further in subsequent series that dive deeper into more advanced things. There are too many interesting topics and I'm not sure what other people would be interested in the most.

I can do more theoretical topics like compiling  languages to Turing machine instructions, compiling regular expressions, making typecheckers, adding SAT solvers for code verification, making string rewrite systems etc.

Or i can do more practical things like, generating assembly, performing AST optimisations and explaining differently optimisation techniques, building JIT compilers, emitting JVM bytecode, LLVM tutorials etc.

I was also thinking about taking ideas for weird esotheric languages that my viewers would send me.

Maybe you could give me some tips on what you would be interested in the most?

EDIT: I see this post got lots of attention from you :D
That's really great. I see most people are interested in low level technical details, although a few also mentioned typechecking. I think I will therefore split the series into two branches: one focusing on generating and optimizing assembly from scratch, without any backend and the other focusing on higher-level features, theory of programming languages, typecheckers and formal methods. But feel free to add even more comments with your ideas. It helps a lot! I will try to upload videos regularly every week or so.",I'll be interested in the implementation of assembler and linker.
Sneak peak: Park programming language,i9klv4,2020-08-14 19:58:26,"Hi All,

I wanted to share with you the programming language I am working on named 'Park'. It is combines some ideas on async IO, green threading and immutability.  I went trough quite some iterations of implementation over the years (its a hobby project that I worked on on and off).  

I spend most of my time on the implementation of the runtime VM. The input for the VM is a AST (read from file) which is converted to x64 machine code at runtime. This code is then executed directly (e.g. there is no bytecode).

The frontend is a compiler that takes a file in the current Javascript derived syntax and produces the AST that the runtime can execute.  This compiler is written in Park itself.

The runtime is written in C++. It contains a low pause (<1ms) concurrent garbage collector, module loader, JIT, fiber scheduler and the implementations of the builtin types.

More info can be found [on Github](https://github.com/toymachine/park-lang) including instructions on how to run the examples.

This Github project was specifically made to share the examples. The actual implementation is currently still private as I am still figuring out the best way to open source it.","I'd love to see the VM code getting added. In particular, I'm curious to see how you handle blocking operations that can't be made async (e.g. file IO), and the garbage collector implementation."
How Futhark implements bounds checking on the GPU,hqn4xj,2020-07-14 04:31:00,,Great read. Anyone here use or used Futhark? Curious what's it's being used for in the wild
What are examples of language syntaxes which do a GOOD job embedding (an)other language(s) in a host language?,h89sc6,2020-06-13 23:40:02,"I'm creating a code generation tool for my hobby game project.  It will take text files describing game data (levels, models, etc.) and produce .cpp, .hpp, and also an Sqlite database.  You could also call it a ""resource compiler"".  The C++ code generated would be for serialization and object relational mapping.

Disclaimer:  I realize there's existing tools that address (parts of) this problem.  I have my own opinions about how it should be done differently.  And like I said, it's a hobby project.

Rather than a templating language like ASP where the high level language is embedded as markup inside the target language, I'm planning on making the generator tool's language a real statically typed language and just give it a good code embedding and string interpolation syntax.

So maybe the generator language has an ML-like syntax, but in the middle of the ML-like source code there might be a block of SQL and then a block of C++ and maybe some tabular data in CSV or TSV format.  These blocks would be assigned to variables in the generator language, and there'd be some kind of escape and un-escape syntax.

Existing examples of what I'm talking about:

* asm blocks in C
* LINQ to SQL in C#
* user defined literals in C++
* Separate tactic and strategy languages in Coq
* Logic programming with miniKanren or core.logic

What are some additional examples of languages which do a good job of embedding (an)other language(s)?  What are some common pitfalls?

One thing I know not to do is use one single token to ambiguously mark both the start and end of a block (like single or double quotes for strings).","Haskell has [quasiquoters](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#template-haskell-quasi-quotation) for this: you define a quasiquoter for the language you want to embed (effectively just a parser) and then use it like:

&#x200B;

`sqlExpr = [sql| SELECT * FROM table LIMIT 10 |]`"
Is implementing a bytecode interpreter in an interpreted language (like python) worth it?,gyf1l6,2020-06-08 00:06:46,"I recently got into PLs and just finished coding a tree walk interpreter for a small language in python. As expected, it sucks perfomance wise ( 2 mins to run fibo(35) ) and i was wondering if switching to a Bytecode VM built in python would make any difference in terms of perfomance ? (doesn't need to be super fast, i just don't want it to be unbearably slow)

Should i just build it in C /C++ and not waste time/effort?","A rule of thumb is that for programs that don't do I/O, Python is 10x to 100x slower than C++.

So it depends what you mean by ""unbearably slow"".

Another way to think about it is that within a single computer there are 9 orders of magnitude (~1 Ghz, with the smallest operation taking a ~nanosecond; a really slow I/O operation can take 1 second).

So Python will eat up 1 or 2 of those orders of magnitude.  For some applications this is too slow; for others it's not."
"A list of over 3,000 computer languages. Now with first release year and category.",d8rl9v,2019-09-25 02:53:50,,Happy to see [gwion](https://github.com/fennecdjay/Gwion) here!
The Economics of Programming Languages,171gd5l,2023-10-07 00:09:47,,"TL;DR: Corporate languages have buckets of money and dev time because they're built by companies with ""land"" or ""real estate"" of some sort that they can hand out to whoever they want; some of them choose to invest this money into programming languages.

Independent languages have a few avenues to get the funding they need to try to keep pace, but at the end of the day, independent language developers need to spend 50% of their time writing code and 50% of their time “selling” the language, maintaining infrastructure, etc. So all Jeff Bezos needs to do is pay 2 or 3 people to work on a language to outpace you, steal your ideas, and turn a profit off your insights. And getting Jeff'd sucks.  
So how do independent languages keep pace? ¯\\\_(ツ)\_/¯"
"Charm 0.3.9 --- now with ""Hello world!""",11fv8cl,2023-03-02 14:25:03,"This is a momentous day for Charm, for the future of programming languages, nay, for humanity itself. Fourteen months since I laid down the first lines of code, it is now possible to write, in Charm, an app *which does nothing except print ""Hello world!"" on start-up and then turn itself off*. I don't know why y'all want to do this, but here at last is this exotic, entirely useless, and yet much-coveted feature.

    cmd 
    
    main :
        respond ""Hello world!""
        stop

I'm still testing and refining it, but it mostly works.

If your lang also has this advanced feature, please share the code for comparison. If you don't --- well, fourteen months' hard work and you too could be like me. Start with something that waves genially at a small continent. Work your way up.","I'm keeping my eye on Charm.  I'm no expert but it seems like a novel and interesting, (not to mention useful) approach.  Congrats on your milestone."
Announcing GDLisp,117l8o3,2023-02-21 05:34:21,,This is cool but I don't think that Lisp is going to take off.
"I'm looking for info on this weird ""choice"" concept I saw in a programming language",zpmlxz,2022-12-19 17:00:41,"I was on twitter another day when i came across a tweet linking to a pdf presentation about a new programming language they were developing. I can't remember who were *they* or the name of the language, sorry.

But the language had this feature i first thought was very cool, but i can't find any useful applications for it.

The feature was called ""choice"", it was a special kind of ""type"" where a variable could have multiple values. they used the pipe operator to create choices.

```
let a = 1 | 2; // i also don't remember the exact syntax of the language, so this is just pseudocode
```

and the special thing about a choice, is that it behaved like a single value but did the computation for every one of it's values.
so say you have a function `sqr`, which squares an integer, then
```
let a = 3 | 4;
let b = sqr(a);
// then `b` is the choice 9 | 16
```

maybe with some operations for manipulating these choices, like filtering or collecting, they could be really useful.

Does something like this exists? Is somewhere aware of the langauge i mentioned or knows a language with a feature similar to this?

and also, is it useful? how would it be used to improve an algorithm or something like that?",This sounds like a recent presentation I watched on the Verse programming language.
What parsing techniques do you use to support a good language server?,t4c8ms,2022-03-02 00:45:13,"I'm planning on implementing a number of small example languages while working through a textbook. Problem is, I'm a TypeScript developer by day, and I'm used to a whole lot of slick IDE features. The last time I did this I found playing with the toy languages frustrating and unenjoyable due to the lack of feedback on syntax errors. I'm willing to put in some extra work to make the editing experience nice, but I'm having trouble filling in some of the gaps. Here's what I know so far:

- For syntax highlighting in VSCode, I need to write a TextMate grammar. Generating this grammar from a context-free grammar definition is [an open research problem](https://www.cs.rug.nl/search/Teaching/DerivingSyntaxHighlightersFromContext-FreeGrammars), (although there is some [prior research](https://www.semanticscholar.org/paper/Approximating-Context-Free-Grammars-for-Parsing-and-Schmitz/e1e76431011747779bf1571b792339fb72594243) in this area). I plan to do this by hand, following the [VSCode tutorials](https://code.visualstudio.com/api/language-extensions/syntax-highlight-guide), but it sounds like it might be [harder than I expect](https://www.apeth.com/nonblog/stories/textmatebundle.html).
- For error highlighting, I need to write a language server that will communicate with VSCode over the language server protocol. VSCode has a [tutorial on this](https://code.visualstudio.com/api/language-extensions/language-server-extension-guide), but it doesn't cover the techniques for writing the parser itself. The [example code](https://github.com/microsoft/vscode-extension-samples/blob/main/lsp-sample/server/src/server.ts#L142-L144) (quite reasonably) uses a minimal regex as the example parser, in order to focus on the details of communication with the server. This is where I'm tripping up.

The situation I want to avoid is one which I've encountered in some hobby languages that I've tried, which is that any syntax error anywhere in the file causes the entire file to red squiggly. IMO, this is worse than nothing at all. TypeScript handles this problem very well; you can have multiple syntax errors in different places in the file, and each of them will report errors at a local scope. (I assume this has to do with balancing brackets, because unbalanced parenthesis seem like the easiest way to cause non-local syntax errors.) Problem is, at 9.5k lines of imperative code, trying to read [the TypeScript parser](https://github.com/microsoft/TypeScript/blob/main/src/compiler/parser.ts) hasn't made anything click for me.

This brings me to my main question: how would you write such a parser?

I've written parser combinators before, but none with error correction, and it's not clear to me that 1) ""error correction"" in the sense of [this paper](https://www.cs.tufts.edu/~nr/cs257/archive/doaitse-swierstra/combinator-parsing-tutorial.pdf) is actually what I want, or whether it's compatible with [more modern and efficient approaches](https://core.ac.uk/download/pdf/195281832.pdf) to combinator parsing. It seems to me like research on parser combinators is still somewhat exploratory; I can find a lot of papers on different techniques, but none which synthesize them into ""one library to rule them all"". I do not want to try to be the one to write such a library, (at the moment at least) were it even possible (at all, or for someone with my level of knowledge). I am also not opposed to using a parser generator, but I know very little about them. While I would prefer not to write a manual, imperative parser, I could do so if I had a clear pattern to follow which would ensure that I could get the error reporting that I want.

So here are my secondary questions: Have any of you written language servers with the level of error reporting that I seek? Do you know of tutorials, examples, or would you be willing to drop an explanation of your approach here? Do you know of tools to ease the creation of TextMate grammars, or parser combinator libraries/parser generators which give good error reporting?

This turned out to be a longer post than I intended, so thank you for reading. I very much appreciate any additional information.

EDIT: I forgot to mention that because I am in control of the language being parsed, I’m happy to limit the parser’s capabilities to context-free languages.","> This brings me to my main question: how would you write such a parser?

As far as I can tell, _good_ error reporting typically requires a hand-written parser (typically based on recursive descent). This is especially true if you want to _recover_ parsing after an error and continue to process the rest of the input. There are some parser generators, like Lemon (used by sqlite), which have a lot more functionality related to error reporting and recovery than others... but unless you write the parser yourself by hand, the best error reporting is still basically of the form ""unexpected token encountered, expected one of `[`, `expr`, or `(`"".

And all that is just for _syntax_ errors, and assuming your parser can be built as a basic LR parser. If you need to parse a highly contextual language (e.g. C++), it's hard. And if you want semantic errors (""this name isn't defined""), you need more than just a parser, you need a partial interpreter."
Know what your functions are doing? - Side effects in 12+ languages,sh1dsz,2022-01-31 21:35:09,,Nice comparison
Destroy All Values: Designing Deinitialization in Programming Languages,sbfcuy,2022-01-24 14:23:10,,"Great article! there's something I don't get tho. The article seems to be skirting around what seems like a reasonable and simple static solution to the whole problem using definite initialization

Why is this not an option?

    fn main() {
        let x: MyType;

        if cond() {
            x = MyType::new();
        }
        // Error: 'x' may not have been initialized by the time it is dropped.
        // Consider initializating it for all branches or declaring closer to
        // initialization.
    }

and

    fn main() {
        let x = MyType::new();

        if cond() {
            consume(x);
        }
        // Warning: 'x' is moved in one or more possible code-paths, so 'x' will
        // be de-initialized for all branches.
        // (Alternatively, issue an error asking the user to explicitly drop or move
        // 'x' for all codepaths).
    }

These seem like perfectly reasonable approaches to initialization. If dropping a value is a concern, then I don't think it's too much to ask that the user ensures that a value is either definitely `init` or definitely `uninit` by the end of the scope."
Green Vs. Brown Programming Languages - Earthly Blog,mw8937,2021-04-23 00:11:35,,Both Haskell and Scala were represented in the “dreaded” list as well as the “loved” list. New list for most polarizing??
What is the earliest occurrence of <> as the inequality operator?,jg0vp0,2020-10-22 22:41:35,"Some friends and I were being grumpy about OCaml having both `!=` and `<>`, which do subtly different things (but with the same type), and we started wondering when the `<>` notation was invented.  From what we can find, Algol-60 used `≠` and Algol-W used `¬=`, but from Pascal onwards, Wirthian languages used `<>`.  We assume that is where ML got its `<>` from.  [This Dartmouth BASIC manual from October of 1964](http://www.bitsavers.org/pdf/dartmouth/BASIC_Oct64.pdf) uses the `<>` operator, and that's the earliest we've been able to find.

Did the ML family of languages really inherit syntax from BASIC, or can someone find an even earlier source?","Pascal is considered to be one of the influences on the design of SML, so I would think that's where it comes from directly. Logo used <> in 1967, so Pascal might have got it from there or from Basic."
caramel: An Erlang backend to the OCaml compiler,j0ob9i,2020-09-27 17:31:36,,"I wish there was more discussion on this as I find this extremely interesting but don't know much about either ocaml or erlang.

Does this allow for any ocaml apps to be run on BEAM?"
Futhark 0.17.2 released,ivq19e,2020-09-19 18:20:07,,very cool
"Lambster, an online lambda calculus interpreter written in TypeScript",ifqsb5,2020-08-24 23:09:59,"Hey everyone!

This is a project I worked on during the start of the summer, and after some rewrites of the front end I think it's in a good enough state to share with people. The interpreter is written in TypeScript and is my first major PL project, so if you have constructive criticism/advice I'd appreciate it. If you want to play around with the interpreter, it's available to try on [lambster.dev](https://lambster.dev) or you can use the CLI by [installing it via npm](https://github.com/minchingtonak/lambster#installation). 

Please let me know what you think. Thanks!

Links:

Lambster: [https://lambster.dev](https://lambster.dev)

Interpreter source: [https://github.com/minchingtonak/lambster](https://github.com/minchingtonak/lambster)

Website source: [https://github.com/minchingtonak/lambster.dev](https://github.com/minchingtonak/lambster.dev)","This is nice! The website is cute, and I like that you can display reduction steps. My one critique is that you don't seem to have implemented substitution correctly. You don't avoid variable capture. For instance, consider the expression `\x. (\y.\x. y) x`. If we try to beta reduce under the lambda, at first glance it seems we should get `\x.\x.x`, and this is what your interpreter does. But if we do some renaming, it becomes clear this is wrong. `\x.(\y.\a.y) x` --> `\x.\a.x`. In the first case, we allowed `x` to be ""captured"" by the innermost binder, which can be solved by automatically renaming the bound variable, so it looks like the second example."
Unseemly: a typed macro language,eq26iu,2020-01-17 23:56:09,"Unseemly is a language with ML-like types and Scheme-like macros.

The unique thing that makes this practical is that macros have types, so only the surface syntax needs to be type-checked. Without type errors in generated code, macros should feel exactly like builtins. The goal is to enable the easy creation of an ecosystem of easily-implemented languages that share libraries (like Racket), but with a shared type system.

Unseemly is based on reasearch I did in grad school, and it's currently a super-barebones prototype, but I can finally demonstrate the core features, so I wanted to show it to you.

Features from the ML family

* Algebraic types (i.e., structs and (rich) enums)
* Typesafe destructuring with `match`
* Generic types (e.g. `List<T>`)
* Recursive types

Features from the Scheme family

* Syntax quasiquotation (`'[Expr | … ]'` quotes an expression, but inside that `,[ … ],` evaluates its contents and interpolates them)
* Pretty-printing respects macro invocations and quoted syntax (the pretty-printer is rather limited at the moment, though)
* Hygenic macros (all operations respect α-equivalence)
* Macro By Example (easily implement n-ary forms without writing boilerplate loops)

Unique features

* Typechecking under syntax quotation (so `'[Expr | (plus one ,[e1],)]'` is a type error if `e1` has the type `Expr<String>`)
* No type errors in generated code (if a macro invocation typechecks, the code it expands to doesn't need typechecking), modulo one unimplemented feature and probably a lot of bugs
* Extensible parsing and lexing (with the right macros, you can write real SQL or real regexes inline, not inside strings)

Other features

* Full-featured REPL, with persistent command history and line editing.
* ""Unseemly"" is a suitable name for a Scheme-like language (suggests nefariousness), and an ML-like language (contains ""ml"" as a substring).

Unseemly is almost perversely low-level (its aim is to be analogous to Racket's core language, not Racket itself). For example, to use recursive types, you have to manually use `fold` and `unfold`! Any normal language makes those implicit, but from Unseemly's point of view, that's what macros are for!

Here are some implementations of higher-level language features:

* [comments](https://github.com/paulstansifer/unseemly/blob/master/src/examples/comments.%E2%89%89)
* [the `if` expression](https://github.com/paulstansifer/unseemly/blob/master/src/examples/if_macro.%E2%89%89)
* [function definitions](https://github.com/paulstansifer/unseemly/blob/master/src/examples/build_a_language.%E2%89%89)
* [function pipes](https://github.com/paulstansifer/unseemly/blob/master/src/examples/function_pipe.%E2%89%89) (stolen from ~~minicaml~~ [gobba](https://github.com/0x0f0f0f/gobba#function-pipes-reverse-composition-and-composition) this morning)

I think these these macros are pretty short, given that they specify the syntax, semantics, and type behavior of the features they introduce!

Right now, Unseemly is *hard* to program in. And there are a bunch of missing features that *can't* be implemented as macros. Also, it's currently an interpreted language, since I haven't gotten around to writing an LLVM backend yet. And its syntax is ... weirder than intended, even though I intended it to be weird (I wanna revise that, too). But I think it's not too far away from being a fun tool for rapidly prototyping new experimental language features.

Links:

* [more detailed pitch](http://unseemly.github.io/)
* [language documentation](https://github.com/paulstansifer/unseemly/blob/master/core_language_basics.md)
* [source repository](https://github.com/paulstansifer/unseemly)
* [issue tracker](https://github.com/paulstansifer/unseemly/issues)","This is super interesting! Great to see more people exploring the ML-like + macros space.

I was browsing through some of the examples and one thing that jumps out to me is that having a syntax highlighter might be next to impossible unless it also embeds an interpreter... is that right? Maybe I missed something.

IIRC Rust kinda' works around that by enforcing balanced brackets for macro arguments, allowing the highlighter to potentially skip the macro's arguments if it can't recognize the tokens."
Formality Language,eeml4i,2019-12-24 00:15:39,,"Sorry if this is a stupid question but what are ""affine"" lambdas?"
Type inference as constraint solving: how GHC's type inference engine actually works - Simon Peyton Jones,cukxxp,2019-08-24 06:24:38,,"Abstract & slides: https://zfoh.ch/zurihac2019/#speaker-simon-peyton-jones, https://drive.google.com/file/d/1NRkP0hz-0Yo49Rto70b2nUwxjPiGD9Ci/view"
When pigs fly: optimising bytecode interpreters,asmow8,2019-02-20 18:13:05,,Very nice. Good information here and you have a pleasant writing style.
"Transpiler, a meaningless word",15rxlp0,2023-08-16 00:35:39,,"I don't think the author showed that transpiler is a meaningless word. But I am also not sure that they are attempting to argue that either. 

My hot take, all transpilers *are* compilers. Transpilers target languages that compilers consume."
What features have you seen in a PL that helped encourage code re-use?,123sn1i,2023-03-28 00:41:43,"I think we have all heard ""Write DRY code"" but I was wondering there were any features you saw in a language that really made you think ""This makes it easy to write DRY code"".

In my opinion, generics appear to be the best method for code re-use. A method may be written once, and then used on any type, which allows for a lot of re-use. Are their good alternatives to having generics? I know that some languages allow you to further refine the class of types that can be shoved into a generic, allowing you to use the abilities of those types. Do you find this ability useful?

People seem to love rust's macro system, and I wonder what other tools exist for meta-programming other than macros. What are some gripes you have with macro systems? Do they reduce how much code is duplicated?

What have I not mentioned that you think makes for a language which really encourages code re-use? If you have an example of a PL which encourages code re-use in a interesting way please let me know, I would love to read about it.","In no particular order:

* modules
* functions as first class language constructs
* mixins (or type classes in functional languages)
* type parameterized types (aka generics) and functions"
Alternative looping mechanisms besides recursion and iteration,1180ewy,2023-02-21 18:35:06,"One of the requirements for Turing Completeness is the ability to *loop*. Two forms of loop are the de facto standard: recursion and iteration (*for*, *while*, *do-while* constructs etc). Every programmer knows and understand them and most languages offer them.

Other mechanisms to loop exist though. These are some I know or that others suggested (including the folks on Discord. Hi guys!):

* *goto*/jumps, usually offered by lower level programming languages (including C, where its use is discouraged).
* The Turing machine can change state and move the tape's head left and right to achieve loops and many esoteric languages use similar approaches.
* Logic/constraint/linear programming, where the loops are performed by the language's runtime in order to satisfy and solve the program's rules/clauses/constraints.
* String rewriting systems (and similar ones, like graph rewriting) let you define rules to transform the input and the runtime applies these to each output as long as it matches a pattern.
* Array Languages use yet another approach, which I've seen described as ""project stuff up to higher dimensions and reduce down as needed"". I don't quite understand how this works though.

Of course all these ways to *loop* are equivalent from the point of view of computability (that's what the Turing Completeness is all about): any can be used to implement all the others.

Nonetheless, my *way of thinking* is affected by the looping mechanism I know and use, and every paradigm is a better fit to reason about certain problems and a worse fit for others. Because of these reaasons I feel intrigued by the different loop mechanisms and am wondering:

1. Why are iteration and recursion the de facto standard while all the other approaches are niche at most?
2. Do you guys know any other looping mechanism that feel particularly fun, interesting and worth learning/practicing/experiencing for the sake of fun and expanding your programming reasoning skills?",don't forget [COMEFROM](https://en.wikipedia.org/wiki/COMEFROM)
Basic building blocks of dependent type theory,10f1fr0,2023-01-18 15:36:21,,"I have not ""seen the light"" yet wrt dependent types. I'm open, btw, I'm not just dismissing it. But I have a philosophical issue with them, and that's hard to overcome.

It starts with: what is a type? For me, a type is a logical predicate that can be made about a term, i.e. source code. I'm aware that type judgements are considered different from set membership and thus different from predicates, but conceptually they are very similar, if not the same. Any term can be written, but only type-valid terms can be judged as having valid types.

Ok, so types, in essence, are the application of logic to terms. This is where a recursive relationship begins, because terms describe logic themselves. This is the difference between static and dynamic behavior - operations on source code, vs. the actual running of the program. Those seem like fundamentally different things, which is why dependent types are so tricky and the logics have to be very constrained to make dependent type checking possible even in the first place.

Basically, I don't see this elegance that is always touted as a benefit of dependent types. It seems to me like a way to have your cake and eat it to, by combining runtime and type-checking time.

I also think the Curry-Howard correspondence is way oversold. Just because types are propositions, doesn't mean all propositions should be types. i.e, if you really want, you can write proofs about propositions about your code independent of the code itself.

[This is the same realization that Xaxier Leroy, creator of OCaml and the formally verified C compiler CompCert, came to](https://www.cs.ox.ac.uk/ralf.hinze/WG2.8/26/slides/xavier.pdf), where he said:

> My practical experience... Dependent types work great to automatically propagate invariants attached to data structures (standard); In conjunction with monads (new!). In most other cases, plain functions + separate theorems about them are generally more convenient.

These are my thoughts about dependent types. Basically, I get that they're interesting, but I don't see them as the solution to all of our problems."
"Stephanie Weirich - How to Implement the Lambda Calculus, Quickly",yuoqll,2022-11-14 12:25:00,,I generally avoid watching videos. Is there a text transcript somewhere? A blog post / paper with similar content available?
"Compiling Swift generics, Part I",y6h8y8,2022-10-18 01:34:44,,"I've been thinking about generics lately, and this is a very nice read. Will need to weigh pros and cons of this vs the C++ approach."
Useful lesser-used languages?,xma1jp,2022-09-24 05:38:07,"What’s one language that isn’t talked about that much but that you might recommend to people (particularly noobs) to learn for its usefulness in some specialized but common area, or for its elegance, or just for its fun factor?","Prolog, a declarative language for logic. It's so weird (in a good way), sometimes it doesn't even feel like real programming, you just giving the problem statement and say ""now solve it"" – and it does.  
If you have any kind of abstract logic problem, or something involving heavy recursion, it's definitely worth to check out."
How good is LLVM in other languages other than C++? (In my case I'm interested in using Rust),v3yugp,2022-06-03 20:58:31,"Basically this as the title says. I would actually want to program my language in Rust, but I'm little bit nervous about that, because using LLVM could become quite complex or maybe there would not be all features, compared to if I used C++ directly.

What are your experiences of using LLVM with languages other than C++?","I'm currently using the [Inkwell bindings](https://github.com/TheDan64/inkwell) for Rust, which I've found actually pretty nice. In terms of generating LLVM IR, the C bindings (which is what Inkwell uses internally) can do anything you want them to (definitely not limited to trivial languages as someone else here said.) I'm even using the LLVM garbage collection infrastructure, with no problems (well, no problems in generating it; the LLVM GC infrastructure works pretty well but is sparsely documented, so actually writing a GC is fairly difficult, but it's doable). The C bindings are actually more stable than the C++ bindings (!), although not quite as stable as the textual IR format; but without the bindings you would have to write code to generate the IR yourself, the compiler would be slower as it must be emitted as text and then reparsed in a different process, and you would have less control over optimization.

The documentation on Inkwell is pretty good, but for a lot of instructions it's best to just consult the [LLVM Language Reference Manual](https://llvm.org/docs/LangRef.html), which is definitely the best piece of LLVM documentation that exists anywhere. Functions on `Builder` for example generally just generate one of these instructions.

I also have a little bit of C++, but that's only for custom optimization passes which are not really needed in most cases unless you have unusual language features and want the maximum performance out of them, and even then mixing the C++ optimization passes with the rest of the compiler which uses the Rust bindings was pretty easy.

Overall I would definitely recommend Inkwell. There will be a bit of a learning curve with LLVM no matter what, but Inkwell works well and you should be able to generate LLVM code just as well as any other method, and you don't have to figure out how to do that yourself, plus you get a bit more type safety than the C++ or C bindings (Inkwell tries to add some type safety on top of the raw LLVM API.)"
"Announcing Neptune: a fast, concurrent and embeddable scripting language",ukuvwt,2022-05-08 13:24:54,"Neptune is an embeddable language that makes writing concurrent code very easy. The interpreter is  fast and minimal. 

Repository: [https://github.com/Srinivasa314/neptune-lang](https://github.com/Srinivasa314/neptune-lang)

Book: [https://srinivasa314.github.io/neptune-lang/](https://srinivasa314.github.io/neptune-lang/)

[crates.io](https://crates.io): [https://crates.io/crates/neptune-lang](https://crates.io/crates/neptune-lang)

VM design: [https://srinivasa314.github.io/neptune-lang/internals.html](https://srinivasa314.github.io/neptune-lang/internals.html)",I posted a month back but I realised it got removed as spam
"A ""lawful"" framework for styling/formatting UIs?",ugb7e8,2022-05-02 06:34:58,"Recently at work I was stuck with a problem (in Android) where a view would be correctly formatted initially, yet upon resizing (a parent ViewGroup was ""hidden"" by having it's width shrunk to zero) and bringing it back again -- the layout got all messed up.

It turned out I was using weights wrong -- but this got me thinking: What if someone made a framework for the styling and layout of UI components that was based on simple, mathematical foundations -- much like what Pijul purports to do for version control. For instance, in this imagined layout language, you'd have guarantees like ""If you resize a component, then immediately resize it back to it's original size, the layout will remain the same"".

I know this is not the usual material for this sub, but I still think it's relevant (layout/UI languages are still languages!) -- and I figured that other language designers may have had similar thoughts, and perhaps tried to build something like this.","Neat concept! This is definitely a PL topic, and overlaps a bit with formal methods topics (which in turn are generally placed under the PL umbrella.) Especially, what are useful guarantees to make for different components of a UI, and how do they interact with one another?

Like, say there's a panel that guarantees ""I will be at least as wide as my widest contents."" So far, so good. Say we put a form in it with dynamic text entry. Should it resize dynamically if you enter too much text? That sort of thing.

With some design choices, there could be a high degree of static analysis that would give the programmer useful stats about the overall behavior. Others would tightly specify dynamic behavior."
Programming language designers unite!,ttrkf1,2022-04-01 22:07:45,,"... At the SUS Foundation, we believe the ""object oriented"" vs ""functional"" debate is a bike shed, so we developed a brand new paradigm to handle dynamic dispatch.  We call it Reality Oriented Programming, and it leverages some of the exciting work coming out of the Ethereum smart contract space.  At runtime, SUS determines what code to run by calling out to trusted 3rd party ""Oracle Factories"", which respond with what your business logic should be.  In this way, SUS programs can adapt to changing realities without modifying source..."
Odin's Declaration Syntax Compared With C,t4xp0y,2022-03-02 18:58:31,,"Any declaration syntax that is not C's shines in comparison to it :)

Btw, I was convinced the omit-the-type-to-infer-it syntax appeared first in Sather, where you can do

    i : INT := 1;

or

    i ::= 1;

but now I wonder. Maybe there was some academic cross-pollination there, both languages were created roughly at the same time (Sather a couple of years earlier, but I don't know if it had that syntax from the start). Just a trivial issue :)."
Wirth vs Unix philosophy?,t1ecb7,2022-02-26 04:57:59,"
It’s known that Plan 9 and Go have taken inspiration from Niklaus Wirth’s Oberon. I’m curious as to how Wirth’s programming/computing philosophy stacks up against Unix philosophy? How are they similar and how are they different?","Not really an expert here, but just some loose thoughts: they're broadly similar in that they both highly value ""simplicity"", for some definition. Or perhaps ""minimalism"".

I'd say there's two big ways they differ:

* The Unix philosophy is very pragmatic; it's more of a ""hacker"" mindset. Wirth's approach is generally academic and perhaps more focused on correctness, at the expense of ease and brevity. Compare how easy to parse Pascal is with how difficult to parse C is. 

* Unix is really, really into text, and is happy to chop it in quick-and-dirty ways to get at whatever data is needed. Wirth generally likes rich, structured, strongly- and statically- typed data. (Ab)use of regexes or tools like `sed` to parse data is very Unix-y but not very Wirthian I don't think; not to speak for him, but while I imagine he's happy to use regexen to define, say, tokenization rules, I think he'd much more quickly reach for a BNF grammar and recursive decent parser for lots of things that Unix users wouldn't."
I made a system to use any language feature in any C-like language (e.g. Python),rxyldk,2022-01-07 12:27:52,[https://github.com/HouQiming/ama/](https://github.com/HouQiming/ama/),"Looks pretty interesting, I'll give it some attention this afternoon"
Are there languages which compile a single source text into a 'cluster' of programs?,rmui81,2021-12-23 20:18:44,"This is something I've been pondering for a little while - I wonder if any languages exist that support this, and indeed whether it's actually a viable concept.

Right now I work in a 'serverless' architecture, where our web application layer is built up of distributed functions executed in a cloud provider (AWS Lambda). In serverless applications, a process tends to be split over several small, asynchronously executed, separate programs.

As a contrived example: imagine that you had a function to validate an incoming HTTP message; a function to store it in a queue; a function to deduplicate the queue; a function to process the queue every night and send widgets to the customer.

Now, with AWS lambda at present, I *could* write this as a single program. I would compile one program that responds to each event along that lifecycle. I could use something like AWS Step Functions to describe the graph of execution, store that graph with my source code, and then I'd have a single place to describe the flow of the entire process.

But what would be interesting (to me) is if a programming language could take a single set of source files, or even one source file, and using an execution graph, output several compiled artefacts that were type-level guaranteed to work together. That could keep my artefacts really small, and let me do things like incremental releases (only updating the changed components).

If your compilation target was an intermediary language like JavaScript, you could have a lot of fun with this, I think. You could have a program that does some IO and can compile to a single Node server app that uses \`async / await\`. However, with hints about concurrency and execution, your compiler could output to separate lambdas speaking over events; or a piece of code that runs on the browser and server speaking over sockets - etc. etc.

I've never used Erlang or any BEAM languages but it sounds like this is close to that area of research - being able to model a program as a set of actors and then use that to be very opportunistic about deployments and artefacts.

Do languages / compilers like this exist?","That’s my research area! Folks have used that idea in many different fields:

Choreographic programming (check out Choral, Chor, AIOCJ) comes from multiparty session types.

Macroprogramming (Kairos, Pleiades, many more) comes from cyberphysical systems.

Multitier programming (ScalaLoci, Links, many more) comes from trying to solve “impedance mismatch” in web architecture.

As joonazan mentioned, Unison is a really interesting new approach. Unlike the others, it hasn’t come out of academia.

I don’t think anyone has settled on an umbrella term for this decades-old concept. A recent paper called them “multiparty languages”, which might catch on."
Composing UNIX with Effect Handlers: A Case Study in Effect Handler Oriented Programming,pknkcq,2021-09-09 09:14:41,,"This was a really neat presentation showing how you can construct a toy version of UNIX (with IO, time sharing, interrupts, and session management) just by composing effect handlers.

- [Extended Abstract](https://dhil.net/research/papers/unix-ml2021.pdf)
- [Source Code](https://github.com/dhil/phd-dissertation/tree/master/code)"
Planarly: a new kind of spreadsheet,npqb1r,2021-06-01 17:53:05,"For the past over one year, we've been working on a ground-up rethinking of the classic spreadsheet. We're happy to finally show you Planarly [https://www.planarly.com/](https://www.planarly.com/) in a technical preview, where code duplication is replaced by array formulas, tables are looped over in \*table comprehensions\*, cells can be referenced using absolute, relative, content- and structure-related methods, and many more! It's probably best thought-of as a 2D visual language masquerading as a spreadsheet.

Best tried in Chrome Incognito mode as we have yet to officially support other browsers. The whole ""backend"" is compiled to wasm and executes entirely in your browser. A completely offline application is in the road map :)

Edit: you can now go directly to a comprehensive demo at https://demo.planarly.com/?file=/public/everything.plan . Best viewed in Chrome.","Very fresh idea!

But you really need to fix the contrast of the UI - it's very hard to read anything, even if I turn up the brightness to full."
Separation Logic Foundations,nlglz6,2021-05-26 21:07:52,,A new volume of Software Foundations just dropped! This time it's about verifying imperative programs using separation logic!
Suggestions for a functional language for videogames,musrlj,2021-04-20 23:23:15,"I want to write a language for writing videogames.

I do not enjoy using object-oriented languages, and the only other paradigm I know well enough is functional programming, so I would aim to something functional-y.

* I want algebraic data types and static type checking.

* I would like to keep things minimal and explicit (so probably no typeclasses), a bit more Elm than Haskell.

* Something very important would be able to prototype stuff quickly, but maybe this clashes with having static type checking?

* It should probably be able to implement a very efficient entity-component-system engine, so it should have features that allowed to implement that.

* And maybe offer some meta-programming capability to generate serializers and deserializers, maybe macros or maybe something like Template Haskell?

Any ideas or suggestions?
What specific features would be necessary to implement the above?

Thanks!","> Something very important would be able to prototype stuff quickly, but maybe this clashes with having static type checking?

I don't really buy the argument that types make prototyping slower, in fact I think the opposite. But, just in case types _do_ make prototyping slower, the gods, in their infinite wisdom, gave us `-fdefer-type-errors`.

> It should probably be able to implement a very efficient entity-component-system engine, so it should have features that allowed to implement that.

Isn't this just a matter of having the right framework. E.g. Consider how Elm Architecture is meant for application UI. You might try to come up with something similar, but more geared towards games.

    type alias Scene msg = { ambience : Audio
                           , backgrounds : List (Scrollrate, Image)
                           , ui : List (Html msg)
                           , sprites : List (Sprite msg)
                           , triangles : List (Triangle msg)
                           , viewport : { cameraLocation : (Float, Float, Float)
                                        , cameraDirection : (Float, Float, Float)
                                        , cameraRotation : (Float, Float)
                                        , viewAngle : (Float, Float)
                                        }
                           }

    type alias State model = { controllers : List ControllerState
                             , clock : Timestamp
                             , model : model
                             }

    type alias Game msg model = { init : model
                                , step : msg ->
                                         State model ->
                                         Cmd model
                                , render : State model ->
                                           Cmd (Scene msg)
                                }

> And maybe offer some meta-programming capability to generate serializers and deserializers, maybe macros or maybe something like Template Haskell?

You want Template Haskell but you _don't_ want type classes? You need to think _very carefully_ about this, as most of the things we traditionally used Template Haskell for are replaced by (and generalized by, and generally cleaned up by) generic deriving and deriving via."
I want to design and build a programming language specifically for competitive programming!,l9m4cw,2021-02-01 05:11:20,"Hi r/ProgrammingLanguages. This is my first time visiting this subreddit!

I'm a long time competitive programmer, and recently I'm training again for competitions with my team. I couldn't help but notice that although most of us competitive programmers use C++, but we usually change it up a ton with macros and functions to make it suit competitive programming. So we should be able to write code quickly where minutes can be very important for you final result.

Check this code out for instance: [https://codeforces.com/contest/1477/submission/105770759](https://codeforces.com/contest/1477/submission/105770759) Written by one of the highest rated coders on codeforces platform in ""C++"". You'll find stuff like:

    #define tcT template<class T
    #define tcTU tcT, class U
    // ^ lol this makes everything look weird but I'll try it
    tcT> using V = vector<T>; 

or 

    #define FOR(i,a,b) for (int i = (a); i < (b); ++i)
    #define F0R(i,a) FOR(i,0,a)

(yes the second one is F0R (F-Zero-R)!)

Why are we defining these? well because it's much faster to type vii instead of vector<pair<int, int>> and much faster to type FOR(i, n) instead of for (int i = 0; i < n; ++i).

This is not the only problem, a lot of boilerplate and lack of algorithms in the standard library of C++ forces competitive programmers to make non-standard algorithm libraries of their own and copy paste it into their source code to submit.

That's why I had the idea of designing a programming language specifically for competitive programming that then gets transpiled to C++ (so one can submit it in most online judge websites).

I don't have much experience with designing or implementing programming languages, I once made a toy extremely simple programming language as a fun project but never anything more than that, but I want to give this project a go.

If you have any advice on how do I go forward with this project, please consider sharing! All resources, etc. are welcome.

I'm looking for more people to work with me on this open source still unnamed project. So if you're interested in brainstorming about the syntax design and name and features and being a part of the team, please inform me!

Cheers!","Interesting! I had and idea for a language that was effiicent to type, but i noticed that the google competition only alloweda certainn subset of langs, how're you getting around that/

I figured if i could compile to C, most competitions would.... 'allow' that? UNsure what the 'spirit' of the rules is here

If you could create a graphical editor for a python-esque lang I'd figure that'd be the most efficient solution, since you could get crazy autocompletion

On the other hand, why not jsut make a template file which contains `#define tcT template<class T` at the top?

Also, why code in c++? Are there really some challenges hwich p ython is too slow for? I figured if they allow python as an entry lang they'd allow idiomatic python solutions, which are probably super slow"
The visitor pattern is essentially the same thing as Church encoding,kqh9ui,2021-01-05 04:34:21,,"One of the major flaws in GoF is that all of the examples of *Visitor* are of recursive data structures, resulting in a red herring that obscures the true purpose of the pattern and causes untold numbers of practitioners to mistakenly believe *Visitor* is about iteration. I'm not even totally convinced that the GoF authors understand the purpose of *Visitor*.

*Visitor* is simply how you do pattern matching in languages that don't have built-in pattern matching."
λvc: Variadic Auto-Curried Lambda Calculus,k3678r,2020-11-29 16:48:29,,"I thought about this exact issue before but found the ambiguity introduced into the syntax would not be worth it.

But I really like your solution!"
Geometry Types for Graphics Programming,j9ucp2,2020-10-13 00:41:00,,This is cool. I have nothing substantial to say. Cheers
"How to Create Your Own Programming Language - Part 1 - Paradigm, Roadmap & Tools",j0qf6n,2020-09-27 20:39:00,,"Feedback: from a user's point of view, the font is not good (hard to read). Something plain and sans-serif would be better, especially once we get into actual coding.

EDIT: Massive props on including subtitles (CC), though!"
neut - a dependently-typed programming language with compile-time malloc/free determination,gprche,2020-05-24 23:16:40,,"I posted some criticism of the implementation approach, or rather of the unreasonable things people wildly assume from the implementation approch, over at r/haskell.

> As hinted by u/ineffective_topos (in a kinder / less direct way), this is a very inefficient implementation of reference counting. Said another way: one way to *optimize* this implementation is to keep a ""reference count"" field on each object, initialized at 1 when the object is created. Whenever we want to copy an object, increment this field instead (you saved a copy!). Whenever we want to discard an object, decrement this field instead (you saved a discard!) unless the field is zero, in which card you do discard.
>
> I think this is a nice project: clearly the author is learning a lot, and we need more people comfortable writing realistic implementations of dependently-typed languages. It is reusing old ideas (linearity-based value management, type-passing translations, etc.) and presenting them to new audiences. Great!"
What modern syntax sugar do languages really need?,exx5bl,2020-02-03 07:08:17,"As the title says, regarding general programming imperative languages, what syntax sugars do languages need and don't have or if you have any good ideas for new syntax sugar, that's also great.",Replacing nulls with Maybe or Options.
You Can Now Try the Oil Language,ddhmj9,2019-10-05 10:54:21,,Thoughts on nushell?
[PDF] Graydon Hoare - ‏ 21compilers and 3 orders of magnitude in 60 minutes,b695dw,2019-03-28 04:16:43,,"Interesting selection of dinosaurs. Key points that stood out for me...

* After 60+ years of work, optimizations seem to only yield about 3-5x performance improvement. Frances Allen got all the good ones 1971: ""A Catalogue of Optimizing Transformations"". The ~8 passes to write if you're going to bother:  **Inline, Unroll (& Vectorize), CSE, DCE, Code Motion, Constant Fold, Peephole**. Many compilers just do those and get ~80% best-case performance.""

* *Front-end* code for Rust, Clang, Swift: 300-800 kloc. V8 (total) 660kloc. Even GHC at 180kloc. vs. Turbo Pascal @ 14kloc and 8cc @ 6.7kloc. (My half-finished (?) Cone front-end is about 8kloc)"
What's the deal with llvm?,14ra832,2023-07-05 21:09:25,"I'm building a language with a whole lot of high level features and I don't see a problem with llvm. Sure, it can sometimes be annoying and it could get slow with huge programs but most people seem to be very negative towards it and I honestly don't understand why.","This [zig issue](https://github.com/ziglang/zig/issues/16270#issuecomment-1616115039) has a lot of commentary on LLVM. The gist of the proposal is to move away from LLVM, but still keep a separately maintained backend for those who need it. 

Among other things, he specifically mentions the headache of dealing with various package managers.

>...at the mercy of Homebrew's broken LLVM installation, Archlinux's out of date LLVM installation, or Debian's LLVM installation that renamed FreeBSD to kFreeBSD in the LLVMTargetOS enum for no reason.

The original comment touches on other problems and expected benefits. I don't have a dog in this fight, but I found some of the discussion insightful."
Demystifying Pratt parsers,14piaj7,2023-07-03 20:56:58,,Great article. Always finded little to no info about pratt parsers. This could help me finally write one!
ScrapScript: In-Development Language with Some Cool Features (Not Mine),131p8jw,2023-04-28 19:41:15,,Author here! Let me know if you have any questions :)
Vortex: Optional Function Typing + SDL library update,130ezb6,2023-04-27 17:10:29,,"Vortex Update: I've now implemented optional function typing, and have typed all of my SDL library functions. This is still a dynamic language (for now) and so type checking happens at runtime. However, introducing this into the language now has made it easier to implement a static type checker down the line.

Now we have the ability to do this:

    const addNums = (a: Number, b: Number): Number => a + b
    
    addNums(10, 20) // A-Okay 
    addNums(10, ""hi"") // Type Error

Here's the updated documentation: [Optional Typing](https://dibs.gitbook.io/vortex-docs/reference/language-reference/optional-typing)

Vortex Source Code: [Vortex on Github](https://github.com/dibsonthis/Vortex)"
Is continuation passing style conversion still used as an intermediate language?,12icyia,2023-04-11 16:14:06,"Hello,

I am curious if cps is still used as an intermediate language? I’d like to read “compiling with continuations”. 

What are some better IRs now if any?

Cps conversion is for functional languages for the most part.

On the llvm YouTube channel there was a talk about using ssa for functional languages. Here’s the video https://m.youtube.com/watch?v=cyMQbZ0B84Q&t=466s

Is ssa the future for functional languages?
Anyways any tips will be appreciated. Thanks","I definitely see papers from time to time using a CPS form as an intermediary, but it's always hard to guess the reality from academic papers. I'd ask u/mttd, since he works on compiler back-ends full time, and seems to read every academic paper on the topic of compilers."
The bruijn programming language,12fkfij,2023-04-08 20:25:28,,"You could make this dramatically faster if you instantiated your environment as a LC expression and then evaluated that; you'd only have to use a stack at runtime.

How would you implement this? Keep track of the number of bindings created and the ordinal of each binding; by subtracting the two and adding the depth of the current expression, you get a reference to that binding."
Why are some languages explicit about recursivity?,119oyym,2023-02-23 12:57:32,"I see that a lot of functional languages differentiate between non-recursive (`let`) and recursive (`let rec`) bindings, with the ""default"" binding being the latter.

Why is that? Is it a choice, like how some languages are explicit with mutability (`let` / `let mut`); is it necessary, or does it bring benefits?","With letrec and eager evaluation, trying to force mutually recursive definitions which depend on each other directly like `letrec x = y, y = x in ...` can go wrong, whereas a `let` cannot go wrong like that. While a bit naughty, non-recursive let also allows shadowing like `let x = x in ...` and `letrec` doesn't.

There's a similar issue for slots/fields/whatever you like to call them of objects in initialization; recursively defined fields e.g. `x = y, y = 42` can read various unpleasant not-quite-initialised things like zeros or nulls. (For Utena these problems are one and the same; bindings are established with nested objects.)"
Give Me Flexibility and Give Me Types,119ifzi,2023-02-23 08:03:40,,I feel that when people model games they really feel the need to model IsA relationships since they are thinking of real-life objects. but the examples in the post would have better been written with HasA relationships and no inheritance
"Most important language features not touched in the book ""Crafting Interpreters""?",ztmadc,2022-12-24 00:56:17,"I just got done reading Crafting Interpreters and writing both Lox implementations (I did a few challenges but not all). Now I want to write a bytecode compiler for a language I'll design myself to get a bit more experience. So naturally, I'm wondering what the most important features would be that weren't touched at all in the book (so that I have something new I can learn). Any suggestions?",The first ones that comes to mind are static types and a module system.
"""Stop Writing Dead Programs"" by Jack Rusher (Strange Loop 2022)",y7tt4x,2022-10-19 13:22:46,,"Literally just watched this Sunday.

It is interesting but so far from my daily work."
Psychedelic Programming Languages,xcd5wr,2022-09-12 21:41:14,,"> fuck this fuck that I know better, fuck you

And then maybe you'll have a final trip and realize ""wow, I had everything wrong... I know nothing..."""
What problems are currently the most researched ones and what problems is your language trying to solve?,vh7w1o,2022-06-21 15:49:40,"Hey everyone, I hope this question is not off-topic here.

I'll give a presentation at the company I work at in the near future about PL research. Basically, I want to give them an overview of what's current and what problems are tried to be solved. My company doesn't do any PL work, we just develop ordinary software (mainly in C# and C++), so I want to focus on general purpose PLs for now.

I thought this sub is perfect for my questions, so here they are:

1. What are the most important problems that the field of PL research tries to solve (at the moment)? Memory and concurrency safety comes to mind.
2. What are the most important problems to you?
3. What are the most important problems your language is trying to solve?

I'm thankful for any direct answer or links to external resources.","1. The PL research is studying algebraic effects, how to reduce the difficulty of dependent types, how to achieve memory safety and concurrency safety with something easier than the borrow checker.
2. I guess the main problem for me is to create an efficient compile-time memory management that doesn't force the programmer to use lifetime annotations.
3. My language tries to apply functional programming to system development, being a pure functional language with rust-level abstraction."
Trying to do errors right,vd86ba,2022-06-16 07:47:26,"I've been refactoring my error messages for the nth time, and I'm about happy now.

This isn't just bikeshedding. Reading error messages is a large part of programming, and so they ought to be readable, perhaps even useful. The practical usability of many poorly-designed pieces of software is limited by their UI and their documentation, and error messages are an important part of both. (Also, bicycles are important and people need a place to keep them.)

And yet this is one of those areas where people (many of whom should know better) often go for the low, low bar of ""it works"". They say to themselves: ""We were required to notify the user of an error condition, and we did it. Job done!"" Perhaps adding ""After all, if they know where the parser choked and what it choked on, they know as much as the parser does and as we do."" (But they don't! Because the designers know the language better than the users!)

It's not just the PHPs out there that are like this, the products of amateurs and chaos. Let's ignore the low-hanging fruit. Big Tech does it too. I spent some long seconds the other day puzzling over an error message of the form:

""unexpected validVariableName at end of statement""

before I realized that if you type ""retun validVariableName"", then the Go parser will object to the valid variable name and not the totally unknown word ""retun"" that precedes it. How was that the most natural thing for the parser to do? And could it not with a minimal effort have done something more sensible?

The people at Google spent millions on Go so that they could use it themselves to be productive and efficient. By which standard, it's pretty good. But they barely bothered with the error messages. To take another example, I feel like they could have gone the extra mile with what happens when you miss off a }. Surely something better could have been done? Instead they've basically just *trained their users* to realize that if they see an error that looks like ""syntax error: unexpected string, expecting comma or ) \[750,42\]"" then this almost certainly means ""you've left off a right brace … somewhere … before line 750 but good luck finding it Sunny Jim, you're on your own."" And after a while I guess everyone who was dogfooding the language stopped noticing that the dogfood tasted funny and just assumed that that's how dogfood tastes.

So, here's what I've been up to. As Charm is a REPL language and a backend language I'm trying to make as many tools as possible available in the REPL, so this is a particularly Charm-ing way of doing errors nicely.

So, let's run a Charm script that's I've carefully riddled with syntax errors. I've tried to make the syntax error system have as few knock-on effects as possible, but there's one in here and we can use the REPL to get to the bottom of it: (ETA: I see Reddit has messed up the semigraphics, did they not use an actual monotype font?)

ETA 2: Just to clarify, this is the user working interactively with the REPL: where you see the  → that's user input.

    tihardc@88665a155203 Charm % ./charm run src/errordemo.ch
    
      ╔═════════♥═════════╗
      ║ Charm version 0.1 ║
      ╚═════════♥═════════╝
    
    Starting script 'src/errordemo.ch' as service '#0'.
    
    [0] Error: attempted assignment in 'import' section at line 4:0 of
    'src/errordemo.ch'.
    [1] Error: declaration of function in 'var' section at line 10:0 of
    'src/errordemo.ch'.
    [2] Error: redeclaration of 'private' at line 15:0-7 of 'src/errordemo.ch'
    [3] Error: '(' unclosed by outdent at line 24:0 of 'src/errordemo.ch'.
    [4] Error: a line ending in ',' must be followed by a line beginning with '..' at
    line 26:0-3 of 'src/errordemo.ch'.
    [5] Error: inexplicable occurrence of 'else' at line 26:4-8 of
    'src/errordemo.ch'.
    [6] Error: if it occurs, 'import' must be the first headword at line 28:0-6 of
    'src/errordemo.ch'.
    
    → hub why 5                                                                                                                                  
    
    Error: inexplicable occurrence of 'else'.
    
    You've used 'else' in a context where Charm can't make sense of it. This
    error is returned when parsing a malformed function declaration, though it may also
    be returned when the surrounding code is so weird that Charm thinks you're trying
    to declare a function even though you're not.
    
    In this case the problem is very likely a knock-on effect of the previous error
    ([4] Error: a line ending in ',' must be followed by a line beginning with '..' at
    line 26:0-3 of 'src/errordemo.ch').
    
    For more information about function declarations, see 'hub help functions'.
    
                                            Error has reference 'init/inexplicable'.                                                             
    → hub why 4                                                                     
    
    Error: a line ending in ',' must be followed by a line beginning with '..'.
    
    Charm allows two forms of continuation, and one is to end a line in a comma in
    positions where a comma would be correct anyway, and to begin the next line with
    '..'
    
    For example: 
    
     ┌──────────────────────────────────────────────────┐
     │                                                  │
     │  someList = [1, 2, 4,                            │
     │           .. 8, 16, 32]                          │
     │                                                  │
     └──────────────────────────────────────────────────┘
    
    Note that such continuations are not bound by the whitespace rules and can be
    positioned as you like for readability.
    
    The error you're seeing is because you have ended a line with a ',' indicating that
    you want a continuation, but then haven't followed it up with a '..' at the start
    of the next line.
    
    For more information about continuations, see 'hub help continuations'.
    
                                                    Error has reference 'lex/comma'.                                                             → hub help continuations                                                        
    
    Charm expects the end of a line to coincide with the end of an expression, i.e. you
    can't write something like this:
    
     ┌──────────────────────────────────────────────────┐
     │                                                  │
     │  foo = ""hello "" +                                │
     │  ""world""                                         │
     │                                                  │
     │  bar = 1, 2, 3,                                  │
     │  4, 5, 6                                         │
     │                                                  │
     └──────────────────────────────────────────────────┘
    
    Continuations must be marked by a '..' at the end of the continued line and a
    corresponding '..' at the beginning. The allowed exception is that the continued
    line may end in a comma where this is syntactic, in which case the continuation
    must begin with '..' just the same. The continuations can be placed wherever is
    most readable: they are exempt from whitespace rules.
    
    For example, the following are valid assignments:
    
     ┌──────────────────────────────────────────────────┐
     │                                                  │
     │  foo = ""hello "" + ..                             │
     │     .. ""world""                                   │
     │                                                  │
     │  bar = 1, 2, 3,                                  │
     │     .. 4, 5, 6                                   │
     │                                                  │
     └──────────────────────────────────────────────────┘
    
    →                                                                                                                                            

None of this was particularly difficult, which is why I'm kind of cross at the people who didn't do it. And if you're working on a language right now … if your language is successful your bad decisions can be set in stone. Don't be that guy.

And if you've come up with anything particularly virtuous, do please show and tell, and I will steal your ideas.

ETA: screenshot in living color: [https://drive.google.com/file/d/1OhQ7QZ1GS7TTL6O\_2\_BGXFVS9mMZAeHx/view?usp=sharing](https://drive.google.com/file/d/1OhQ7QZ1GS7TTL6O_2_BGXFVS9mMZAeHx/view?usp=sharing)","The detailed explanations of the error messages are fine, imo, but not very important.  To me, the most important thing is that an error message should be very terse (so I can recognise it at a glance), and should show the relevant area of source code, colourising or otherwise highlighting the erroneous part, so as to draw my eye directly to it."
"""I find your lack of syntax variety disturbing"" | Syntax Discussion",v1t7p5,2022-05-31 23:44:23,"I just want to make it clear that the title is mostly a joke and it's based on a Star Wars quote. So it's not directed at anyone in specific or at the subreddit, just having fun with the title, I hope that's alright.

That being said, I've noticed that most languages, more specifically, most popular languages seem to have a syntax that's based on C. And that got me thinking, why is that? Does a language need to have a ""C-like"" syntax to be popular?

According to [Wikipedia](https://en.m.wikipedia.org/wiki/List_of_C-family_programming_languages) there seems to be 70+ ""C-family"" languages.

And that includes: C, C++, C#, Java, Javascript, Typescript, Perl, PHP, Go, Swift, Rust, Dart and Zig.

I would say that these are well known and mostly ""popular"" languages, but the point that many people will know and recognize most of these languages.

Now onto the the [""Lisp-family""](https://en.m.wikipedia.org/wiki/List_of_Lisp-family_programming_languages), there seems to be around 30 according to Wikipedia.

And that includes: Clojure, Common Lisp, Racket and Scheme.

Most of the languages here seem to be Lisp dialects instead of unique languages like the C-Family.

And finally the ""English-like, no brackets family"", please let me know if there's a proper name for this family of languages and I'll update the post. I couldn't find a name for them.

They are also the ""family"" with the least amount of languages as far as I know. That includes: Python, Lua, Ada and COBOL.

These languages don't use brackets like the C-Family or parentheses like the Lisp-Family to separate blocks of code or scope, instead they might use indentation-based syntax (Python) or the ""end"" keyword (Lua, Ada and COBOL).

What's interesting about these languages is that, although there's not that many of them, they seem to be really good at what they were created for.

I might have missed other families of programming languages, if so please let me know.

I hope it's alright for me to make a post like this in this subreddit, I'd like to start a discussion about different language syntaxes, pros and cons, popularity and what can be improved.

And other questions like: 

""Does a language need to have a C syntax to be popular?""

""Why most languages seem to be based on C?""

""Why does the Python/Ada/Lua syntax seem to be less popular?""

""Why does lisp have so many parentheses? /s"".

Hopefully this post can be used as a civil and meaningful discussion thread for language syntax.

And I'd also like to hear about your own programming languages, and what influenced your choice of syntax for them.

Edit: Fixed formatting and broken links.","Basing the syntax on something that is already familiar for the end user appearently lightens the mental burden of learning the language. If a language is C style, people will see this as familiar, and perceive it as ""friendly""."
Private Programming Languages,shimgy,2022-02-01 09:44:19,"I have been thinking, most of the languages are for public use. But what if a company decides to their own programming languages only for them? And doesn't releasing it to others without a pay.

Someone who knows the history of programming languages can tell me if this did happena and how it went?

Sorry if it's a dumb question","It does happen, and it can be beneficial. A domain specific language can be useful internally. Of course theres a scale issue where the company needs to be big enough to support its own language, but not so big that it benefits from controlling industry standards (if its big enough it can make its language take over an area). That said, even at places like Google there are some internally used languages.

Also there definitely are programming languages which are proprietary and sold with a license to use. It's niche but definitely a thing in certain enterprise fields."
Langjam 0001 winners!,pegcso,2021-08-30 19:39:33,,This is the best kind of hackathon that I currently know of! I’m so sad I couldn’t participate this year but I’m definitely looking forward to the next Langjam!
"From a PL perspective, how significant are the ""shortcomings"" of LaTeX?",nnv7n6,2021-05-30 04:04:20,"*\[Edit: As pointed out, I really mean the language ""TeX"", since LaTeX is actually an API wrapper\]*

I've often felt that LaTeX just seems ""messy and weird"" in an aesthetic sense.

Obviously it's also hugely powerful, flexible, miraculous in its way, and has stood the test of time.

Am I ""being weird"" and LaTeX is ""more reasonable than it looks""?

It seems full of cryptic commands and symbols and difficult to infer behavior.

In a way it seems kind of like old school Perl -- you can do amazing things rather concisely, but good luck learning it for the first time.

How do people in the PL community feel about ~~LaTeX~~ TeX as a language? To keep things as ""concrete"" and factual as possible, it would be great if people could stick to specific language features that are or aren't ""best practices"" relative to typical PL theory/practice.

Of course TeX is kind of a formatting language rather than programming per se -- I don't know how much that changes the considerations!

Thanks for any thoughts.

\[Edit: I just want to reaffirm that I am full of appreciation for the power of LaTeX! I don't want to give the impression that it's not a hugely important and useful technology. I am really trying to elicit some more scientifically ""informed"" perspectives/discussion about which principles this language does or doesn't adhere to in a way that affects developer/user experiences\]","Anecdotal evidence: One of my LaTeX homeworks at uni (actually it was postgrad, whatever) was to write a macro which takes an integral parameter `n`, and renders the [order `n` Hilbert curve](https://en.wikipedia.org/wiki/Hilbert_curve)

I succeeded, but needless to say, TeX is a _bad_ programming language...

More seriously: Somebody tried to create an alternative to (La)TeX called [Lout](https://en.wikipedia.org/wiki/Lout_\(software\)), with a more proper programming model. It makes a lot of sense, but LaTeX has the first-mover advantage, it's the de facto standard in mathematics, physics and computer science, and it produces beautful output (compared to the alternatives). So I guess we are stuck with it."
Advice/best practice/arhitecture pattern for building language with LSP in mind?,n3yrra,2021-05-03 23:21:33,"Hello,

I'm evaluating a possibility to build a custom definition language for internal use-case. I've already built a few languages on a ""classic"" test->Compiler->Output pipeline.

What I want to have from the start is a good tooling integration, mainly a very good integration with VSCode or Monaco (to host editor on our website).

I'm looking for architectural patterns or best practices for implementing LSP from the start. E.g. how do you handle performance, caches, intellisense, error feedback, etc.

Do you know a book or a good series of articles for this matter?","I would recommend checking out query based compiler architectures. The rust library [salsa](https://github.com/salsa-rs/salsa) serves as a good example for how to implement these frameworks (and if you are using rust, then you can just build your compiler on top of this library). Also, check out this blog post from the author of Sixten in which he goes over how he built his own query based compiler architecture before salsa. [https://ollef.github.io/blog/posts/query-based-compilers.html](https://ollef.github.io/blog/posts/query-based-compilers.html)"
How do you create all the libraries needed for your new programming languages to be useful?,mhlzpb,2021-04-01 10:51:10,"Whenever I switch to a new langugae my major problem is not being able to use all the libraries I know and wrote for language I already know. This problem is even bigger when I try to create my own programming language, since there are no libraries I can use and it feels like I have to reinvent the wheel a thousand times.

How do you deal with this issue? Do you reimplement everything from scratch? Do you support importing libraries from other langugaes? Do you write FFI to use foreign libraries? What other options are out there?","FFI is the most practical and isn't necessarily that hard, since you can usually link to anything via C or a platform ABI. 

Rewriting everything from scratch can be fun (you get to use your new language!) but gets old quick when you have to write things you don't care about.

Other possibilities include transcompilation, embedded language VMs, and other ""exotic"" techniques."
Initial Release: the Idyllic Language (v0.1.7),lera7m,2021-02-08 01:45:49,"Hey, r/ProgrammingLanguages!

I couldn't be more excited to announce the first working release of a language I've been working on for the past few days: the Idyllic Language! While there are a bazillion ways of describing what it is, I'd say the best way of introducing it is as a scripting languages built from the ground up for (REST; I'm a huge GraphQL advocate, and support for GQL will be coming soon) APIs. Instead of compiling to a binary or executable, Idyllic programs compile to a runnable web server. You can check out the GitHub (stars are always appreciated) at [z.rishi.cx/g/idyllic](https://z.rishi.cx/g/idyllic), and an example of a runnable Idyllic program at [z.rishi.cx/g/idyllic-todo](https://z.rishi.cx/g/idyllic-todo)!

Here's what an Idyllic program (affectionately called an Idyll) looks like:

    define middleware { test, logger } from ""./api""
    define guards { authed } from ""./api""
    define handlers { getAllTodos, postTodos } from ""./api""
    
    global
      | middleware logger
    
    fragment getTodosFragment(level)
      | guard authed(level)
      | middleware test
    
    route ""/todos"" {
      | middleware test
    
      get {
         | expand getTodosFragment(""user"")
         getAllTodos
      }
      
      post {
         | expand getTodosFragment(""admin"")
         postTodos
      }
    }

This `spec.idl` file would compile down to a simple TODO API that has support for GET and POST requests to the \`/todos\` endpoint. All computation in Idylls is handled by the Node.js runtime—we import guard, handler, and middleware functions from Typescript files and put them together in an Idyll.

In essence, the Idyllic Language is meant to reverse the paradigm that's usually used to create APIs in Node. Normally, you'd start off with a framework and define your functions according to the provided specification, but no matter what, this leads to a massive amount of useless boilerplate. At their core, APIs are just web wrappers over functions defined in a given language (of course, with some HTTP constructs laid out as well), and Idylls allow you to get back to those minimalistic roots—in the Idyllic paradigm, you write your functions in Typescript completely independently of a web server, and then construct your API when you've finished.

It's worth noting that there are two stages to this language compilation process: objectification and serving. The `@idyllic/compiler` package exposes a powerful compiler for `.idl` files that goes through the 5 stages of base compilation (and links up dependencies automatically—the language has first-class Typescript/JS interop), eventually resulting in an Idyllic object. This object is a simple representation of how data flows through the API, and is then fed into a server implementation of Idyllic. I've created a fast reference web server implementation over at `@idyllic/server`, which can come close to (and in some cases, beat) Express' request/response times.

The Idyllic language itself has dozens of awesome features to construct APIs, which include (but are not limited to):

* Static typing with Typescript & definition types
* Parameterized, first-class macro support with Fragments
* Data pipelines with Sequences
* First-class support for Middleware and Guards
* Query parameter capturing
* Request type definitions

I've written up a pretty comprehensive guide to the language and exposed APIs at the repository's [README file](https://github.com/rishiosaur/idyllic/blob/main/README.md) (I can only cover so much in this brief summary)—feel free to check the other features out over there!

Making something like this has been a dream of mine for quite some time, and I'm so pumped to see where this goes in the future!",Do you have a code example to demonstrate that boilerplates are indeed reduced in Idyllic compare to that of say Typescript+Express?
Python is a UI language there,lcevql,2021-02-04 20:46:16,,"Indeed this is the key distinction in language design between the scripting use case and the application development use case.

In the scripting use case the only thing you care about is getting the right output for *your* input. Once the output you get is correct, that's all that matters. Any error from a static type checker that you never encounter at runtime for your input is a distraction. There is no cost to failure. If the code later fails for some new input you have given it, you just fix it and move on.

In application development there is a high cost to failure and the code has to behave correctly not just for *your* test inputs, but whatever arbitrary inputs or configurations or environments might be encountered in production or on a user's device. It's in this abstract reasoning about program behavior for arbitrary inputs (""abstract interpretation"") that statically typed languages excel.

Scripting is a kind of half way point between using a GUI and doing software development. Technically you are writing a program, so you are developing software, but you only care about correctness for your immediate use.

Data science is all about scripting, and Python is presently considered the most accessible scripting language."
"All you need is λ, part one: booleans",j0esc1,2020-09-27 05:58:44,,"> [...], he then proved the Church-Turing thesis: that anything computable with a Turing machine can also be computed in the lambda calculus.

AFAIK, while Church-Turing thesis is considered to be true, it has not been proved. Mostly because the Church-Turing hypothesis is concerned about the informal notion of ""effectively computable"". Then Turing machines, lambda calculus, general recursive functions, register machines, etc. are an attempt to formalise it. All such formalisations have been shown to be computationally equivalent. The hypothesis states that all such formalisms are equivalent to the ""effectively computable"" notion."
Should I worry about copying another language's syntax?,irjxr9,2020-09-13 04:18:24,"I don't know if I really need to explain much here because the title is pretty self-explanatory, but here I go anyway.

For example, I really like go's type syntax for functions:

```go
func name(argName type) type {}
```

Should I be worried about using some syntax from other languages?","You should worry if you *don't* copy other language's syntax.

Familiarity to programmers is a great boon."
Dependent types and usability?,hb6rn4,2020-06-18 11:29:55,"As far as I can see, there is no ""popular"" dependently typed language right now, that's in ""general use"", say as much as F# or maybe Ocaml.

What do you think the reasons are for this? On paper, they seem awesome, but I have difficulty thinking about a lot of cases where I would use them. OK vector examples are cool, but how many more use cases are there like that? And at what cost?

I'd like to hear your thoughts on this. I'm sure some of you are hard at work, resolving or researching some of these issues so I'm looking to get educated.

Thank you!","Comments here seem to suggest that a dependently type language would require you to write proofs everywhere. This is _not_ true! You can avoid it if you just want to program using simpler types - so long as there is a good enough ecosystem for this in your hypothetical language of choice.

I'd also say that you don't need to be proving fancy theorems day to day to get value out of them. You can do a bunch of cool stuff with type directed meta-programming that's a whole lot more natural in a dependently typed setting, than in using C++ templates or typeclass induction in Haskell/Scala/Rust.

Some of the language design challenges I see for languages with full-spectrum dependent types include:

- Compiling efficient code in the presence of full-spectrum dependent types. When you have these you remove the clear phase distinction between runtime and compile time, which makes some important compiler optimizations hard (especially in the presence of ahead of time, separate compilation). We need better ways of recovering this if we want to be able to build software that performs well. I'm hoping [multistage programming](https://github.com/metaocaml/metaocaml-bibliography) with [modal dependent type theory](https://arxiv.org/abs/1908.02035) can help here.
- It's really hard to maintain abstraction boundaries in some of these languages. Changing internal definitions can mess up client code, which is really annoying! It's important to have good support for making parts of definitions abstract in order to avoid this. Some systems [have some support for this](https://agda.readthedocs.io/en/v2.6.1/language/abstract-definitions.html), but I'm not sure how confident researchers are over their soundness, and I'm not sure how common it is to actually use this in practice.
- Code reuse is hard if you go down the route of 'safe-by-construction' datatypes. You might have a regular list. Then you add a length to it and you have to implement a whole new data structure, along with all the library functions. If you want a provably sorted list you need to make a new one again, and then what if you want a sorted list of a certain length? I think [ornaments might be able to help here](https://hal.inria.fr/hal-01666104/file/ornaments-popl18-final.pdf), but I don't think any of the 'mainstream' dependently typed languages have implemented these. Another solution popular in Coq is to keeping the datatypes simple, and doing the proofs about these datatypes alongside.
- It would be nice to combine dependent types with SMT solvers in order to automate proofs, like in Liquid Haskell. The tricky thing is making sure that terms are valid SMT expressions that can be solved. I think [F*](https://fstar-lang.org/), but I've found the code in the tutorial a tad hard to understand.
- Helping people judge when _not_ to go on a type astronautical expedition is a challenge. While advanced types are much easier to understand in a dependently typed language than say, in Haskell or Rust, it can still not be worth it in terms of time investment. I have enough trouble trying to avoid this in Rust already.

Some more mundane stuff:

- Have decent package management
- Have good backwards compatability
- Better support for editors that are not Emacs.
- Libraries that are easy to understand by outsiders to the theory

All in all, I'm extremely excited about the potential of using dependent types in day-to-day programming, and I really want them to become mainstream. There's been a huge amount of great work that's been done in the area by the academic community, and there are many fantastic systems out there now, but we still need to do a bunch of work to get this stuff into the hands of most programmers. I'm not really sure this is necessarily job for the academic research community - they have their work cut out in researching the next frontiers. That's why I'm trying to learn more about them and trying my hand at my own implementations."
The minicaml programming language was renamed to gobba and now has a cute pixel art mascotte drawn by a friend!,eqjdlv,2020-01-19 01:31:43,,"Ah, the important stuff is done. 😃"
Unison: a new programming language with immutable content-addressable code,emrccv,2020-01-10 22:05:35,,"I was curious to try this but I don't want to join the slack channel to learn how to install it and the link to the installation document listed after the link to the slack channel leads me to a github page without any content.

(edit: upon closer inspection. The link provided is the ""edit"" link for the github pages. 

should be  https://github.com/unisonweb/olddocsite/blob/gh-pages/_includes/quickstart.markdown   

instead of https://github.com/unisonweb/olddocsite/edit/gh-pages/_includes/quickstart.markdown )"
Seven deadly sins of talking about “types” (2014),eajysq,2019-12-14 21:54:14,,"> Although many implementations happen to use syntactic compile-time reasoning, a.k.a. type checking, to lessen the run-time checking overheads, this is an implementation detail.

In correct; whether you type check at compile time or run time significantly effects the semantics. In Haskell, an incorrectly typed program isn't a Haskell program; incorrect typing is simply not part of the language. In Python on the otherhand, although it does have type errors, they are exceptions; a Python programmer can write their programs in such a way that type errors being raised and *catched* is part of normal functioning. Which style you like better is somewhat subjective, but it is ultimately about semantics, not implementation."
Papers and Algorithms in LLVM's Source Code,b22tw6,2019-03-17 15:35:24,"In a [recent thread](https://www.reddit.com/r/ProgrammingLanguages/comments/b18b7h/compiler_performance_and_llvm/eikhkbc/), I suggested that grepping LLVM's source code for references to algorithms might be an illuminating exercise.  (Although it's not an intro by any means; [here's an intro](https://www.aosabook.org/en/llvm.html) .)

I went ahead and did it, using some shell scripts of course!  I copied out a bunch of interesting comments here:

https://github.com/oilshell/blog-code/blob/master/grep-for-papers/llvm.txt

Notes on the notes:

- Several dense references to SSA and related algorithms
  - including some comments about computational complexity vs. ""real world"" speed
- Memory models and concurrency need a lot of attention
- Mitigations for the Spectre CPU bug 
- I did not know that LLVM does ""outlining"" -- the opposite of function inlining!!!
- LLVM has very optimized internal data structures, and there are several references for those
- Bit twiddling hacks
- A little number theory
- Knuth shows up
- A former teammate shows up (Fergus Henderson)

All in all, I knew LLVM was big and dense, but I was still impressed with what I didn't know that I don't know :)

I've done the same with CPython, which has some gems (e.g. dictionaries and memory allocation), but it's not as big and dense.

If you've read any of these papers or are familiar with the algorithms, leave a comment!

----

BTW, I got this idea from two posts that did the same for the JVM and .NET:

- https://lowlevelbits.org/java-papers/ (comments: https://news.ycombinator.com/item?id=13022649)
- https://mattwarren.org/2016/12/12/Research-papers-in-the-.NET-source/


","Because Reddit is buggy and lost some of my edits (grrrr...) 

https://llvm.org/pubs/ - A complementary link

https://github.com/oilshell/blog-code/tree/master/grep-for-papers - shell source code that generates this set of static pages, which I manually sifted through:

http://www.oilshell.org/grep-for-papers/llvm-7.0.1.src.wwz/
"
What's the smallest statically typed language implemented in itself?,161ke75,2023-08-26 11:52:06,"Do we agree that the smallest languages implemented in themselves are generally Lisps and Forths?

Here is a 469 line Forth compiler implemented in Forth:

https://github.com/kragen/stoneknifeforth/blob/master/tinyboot1.tbf1

Here is a 301 line BF interpreter implemented in BF:

https://github.com/canoon/bfbf/blob/master/bf.bf

(I actually don't know which  Scheme-in-Scheme is canonical.    I guess you can argue a bit about this, because almost all of them will all have a garbage collector in C.   Here's a tiny one, but it's not implemented in itself: https://www.piumarta.com/software/lysp/  )

---

I thought the answer might be C4: C in 4 Functions

https://github.com/rswier/c4

BUT there is no type checker.  It's a dynamically typed language, with only `int` and `char`.  (I encourage everyone to run the 5 commands in the README, ending with `./c4 c4.c c4.c hello.c`.  It's a brilliant piece of code.   The 4 functions are `next() expr() stmt() run()`. )

There are no structs -- if it had structs, I would consider it statically typed.

It is a bytecode interpreter for C, not a compiler.

---

So I was hacking on a type checker recently, and my question is: What's the smallest language with a type checker implemented in itself?

Standard ML famously compiles itself, is a relatively small language, is statically typed.  Though I just compiled it and couldn't figure out how big it is.  It's a huge distribution with hundreds of thousands of lines of code now!

https://www.smlnj.org/dist/working/110.99.4/install.html

---

The latest release of TCC by Bellard is 43K lines, although I'm sure it was much smaller in the past.  I'm sure it has a type checker.

https://bellard.org/tcc/

---

So what's the smallest statically typed language  implementation you can think of that can ...

1. compile itself ?
1. interpret itself?

Some variant of Pascal maybe?

Early versions of TCC ?","PL Zoo has some nice implementations in ML, but none of them claim to be self-hosted

https://plzoo.andrej.com/

It may be possible to write an implemetation of mini-Haskell in mini-Haskell, or turn it into something powerful enough for that:

https://plzoo.andrej.com/language/minihaskell.html

Actually the hard part would probably be the lexer and parser.  As far as I remember most of them use ML Lex and ML Yacc, which complicates the self-hosting claim."
The path to implementing a programming language,1504ozy,2023-07-15 14:57:18,,"Very nice. This was quite helpful. It would also be nice to have a little introduction to some of the other tooling pieces like syntax highlighting support, language server, etc."
What are the advantages/disadvantages of immutability as a property of a type vs. immutability as a property of an object/reference/parameter?,13vozxh,2023-05-30 20:31:00,"PHP 8.2 introduced [readonly classes](https://wiki.php.net/rfc/readonly_classes), which makes me think about the advantages/disadvantages (in general, not PHP-specific) of this approach (making all instances of a type immutable) vs. postponing the decision to where the type is actually used, as C++ does with `const` [annotations](https://en.cppreference.com/book/intro/const) (and Rust with `mut`).

Are there any references about this topic you can point me to?","Immutable data can be safely shared.

Immutable references can potentially lead to unspecified behavior if they coexist with mutable references. This is what Rust works hard to prevent."
"What is language complexity, when is it good, and what languages expose complexity well?",vtph4p,2022-07-08 02:30:15,"Hey all, something I've been pondering a while. What makes a language too complex? What even is complexity? And more importantly, when is it a good thing, and when is it a bad thing?

I'll throw out some interesting cases, which might inspire some opinions.

Python is a simple language, and it doesn't have static typing. However, even though the compiler doesn't track types, we usually still do that in our heads. The types are there, even if the compiler doesn't know about them at compile-time. We still need to track types manually. This seems to be **inherent complexity**, complexity that is there even if the language does not track it.

In Vale's design, one can write an entire program with just the base building blocks: owning references and non-owning references. If one wants to go the extra mile for more performance, they can use inline objects, regions, allocators, and override gen-checks with an operator. However, these don't change the semantics of the program, just the performance characteristics. A new user can successfully ignore these details and know what the program is doing. This **ignorable complexity** seems pretty good. (Note: These features aren't all done, but useful for discussion.)

Similarly, in C#, one can write an entire program without knowing about the `struct` keyword. But if one wants more performance, they can opt-into more complexity. Perhaps this **opt-in complexity** is a good thing?

In Rust, the borrow checker is based around one central principle, aliasability-xor-mutability (we can have one mutable reference xor many shared references to an object). This simple rule creates a lot of complexity for real-world programming, and makes us use patterns we might not naturally have opted for. Is the borrow checker simple or complex? Or perhaps both: fundamentally simple, but with high **emergent complexity**.

Also in Rust, the borrow checker's complexity hits the programmer all at once when learning the language. This is **up-front complexity**, compared to the more gradual complexity of e.g. C#.

Of course, Rust trades off complexity, but let's note that it often gets a great benefit in return: zero-cost memory management. **Complexity can be worth it.**

Also in Rust, the borrow checker can clash with other aspects of the language, such as async/await. Two simple features can cause **interaction complexity.** The opposite of this is ""composability"".

Most languages have complex sub-languages to express generics (e.g. `<T extends IShip>` etc). However, Zig was able to combine one feature (comptime) with another feature (static typing) to be able to add generics with no complex sub-language. Their features **aligned to reduce complexity** compared to other languages, assuming they would have added generics anyway (which is uncertain but beside the point).

Lastly, some language's complexities seem to arise in certain situations and not others. Functional programming is stellar when transforming data, but in cases with a lot of state, we need to bring in more complex concepts such as monads. We might call this **situational complexity.**

**So when is complexity okay and not okay** to add to a language?

Given the definitions above, my opinion is that:

 * Surfacing inherent complexity is often good.
 * Ignorable complexity is great when it gives us a lot of power. It doesn't harm readability much because new users can know they can ignore it.
 * Opt-in complexity is usually fine to add to a language, but perhaps less so if newbies see it a lot and get confused.
 * Up-front complexity should be avoided in favor of opt-in complexity or ignorable complexity.
 * Interaction complexity should be avoided if possible, one should instead favor other simpler approaches.
 * When orthogonal features can align to reduce complexity compared to other approaches, that's stellar.
 * Situational complexity should be avoided in a general purpose language, but for a domain-specific language, it should preferably not surface in the target domain.

Of course, these are lofty ideals, but in real language design we need to add complexity to offer the power our users want. Language design is an art in balancing complexities and the powers they enable in the best way possible.

I'd love to hear your opinions! What do you think complexity is? What are some great examples of languages handling complexity well?","I liked your example of functional programming being good for some types of applications while being complicated at other. Let's take this example further to REGEXP as a language and SQL. regexp can be quite complex for some - but for its domain it is well suited and understood. Same for SQL. So maybe the whole idea for one language to be ""general"" is just to ambitious for the variety of  computational tasks we face. Maybe we should have a good family of languages with good interactions/sharing between them."
"Vale's Higher RAII, the pattern that saved me a vital 5 hours in the 7DRL Challenge",tlis02,2022-03-24 06:20:53,,"One thing that's funny about this is how *coarse* the actual abstraction is; there's a lot of information that's thrown away. And yet, despite this, it's still good enough to catch actual bugs.

In particular, the `HashMapToken` can't be (implicitly) dropped, so you must call `.remove` on a `TokenedHashMap`. But there's nothing that actually enforces that you `remove` the right key, or even that it's removed from the right hashmap!

This isn't a criticism - I think it's rather remarkable how by providing on one small language feature, you're able to eliminate a huge class of mistakes without even trying that hard. There are definitely cases where you *could* accidentally remove the wrong thing (e.g. maybe one monster eats another one, and you accidentally mix up their tokens and positions) but the situation where this happens is convoluted enough that it's unlikely a programmer would get themselves into such a situation and not notice.

Rather, I would say that this really shows how a *little* bit of extra static checking can *in practice* build an abstraction that is ""more correct than the sum of its parts"" because forgetting to do something is one thing, but unintentionally swapping multiple tokens/locations/hashmaps requires a cavalcade of mistakes that are unlikely to all happen together."
Being explicit about implicit conversions,riva1r,2021-12-18 08:47:17,,"I don't think the acylic requirement is really necessary. While it does add complexity when you have aggressive type inference, implicit conversions add complexity for just about any language period so you even if the conversions are acyclic.

However, I do think conversions also need have a requirement of being ""fast"" for some definition of ""fast"". For example, TreeSet<Int> and HashSet<Int> are both essentially equivalent and satisfy every property for implicit conversions besides the acyclic requirement, but I don't think implicit conversion would be a good idea since converting between them could take a long time."
Pen and paper programing language,q8zeji,2021-10-16 06:48:59,,"tldr in the beginning 

Well, what exactly is this?

This is \[insertname because I don't want to commit to a name yet\] and its a programing language designed for being executed from an image(and it comes with its OCR software).

Why not just OCR a regular language?

try it

And what does it do differently?

This language is designed to be very limited in the number of characters it uses and to have a constant line structure that is very easily error corrected.

So does this work?

Sometimes

Hey r/ProgrammingLanguages I just recently saw all the cool stuff you were working on wanted to share my hs final project from a few months ago and also to get your input(more in the end). Like I stated previously it's a programing language specifically designed for simplicity in being read from an image and a program that can read it from an image. Each line of code in this new language is then converted in a new python file to a line(though there always is more than one function per line) but that file also needs to import another file that contains all the program data structures and backend stuff.

Each line of code will always be 3 words, A function and 2 parameters. This way the OCR can always operate as follows for every line. 1 get the raw translation of the first word so let's say ASSIGNINT in line 12 is read as ASSIGNIXT 2 find the closest match from all the function names to this function for example, the list of functions is \[ASSIGNINT, RETURN\] then the program will pick ASSIGNINT as the closest match 3 use the information about the function to read the next 2 parameters ASSIGNINT gets 2 parameters that are both the name of INT vars so we can find the closest match for both of them and we also know that both are English names and we don't need to read any digits.

So then the only place where we can't error correct is a) the name of a new var which isn't terrible because unless the error is really bad the program is most likely to also match all the other var calls to it b) it reads wrongly a new int(terrible)  a new string(bad but less likely to result in an error) or worst of all it reads incorrectly the number indicating at which line a loop ends(i should probably change this part of the syntax ).

The language is also designed to be easily debugged so for example when it crashes(or if the program correctly ends like in the example) it will print all of the vars and their values. it also has its own error messages and has line and recursion limits to avoid someone doing a while true on me

One thing I would like to add is a program checker python utility that can tell the ""program backend"" how the program should work and automatically check it. I already have logic checks in each line and function call(yeah this language is really slow) and accessing vars would be very easy and it would fit the theme (for example students write the answer for the test on a page and all the teacher needs to do is to create a new file that describes how the program should work and take a picture of every test ).

So why did I come here?

&#x200B;

I feel like the biggest problem with this language is that it's not that fun to program in and I would like your advice on how to improve the syntax or if you have any other thoughts and or feedback or criticism that would also be greatly appreciated.

&#x200B;

EDIT:

omg why is copy-paste so broken"
virgil: A fast and lightweight native programming language,pxux95,2021-09-29 21:02:58,,... aaand all relevant [research papers](https://github.com/titzer/virgil#research-papers) are behind ACM paywall :-/
Anatomy of a type checker bug,na8abr,2021-05-12 05:30:13,,"If you're implementing a substructural type system, wouldn't it be easier to implement move by default instead of aliasing? Rust-style"
Waid Language,mde90l,2021-03-26 10:05:30,"Hello everyone!

So, first of all, I'm a 20 year old university student with absolutely no formal education on the subject of programming languages and their implementation.  I've been programming for around 7 years and around 2 years ago I suddenly became interested in this topic.

After a lot of studying in my free time, last year I finally wrote my first programming language. It is obviously not perfect and there are a lot of things I didn't think too thoroughly before starting. Nevertheless, it works and I'm very satisfied with the result considering my inexperience.

You can find its source code here: [https://github.com/TaconeoMental/WaidLang](https://github.com/TaconeoMental/WaidLang)

My objective wasn't really to create a programming language to be used in the real world, rather than one to learn and have fun.

I haven't had the time to write documentation, but in the meantime I'm pretty sure that the examples are enough to get a grip of the language. The only thing that might be hard to find and understand is the error handling system (a very basic one), but in very basic terms it's return code based, and it works something like this:

*Every function in Waid, by default, returns a tuple of values in the form of (****value****,* ***error***).

*Whenever you call a function and use it as a value (assigning to a variable, passing as a parameter to another function) the* ***value*** *part of the tuple will be used. The only way to access the* ***error*** *part of the tuple is through the* ***\~>*** *operator. This second value is intended to be used to pass error codes which can be values of any type.*

Here's a simple program which hopefully illustrates this feature:

    include ""io""
    
    # Function that divides two numbers
    divide: func(x, y) =>
        if y == 0:
            <- null, ""Error values can be of any type""
        endif
        <- x / y # The same as ""<- x/y, null""
    endfn
    
    num1 => !(toNum !io::input)
    num2 => !(toNum !io::input)
    
    result => !(divide num1 num2) ~> error_value
    if error_value: # if error value != null
        !(io::printLine ""Division by 0"")
    else:
        !(io::printLine result)
    endif

I'm not planning to keep working on this project, but I would love to create another programming language in the future if I have time.

Any comments will be greatly appreciated :)

Cheers.",Congrats! Looks great
"Pointers Are Complicated II, or: We need better language specs",kd6cfb,2020-12-15 04:55:36,,"Whenever I see examples like the one at the start about failing to optimise the addition out of the loop due to accidental UB, I always think that 'UB' as a concept is \*way\* too coarse

Sure, \*maybe\* there's some machine that will produce a wacky result from an overflowing addition - but there aren't any systems that'll produce any other side effects as a result (other than setting condition flags? this probably doesn't affect anything though)

Surely it's more productive to instead have 'undefined results' - e.g. an overflow causes an 'undefined result' which fits into the result, but may not be a valid value for that result? This way we can avoid the weird backwards assumption that compilers make where they see 'well, this is UB so therefore can never happen, so we'll just assume it doesn't', limiting optimisation opportunities like this (and causing weird compiler bugs)

Is there any reason why this isn't done?"
Let should not be generalised during type inference,k4gkxc,2020-12-01 15:54:40,,"Does this simplification reduces the complexity of ML typing, currently DEXPTIME? I think the original proof used let expressions extensively."
Alexis King - “Effects for Less” @ ZuriHac 2020,h94s45,2020-06-15 07:58:46,,"Really excellent technical presentation going into benchmarks, optimization, and compilation strategies, with respect to implementing effect systems in GHC, but has many insights that might be interesting for people learning about compilation and the trade-offs involved.

I'd really love it if statically known effect stacks could be optimized to remove the dynamic overhead to zero for compute-bound tasks (I want to use effects in my inner loops!), but it's really exciting to see how well it does in the dynamic case."
A Survey of Languages for Formalizing Mathematics,grkmw9,2020-05-27 22:22:38,,interesting!
"Type Inference by Example, Part 1",fyyklj,2020-04-11 14:58:02,,"I've implemented a minimal ML in F#. Took me a couple of years on and off. I spent a lot of time rewriting my lexer and parser over and over again trying different approaches because I'm not happy with any of my designs. The only hard part about the interpreter was avoiding stack overflows. Type checking, on the other hand, was a freaking nightmare. Most of the example implementations on the web are student projects either completely misguided or full of bugs. There are a handful of academic papers giving simple overviews but they either omit key details or fall short. I found everything fell short when it came to applying HM to mutually recursive functions. I've implemented something that seems to work but I'm still not sure it is right.

So... good resources about implementing type inference will undoubtedly make the world a better place! Nice work!"
"[Toy Project] Lexers, parsers, type checkers, and interpreters for various tiny languages.",f5p061,2020-02-18 17:03:58,"This is a work in progress and I hope it is of interest to others here. I have started studying type systems through Benjamin C. Pierce's ""Types and Programming Languages"". While doing so, I am implementing the languages and type systems presented in the book in C++.

I chose C++ since: (1) it's the main language used in my job, (2) the language of choice in the book is OCaml and it was mentioned that implementing the same systems in a language like C++ (which lacks garbage collection and pattern matching over structures) is a challenge. So  I wanted to add to the knowledge presented in the book by implementing these languages in a production and challenging language like C++.

So far, I implemented the first 4 tiny languages. Every implementation contains: a lexer, a parser, an interpreter, and, if suitable, a type checker. In addition, a large number of tests is written for every stage of every language implementation.

Here is the GitHub repo: [https://github.com/KareemErgawy/types-and-programming-languages](https://github.com/KareemErgawy/types-and-programming-languages)","That's fun. I have the same project. It's not public yet, though. 

By the way, you should break your ast classes up so Term is a base class and each concrete term derives from it. Otherwise you're wasting a ton of memory."
[Thesis + Presentation + Source Code] The Nuua Programming Language,ci7mbu,2019-07-27 03:55:00,"So, few of you already know that, but turns out I've managed to design a programming language and implement a virtual machine for it. This is in fact, my bachelor thesis, and it's available to all public. It might be useful for people trying to learn similar topics or to get an idea of some tricks and solutions given to some common problems.

&#x200B;

**The Design of an Experimental Programming Language and its Translator**

Source code of the Compiler + Virtual Machine (C++):  [https://github.com/nuua-io/Nuua](https://github.com/nuua-io/Nuua)

Thesis PDF: [https://raw.githubusercontent.com/nuua-io/Thesis/master/Campobadal\_Thesis.pdf](https://raw.githubusercontent.com/nuua-io/Thesis/master/Campobadal_Thesis.pdf)

Presentation PDF: [https://raw.githubusercontent.com/nuua-io/Presentation/master/Long/presentation.pdf](https://raw.githubusercontent.com/nuua-io/Presentation/master/Long/presentation.pdf)

&#x200B;

I would like to note that this was my first ever programming language with the goal of a successful implementation. There's no other goal a part of education.

&#x200B;

Official site (it redirects): [https://nuua.io](https://nuua.io)

&#x200B;

Feel free to ask questions below if needed.

&#x200B;

Degree: ICT Systems Engineer.

Qualification: 9.5 / 10 with honorable mention.",Cool! The GitHub readme has no code examples. And the website link just redirects to the Github. Could you add some code examples to the readme?
Generating C code that people actually want to use,awfbpa,2019-03-02 15:35:20,,"Wow, great project I didn't know about!  I like whenever someone actually manages to land research in production :)  That's very difficult and you always learn something new.

I would like to see more blog posts about that.  How much verified code do Mozilla and MS use, and how much of it is from Project Everest?

The points about translation units were also interesting.  I know that sqlite ships with the ""amalgamation"" for this reason, but I didn't know if compilers had caught up in the last 10 years, with LTO and so forth.

But it seems like you can't rely on that, because there's still a lot of diversity in build systems and compilers.

I will probably do something like that for Oil -- generate a bunch of C++ in a single translation unit.

This also makes me want to go deeper in to OCaml :)  There's a lot of cool things being done in that ecosystem.


"
Static TypeScript: An Implementation of a Static Compiler for the TypeScript Language,162x1wp,2023-08-28 01:51:13,,"Found this very recent follow-up project, DeviceScript:

https://news.ycombinator.com/item?id=36059878

https://github.com/microsoft/devicescript

Now with source code!"
Representing heterogeneous data,15i92a9,2023-08-05 03:18:26,,"> But once you hop over to sum types, you lose that syntax entirely and have to instead sort of “invert” the code and use pattern matching and destructuring.

I ... don't think this is something you necessarily want to prevent. The fact that you need different code to access Ranged or Melee fields is a symptom of the fact that the code downstream of those accesses necessarily depends on which variant is present, whereas with a universal field it doesn't have to. More precisely, you can try to hide the branches downstream of your variant accesses, but you can't eliminate them, which means there's a risk you're just obfuscating.

Can I take it you don't want to attach properties or methods to your records? That seems like the obvious option in this general style of programming. I'd at minimum be tempted to put some of those branches on weapoy type into helper functions. It's at least an obfuscation risk with a proven payoff.

At higher power levels, refinement types are an option: downstream of a type check, the object statically has the fields of that subtype. That seems like it would even be easy for newbie programmers to grok.

It seems to me that if you're prepared to turn invalid field accesses into runtime errors, you may as well just dispense with static typing in that regard. All it's buying you is safe construction and nicer error messages, both of which can probably be done other ways. I feel like one way or another your approach sacrifices a lot of safety to dodge a learning curve that's not actually that steep.

If it was up to me, I'd use something that at least desugars to classic sum and product types (barring some wacky generalized nonsense I haven't finished figuring out). I might throw in methods and computed properties for where those help. May as well get the theoretical benefits of existing type theory, if you're not just going to do bags of properties.

I think your variant record thing is almost there, if you went the refinement types route. You'd lower it to a product with an inplicit sum-typed field containing the type-specific fields, and forward field references where appropriate.

Sorry this is a bit of a ramble. :)"
"What Vale Taught Me About Linear Types, Borrowing, and Memory Safety",13xkz1o,2023-06-01 23:45:01,,"Some of this is over my head but it feels like you inadvertently just created a monad?

So roughly speaking in psuedo-Haskell syntax, `RandInt` takes a type `Random ()` and returns the type `Random i64` where `Random x` is a monad that captures the random state/object over type `x`?"
Functional bytecode,12id0uv,2023-04-11 16:17:42,"I'm interested in whether work has been done to create a bytecode that is less imperative and more of a functional style. My hunch is such a bytecode may be more amenable to fast interpretation, since stuff like loops may be dispatched more directly to native code (instead of individual flow control ops). Has anyone seen anything like this? How annoying would it be for traditional languages to get translated into such a bytecode (does it require vectorization?)?",One example is the Spineless Tagless G-Machine from Simon Peyton Jones. It's a graph machine that proceeds by graph rewriting and is used for compiling/executing Haskell.
Unison: A Friendly Programming Language from the Future • Runar Bjarnason,10lnjfn,2023-01-26 16:52:57,,"""If a name gets a new definition, nothing breaks""

Doesn't that mean that if you change a definition, then you have to manually update all its uses, and then change all of their uses, and so on? Presumably you could automate that, but then you lose the ""nothing breaks"" again. In your IDE you presumably also don't want to see all old versions of your code, so you need some kind of filter system that determines the relevant versions.

At the end of the day, the programmer is then interacting with an abstraction built on top of the hashing storage model, and at some point the Git-like hashing storage model just becomes an implementation detail or performance optimization. The more interesting question is what the higher level UI exposed to the programmer is.

It may still be a better system overall, with respect to caching builds and the new types of UI enabled by the storage model, but I wonder what higher level UI exactly they have in mind here."
Compiled and Interpreted Languages: Two Ways of Saying Tomato,108gsh4,2023-01-11 02:20:03,,"There's a perfectly clear way to distinguish whether a language is interpreted or compiled (or both, or neither): whether it has any implementation that does so at the moment.

It's less functional and more stateful than one would like a definition to be, though."
I am disappointed by dynamic typing,z8674y,2022-11-30 05:07:05,,"I think you've conflated dynamically-typed languages and reflective languages (those that can introspect and manipulate their own structure and behavior at runtime)? Both reflective languages and dynamically typed languages require type information (and other metadata) to be present at runtime. Reflective languages use this metadata for introspection and structural manipulation. Whereas, dynamically typed languages instead utilize the type information primarily for the dynamic dispatch of function calls.

Unsurprisingly then reflective languages are often dynamically-typed, but this isn't a strict requirement. One example of a reflective statically-typed language is Java (although I'm not super familiar), which is able to examine and manipulate classes at runtime.

The main point of your argument seems to be that runtime introspection and manipulation is a powerful but underutilized tool. I'm not sure I agree, but even so, this isn't really an issue with dynamic typing as your title suggests."
New integer types I'd like to see,xr85kg,2022-09-29 21:40:10,,"I like the idea of having wrapper types for the examples, ie `non_negative<T>` implements unsigned arithmetic for any integer type `T`.

You could also have something like `total<T>`, which adds `+inf`, `-inf` and `NaN` to your numeric type. That way no integer operations will ever throw: `abs(INT_MIN) = +inf; -1/0 = -inf`.

This also allows you to give IEEE floating point total order. `not_nan<double>` could be the same as `double` but without NaN so procedures such as “binary search a list of doubles” is well defined for all inputs."
Kamby - A programming language based on LISP that doesn't seems like LISP,x7xrrc,2022-09-07 14:31:58,"[https://kamby.org/](https://kamby.org/)  
A small, embeddable and convenient language for who want to use and understand what is happening behind the scenes. The core is just \~400LOC and binary has just 20kb.  
Kamby Programming Language is a Lisp dialect with some conventions to create a lange more intuitive and compact.  
Internaly the implementation follows some basic concepts like S-expressions and car/cdr as any Lisp language. Kamby has some conventions to make the syntax more friendly.","    test = {
        x = 6
    }
    
    x := 5
    
    test
    
    print x
    
    Output: 6

Are you sure you want your language to be dynamically scoped? The language looks nice otherwise but dynamic scope is quite the deal breaker."
Hare: thoughts / experiences?,ws8xhr,2022-08-19 16:50:33,"i'm interested in this subreddit's thoughts about, and/or experiences with, Hare:

https://harelang.org/

> Hare is a systems programming language designed to be simple, stable, and robust. Hare uses a static type system, manual memory management, and a minimal runtime. It is well-suited to writing operating systems, system tools, compilers, networking software, and other low-level, high performance tasks.

My apologies if there's already been a thread discussing this; a search for both 'hare' and 'harelang' within this sub returned no results, but if someone has a link to an existing discussion, i'll happily go read that. :-)","I find it hard to get excited about another 'better C' considering how many of those there are, unless there is a unique selling point. Besides than the floppy disk thing, which admittedly is cute.

That said, it looks good. Plenty of documentation and the parts I've read are clear.

Requiring error handling is good. Propagation looks nice too.

Tagged unions are nice. is the tag just the type? Not sure about `as` to assert a variant.

Compile time length arrays? At first glance it seems like dynamically allocated memory is more different from static arrays than I'd like.

'Defer' I'm not sure about, I prefer destructors myself. Defer is flexible but perhaps too much so."
Georgia Tech professor's thoughts on C/C++ alternatives,wnm2e1,2022-08-14 03:05:45,,"A curated list of langauges like the ones mentioned in the video:  
https://github.com/robertmuth/awesome-low-level-programming-languages"
Approachable Programming Languages Papers for Undergrads,vumld2,2022-07-09 06:26:01,,"The Cousot paper is a bit, uhhh, *heavier* than the rest of the list. The Wadler & Siek papers are very approachable. Actually, just about anything by Wadler, Knuth, Siek/Taha are good — they're super into making sure as wide an audience as possible can get their main ideas."
Why is ATS not considered in the design of modern system languages?,uacib8,2022-04-24 03:10:01,"Quick though.
So, i've been studying ATS for a while now, and i'm pretty amazed by what it can do. It can perform in a type-safe way some operations that are typically considered unsafe, such as pointer arithmetic.
So, i've been wondering why modern system languages like Odin, V and Zig didn't take any concepts from it. As much as i love these languages, i think they are too conservative. Rust has taught us that a good type system can improve the quality of system programming, so why not keep going that way?
I understand that ATS can be kinda obscure, as its syntax is really hard to read. But i think with some abstraction and syntax cleanup, ATS may be the future of system programming.","For the benefit of the fossil here: what’s ATS?  (Google results are all over the place, nothing that makes sense in this context)"
People that are creating programming languages. Why aren't you building it on top of Racket?,sr9k8g,2022-02-13 12:00:26,"Racket focuses on Language Oriented Programming through the [\#lang system](https://docs.racket-lang.org/guide/hash-languages.html). By writing a new #lang you get the ability to interface with existing Racket code, which includes the standard library and the Racket VM. This makes developing a new programming language easier, as you get a lot of work done ""for free"". I've never created a new programming language so I don't know why you would or would not use Racket's #lang system, but I'm curious to hear what more experienced people think. 

Why did you decide not to choose Racket to be the platform for your new language?","I'm a fan of Racket (my intro to PL was a course taught by one of the founding members of the team), but I think a lot of people in this community *like* the idea of doing everything by hand. Some of them want performance, or maybe they want a syntax without S-expressions, or maybe they just want to do it all themselves to learn."
What is your C++ ?,qm3rln,2021-11-04 04:43:08,"C++ is well known to be a huge language that can be used in very different ways, going even further than other multi-paradigm languages like Python or OCaml.

Some people use C++ as a ""C with classes"", other as a Java-style OOP programming languages, others do everything STL.

As for me, I have not found the subset of C++ that fits my needs and tastes and I regret it because I know C++ fill many gaps I find in other programming languages.

So, I would like to ask you fellow programmers how you use C++?
What style would you advise for some game programming?

PS: My main programming languages right now are C (for library stuff) and Python (for everything), but I also like to program in Scheme (mostly for CLI tooling).","I wish it was concepts and template malarchy, but it's mostly just structs, functions, and a dash of virtual classes and unique ptrs when we have something to hide at work. At least nobody complains about lambdas or assigning inside the `if`."
"Yoakke, a new .NET library for implementing compilers",o7oupu,2021-06-25 22:30:38,"Hello there!

I've written many (toy) compilers, including for new experimental languages and reimplementing the existing ones in other languages. I've rewritten the same components – lexer and parser abstractions, lexical scopes, … - many-many times. There are some tools that I've used which supposed to speed up the process like Flex and Bison, but in my experience, they were way too cumbersome to use and didn’t let you roll your own partial solutions, if needed.

For the past year or so I've been writing a compiler in C#, for a language called Yoakke. I've noticed that unlike my previously used languages - mainly C++ and Java -, it has lot nicer abstraction and metaprogramming capabilities - especially with source generators introduced. I’ve decided that I’d take the infrastructure I’ve built up for the language and turn it into reusable compiler components.

And so, the [Yoakke Compiler Infrastructure](https://github.com/LanguageDev/Yoakke) library was born. It’s still a young project with a gigantic roadmap ahead, but the first few components are already documented and testable. Its goal is to provide solutions that will fit for most cases, but lets you roll your own if you needed, while still getting some help along the way.

Please have a look and tell me what you think!","Oh, that's cool. I've used Sprache before, but this looks great.

Out of curiosity, have you looked at `records`? They're a relatively new addition. I see you mention auto-equality/hash implementation using an annotation on classes, but records come with this for free."
Tree-walking interpreters and cache-locality,mrifdr,2021-04-16 00:33:52,"I was reading [this section from Crafting Interpreters](https://craftinginterpreters.com/chunks-of-bytecode.html), and it mentions that one of the big reasons that tree-walking interpreters are slow it because traversing pointers to sub-nodes in the AST has poor cache-locality.

Could this issue be mitigated if you ensured that all nodes existed in a contiguous region of memory? I can imagine, while parsing, whenever you created a new node, rather than allocating space just for it and giving the parent node the pointer, you could push the node onto a vector or some other such growable contiguous array and give the parent a reference to that spot.

Maybe my understanding of how cache-locality works is flawed, but wouldn't this solve that particular issue? With a bit of tree-flattening and other optimizations on top, could you create a tree-walking interpreter that runs at a decent rate?","> I can imagine, while parsing, whenever you created a new node, rather than allocating space just for it and giving the parent node the pointer, you could push the node onto a vector or some other such growable contiguous array and give the parent a reference to that spot.

Yes, that will definitely help! Once you do that, you may consider other stepwise refinements.

* Since subexpressions are executed first, it makes sense to have them earlier in the array than their parents. So instead of walking from the root to the leaves, walk the leaves first and then have the parents follow them.

* At that point, parents no longer need references to their children. Instead, you just need some convenient place to store the results of the child evaluations. Maybe a stack.

* Now your array of instructions doesn't need any actual links between them. It's just a flat list of which operations to perform.

Ta-da, you just reinvented stack-based bytecode."
Foundations of Probabilistic Programming,k07l5h,2020-11-24 23:57:04,,"Awesome, love open access books!"
Just write the parser,jeqva8,2020-10-20 22:43:35,,"I agree that recursive descent is the first technique you should learn, and it's applicable to a wide variety of situations.

But I would say that grammars are useful for one particular situation that is relevant to this sub (but not students in a compiler course): **designing** your own language!

That is, I like hand written parsers when you have a corpus to test against, and grammars for a new language design.  The latter is a very niche subject, but still important!

-----

Also, I think the [theory  of regular languages](http://www.oilshell.org/blog/2020/07/eggex-theory.html) is useful and practical for students, but the theory of CFGs is less so.

With CFGs, most interesting questions are undecidable.  Most CFGs can't be recognized efficiently.  And as mentioned, most ""real"" languages aren't context-free.

With regular languages, you get `O(n)` time and `O(1)` space, which is a very important engineering property.  Regular languages also much more widely used than CFGs, e.g. in network/server configuration  and in big data."
A Lisp interpreter written in Umka,izjjqp,2020-09-25 21:37:06,"[Umka](https://github.com/vtereshkov/umka-lang) is a new statically typed embeddable scripting language. To prove that it is suitable for writing programs larger than 100-line scripts, I have implemented a toy [Lisp 1.5 interpreter](https://github.com/vtereshkov/umka-lang/tree/master/examples/lisp) in Umka. The inspiration was drawn from a [similar interpreter](https://github.com/robpike/lisp) written in Go by one of the principal Go designers, Rob Pike. 

Lisp is famous for having an extremely compact core. The core semantics is completely defined on a single page (p. 13) of the [Lisp 1.5 manual](http://www.softwarepreservation.org/projects/LISP/book/LISP%201.5%20Programmers%20Manual.pdf). This core provides a Turing-complete language with conditionals and recursive functions (but without loops) and can be freely extended with any additional functions. Since the only data structure in Lisp is a tree, a Lisp interpreter is a perfect test for garbage collection. Umka has passed this test successfully.

My Lisp interpreter currently implements only the basic functions (`car`, `cdr`, `cons`, `atom`, `eq`), keywords (`lambda`, `label`, `quote`, `cond`) and constants (`t`, `nil`). It follows the Lisp 1.5 definition more literally than Rob Pike's interpreter. In particular, it offers `label`, but not `defun`. Arithmetical functions will be added soon.",Since we're in /r/ProgrammingLanguages \- how difficult it is to embed Umka? Can it call C (i.e. host) functions?
Other easily embeddable languages like Lua.,h7u9v7,2020-06-13 06:10:09,"I am only familiar with embedding Lua and Python into C/++. After struggling with Python for years (1999–2006) moving to Lua was ... amazing. I haven't looked at embeddable languages since then, and would like pointers from y'all for languages that are *great to embed*.

Thanks!","In addition to Io and Janet, which have been mentioned, you might want to check out [Wren](https://github.com/wren-lang/wren), a very elegant little language."
Morel: A functional language for data,fedw92,2020-03-06 22:04:07,,"> ""[Morel being Turing complete] is necessary, because all functional languages are Turing complete...""

Counterpoint: Coq (and of course there’s many others)."
"""Thinking the unthinkable: What we cannot think in programming"", by Tomas Petricek. ""In this article, I try to discover some of the hidden assumptions in the area of programming language research. What are assumptions that we never question and that determine how programming languages are designed?""",e7n92w,2019-12-08 09:37:01,,"> As a programming language researcher, you can ask whether a mathematical model of a language has certain properties (is a type system sound?), but you'd be treated as lunatic if you (non-jokingly) asked if a certain language feature is beautiful

But that's exactly the question every programming language designer asks!"
My data processing tech project turns out to be a compiler...,c2gjzv,2019-06-19 20:36:32,"tl;dr: I started writing a data processing tool and quickly realised I was actually writing a compiler for a DSL. This is a little outside of my comfort zone and I don't have anyone to talk to about it. I am shocked to learn no-one wants to talk about compilers.

I'm a Machine Learning engineer. I've got a six-week gap between jobs coming up, and decided to pick a weighty project that'd be interesting, educational and career-relevant.

About a year ago, I had a project where I needed to search for very specific patterns in time series data. I realised that the ""proper"" way to do this would be to construct finite automata for these patterns and match the time series data like regexes. I didn't have anywhere near enough time to do that on the project, so I hacked together something that converted all my time series data into string representations and searched those with regexes. This worked brilliantly, but in the back of my mind I really wanted to solve the problem ""properly"".

So for my six-week work-break project, I decided to build a tool that converts time series data into a sequence of attributed objects, and parses a user-specified string (a sort of super-regex) as a FSM that can match to that sequence of objects. I also decided to write it in C++ for performance reasons and because it's straightforward to make bindings for higher-level languages like R and Python, which is where people might realistically be doing this kind of work.

(Another reason to chose C++, a language I have minimal experience in, is because it's a fairly prestigious skill to have in the Machine Learning world. It's used for lower-level algorithm implementations and interacting with specialist hardware like GPUs, and it would be to my career advantage to have a serious C++ project under my belt).

I'm planning on repurposing regex syntax as much as possible, partly because if there's user-uptake, they'll probably be familiar with the syntax, and partly because the regex operators are going to be referring to exactly the same operations when constructing FSMs. An example pattern would be something like `[@event_type=""EVENT_A""][@event_type=""EVENT_B""; @property < 150](.*)[@event_type=""EVENT_A""]`.

I had a pretty good idea of all the components I needed to put together for this project, but it wasn't until I started explaining it to my peers that I realised what I was getting myself into. I'm writing a compiler (a gnarly task I've never done before) in C++ (a notoriously fussy language I have almost no experience in). I checked out _Compilers: Principles, Techniques, and Tools_, which looks like it has all the implementation and theory I need, but it's still a little outside of my comfort zone.

I'm undaunted. I still want to do this, but it turns out I don't have anyone to talk to about it. The people who might use it (data folks who live in very-high-level-language-land) don't care about lower-level implementations. The C++ folk I know don't care about the domain-specificity of the problem and how that might drive design choices I have to make.

So after making the shocking discovery that most people don't want to talk about writing compilers for fun, I've come here. As far as I can tell, there's nothing quite like this tool out there, and I think it might actually have a lot of use-cases (interrogating log files, activity streams, panel data, etc.), so I want to build it well, but I'm probably not going to get it to a working state if I don't get feedback from other people who actually know what they're doing.

I'm pretty sure I'm in the right place, yeah? Any comments, suggestions, or dire warnings?","Recently posted on r/Programming was this free online book:  [http://www.craftinginterpreters.com/](http://www.craftinginterpreters.com/) 

It's a pretty approachable introduction to implementing an interpreter and a compiler. One is implemented in Java, the other in C. Should help you get on the right road."
The case for Nushell,165co33,2023-08-30 19:58:10,,"FYI you can simplify the fish for loop to get rid of the semicolons:

    for i in (seq 1 10)
        echo $i
    end

One of the best things about fish is that it does a great job of smoothly automatically updating indentation and intelligently interpreting the enter key as you go about writing things like loops in your prompt, and the same syntax works just as well in scripts.

> Cons: As it says on the tin, it's a shell for the 90s. It ain't the 90s anymore.

This con doesn't actually say anything specific. Fish is honestly great for the 2020s and beyond. Excellent autocomplete and syntax highlighting out of the box, proper XDG base path specification support, excellent configurability via a GUI, the ability to easily modify environment variables and have them immediately reflected across all shells with no rc file fiddling needed, and the ability to source bash scripts using bass.

Fish is still updating itself to stay relevant in the modern age as well, with the current rewrite from C++ to Rust and the planned future upgrades for migrating from `wchar_t` to UTF-8 and incorporating more concurrency.

Having said all of that, I do like nushell a fair bit as well. Just wanted to make the case for fish since it's my daily driver and I'm a big fan of it."
Tree-Structured Concurrency,14ng0eq,2023-07-01 07:48:55,,"Fantastic blog post! I'll be bookmarking this because I've also been trying to put together a well justified explanation why I think structured concurrency is important. 

Re: ""Guaranteeing Structure""

I have to shamelessly plug that the language I'm working on, Claro, comes with structured concurrency out of the box and enforces thread safety at a language level, forbidding data races and deadlocks. Consider checking out the very rough draft docs if you're interested: [Graph Procedures ](https://jasonsteving99.github.io/claro-lang/graph_procedures/graph_procedures.html)"
"Unifying uniqueness and substructural (linear, affine) typing",13i4nm0,2023-05-15 18:46:12,"This was prompted by [another post](https://old.reddit.com/r/ProgrammingLanguages/comments/13hulvw/deletion_or_invalidation_of_variables/), but I think it's a novel enough idea to warrant its own discussion.

There's often some confusion with the terms Linear types, Uniqueness types, Affine types, and how they relate, and some languages call themselves one thing but are more like another. In replying to the previous post I gave some consideration into how these distinctions can be made clearer and came up with a method of describing the relationships between them with a model which could be the basis of a more general type system in which these types can be used interoperability.

I'll first explain briefly what I mean by each type universe:

 * **Free** (normal types): Can be used an arbitrary number of times and there is no guarantee of uniqueness.
 * **Unique**: The value is uniquely referred to by the given reference and must have been constructed as such. It is not possible to construct a unique value from an existing value which may not be unique.
 * **Linear**: The reference must be used exactly once, but there is no guarantee that the value which constructed the linear reference had no other references to it.
 * **Affine**: The reference may be used at most once, and like linear types may have be constructed with a value for which there is more than one reference.
 * **Relevant**: A reference which must be used at least once, no uniqueness. Included for completeness.
 * **Steadfast**: The value is unique and must be used exactly once. (Term created by [Wadler](https://homepages.inf.ed.ac.uk/wadler/papers/linearuse/linearuse.ps))
 * **Singular**: This is the novel universe I've created for the purpose of this model. A singular is a value or reference which is uniquely constructed (and may only be constructed from other singular parts). The difference between this and `Unique` is that the singular's value may be used more than once (if moved to the Free universe), whereas `Unique` cannot be moved to `Free`. `Singular` still guarantees the reference is unique and the value was unique on construction.

Note that some uniqueness type systems already allow relaxing of the uniqueness constraint. I've made the distinction here between `Singular` (soft unique) and `Unique` (hard unique) because it is desirable to have values which never lose their uniqueness. And although a `Unique` type can be coerced into an `Affine` or `Linear` type, this does not make the internal value non-unique.

The relationships between these universes can be visualized in [this diagram](https://i.imgur.com/1bSzaia.png)

I've defined 3 means of moving between universes: `must_use`, `use_once_only` and `relax_unique`. All of these consume their input reference and return a new reference in another universe.

 * `use_once_only` produces a reference which, once consumed, may not be used again. 
 * `must_use` forces the returned reference to be consumed before it  loses scope
 * `relax_unique` revokes the ability to mutate the value under the returned reference.

The diagram is made so that these all point in the same direction. The diagram is also in 3 columns, indicating, from left to right: non-uniqueness, uniqueness-of-reference and uniqueness-of-value.

Now the interesting part of this model comes in how values are constructed. If you want to construct a value for a given universe `U`, you may only construct it from values from the universe `U` itself or from other universes which point to it (directly or indirectly) in the diagram.

If you use values from different universes in the construction of another value, then the constructed value must be *at least* in the universe which all argument types can be converted following the arrows. So for example, a type constructed from `Free` and `Unique` arguments must be at least `Affine`, but it may also be `Linear`. Anything can be made `Linear` since all paths end here. A value constructed from `Singular` and `Free` arguments must be at least `Free`.

Only `Singular` can be used to construct `Singular`. Once you move a value from the `Singular` to the `Free` or `Unique` universe, it is no longer possible to move it back to the `Singular` universe, even if there are no other references to the value.

These rules enable a kind of universe polymorphism, because a function `Affine x -> Affine x` should be able to take any argument of a type which points directly or indirectly to `Affine`. That is, the argument can be `Singular`, `Free`, `Unique` or `Affine`.

Functions cannot return a value in a universe less than their arguments, so `Affine x -> Unique x` is invalid.

How this relates to borrowing: A read-only borrowed term must be `Affine` or `Linear`. A writeable borrow must be `Unique` or `Steadfast`. If you want a read-only borrow of a unique type, you must lose the uniqueness constraint with `relax_unique`.

For guaranteed unique terms (`Singular`, `Unique` and `Steadfast`) it is possible to perform mutation internally without any loss of referential transparency, since it is guaranteed that no other references to the previous value may occur.

EDIT:
Renamed Singleton to Singular to prevent overloading commonly used terms.

EDIT:
Removed code sample: see [comment below](https://reddit.com/r/ProgrammingLanguages/comments/13i4nm0/unifying_uniqueness_and_substructural_linear/jk9c0c6/) on using bitwise operations for efficiency.

EDIT: As u\twistier points out, there is a [missing universe](https://i.imgur.com/Fm9bFLE.png) in the diagram above which can be added to complete the cube shape. I have called this type `Strict`, and it has a useful purpose for allowing mutation inside a loop for disposable values. [See below](https://old.reddit.com/r/ProgrammingLanguages/comments/13i4nm0/unifying_uniqueness_and_substructural_linear/jk8h0fv/)",Also see [Linearity and Uniqueness: An Entente Cordiale](https://granule-project.github.io/papers/esop22-paper.pdf).
How do you deal with lack of motivation?,128b7pg,2023-04-01 12:33:29,"I'm at the starting point of writing another language after having ""*completed*"" my first language [Glide](https://github.com/dibsonthis/Glide). But I'm struggling with staying motivated, not because I'm losing interest in language design, but because I'm starting to lose the point of why I'm doing this in the first place.

I started writing languages because of 2 major things, firstly that it was fun, and secondly for learning purposes. I believe I've achieved a lot of both designing and implementing my first language, but now I don't know if those goals are enough.

In an ideal world, I would like to create something that solves a real problem, that people can use in their day to day jobs, something useful. And I know that if I work on another general programming language, this won't happen. I can't shake the knowledge that other languages have implemented the features I've planned (barring syntax and all that), and the barrier of adoption is high enough that whatever I create will just be thrown in the pile of dead languages that no one really knows of.

I think to boil all of that down, I'm losing motivation because I don't think I'm working on anything unique and useful enough to be taken seriously and actually used. And that used to be okay when I was working on my other languages, but somehow the motivation to do this purely for fun is slipping.

I was watching a documentary on GraphQL the other day, and it's inspiring to see a technology go from inception to being widely used in production, and honestly I aspire to create something people would use. But I'm also afraid that another GPL is just not the answer. Which lead me to think, maybe a DSL is the way to go, but writing a DSL to me personally isn't as exciting.

I guess if I find the right problem to solve, the whole GPL vs. DSL argument becomes invalid anyway.

Sorry for the ramble, I didn't really know which direction to take this post. But wanted to get this stuff off my chest and hopefully hear your thoughts and experiences around this. Cheers!","If making PL was a fun hobby that is no longer fun, you can just stop. What's the problem? Go do something else, like basketball, or rock climbing, or weightlifting, or competitive programming. Forget about grand justifications and just have fun."
Zang - A dynamically typed programming language,121obyd,2023-03-25 22:53:49,"[https://github.com/cmspeedrunner/Zang](https://github.com/cmspeedrunner/Zang)

it has a text editor with syntax highlighting as well!

what do you guys think!","Damn, you made great progress since your [zenith](https://github.com/cmspeedrunner/zenith) project!

I think you're ready for something with static typing now.

Great work, keep it up!"
"Are there programming languages designed to be written on a smartphone device? We write code in letters, but given the rise of the iPhone and tactile screens I would expect a new gen of languages designed to be coded on a touch screen?",11x3k9q,2023-03-21 10:00:52,"There's a Wikipedia entry on [Tactile Programming Language](https://en.wikipedia.org/wiki/Tactile_programming_language), but it's pretty bare.

I know of the blockly language which seems the closest, but it's still not designed for touch screens from the ground up and is quite clunky to use on a small screen. 

There's Enso which is visual-text interchangeable but again is not fully suited imo.

Just wondering if there's one out here.","There is no need for a touch screen for writing code.  

There is no need for a keyboard either.

You can use a microphone and an eye tracker for programming.

For example [https://www.youtube.com/watch?v=FOJ6OvPf\_nM](https://www.youtube.com/watch?v=FOJ6OvPf_nM)"
A brief interview with Tcl creator John Ousterhout,10xkhu9,2023-02-09 11:47:56,,"I only knew of Ousterhout through his books, not that he worked on Tcl"
A package to pretty print trees to the console,10cnkq5,2023-01-16 00:13:28,"[https://imgur.com/a/R6jHJR3](https://imgur.com/a/R6jHJR3)

I made this package a while back and it's been extremely useful  (for printing the AST) while developing the language I'm working on now, so I thought I'd share.

Python version: [https://github.com/AharonSambol/PrettyPrintTree](https://github.com/AharonSambol/PrettyPrintTree)

C# version: [https://github.com/AharonSambol/PrettyPrintTreeCSharp](https://github.com/AharonSambol/PrettyPrintTreeCSharp)

Java version: [https://github.com/AharonSambol/PrettyPrintTreeJava](https://github.com/AharonSambol/PrettyPrintTreeJava)","Certainly better than what I first imagined. How does it deal with the problem of too wide a tree for the screen though.

You should fix that misalignment though ..."
The downsides of compile time evaluation,z0bhn3,2022-11-21 02:32:35,,"I sometimes have this feeling about Zig. One of the big pieces of ""ethos"" of Zig is to make it clear exactly what's happening for each line of code (similar to C) but because code that runs at comptime looks exactly the same as code that runs at runtime in many cases this confuses the actual runtime implications of your code at times.

I suppose it can be viewed as ""running possible code at compile time is always an optimization so it doesn't matter"" but I wish it was more clear when and when not code would actually be output into the final executable.

For example, switching over a comptime-known variable (ex: the target OS) looks exactly the same as switching over a variable that's only known at runtime and one emits numerous branches and the other emits only one. I can find this confusing."
Programming languages that best represent each programming paradigm?,xgvtkn,2022-09-18 03:49:13,"I'm a senior in college and I'm taking a programming languages class. We are learning about the programming paradigms and the differences between them. Online I see alot of different information about the category of each language. Because so many languages are hybrid, I am having a hard time selecting. What languages from each category would you recommend I learn and analyze if I want to gain a better understanding of how each paradigm works? Thank you for your time!",A great book for this is 7 Languages in Seven Weeks by Bruce Tate. The purpose of the book is exactly as you described - become familiar with different paradigms using languages designed for that paradigm. I believe there’s an updated version as well that might have newer languages (not that it really matters).
Let vs :=,x2bxsk,2022-08-31 20:08:02,"I’m working on a new high-level language that prioritizes readability.

Which do you prefer and why?

Rust-like

    let x = 1
    let x: int = 1
    let mut x = 1

Go-like

    x := 1
    x: int = 1
    mut x := 1

I like both, and have been on the fence about which would actually be preferred for the end-user.","I prefer let. It makes it more immiediately clear that ”this is a declaration” and not assignment. I dislike having assignment and declarations be the same, or even very similar."
"Ceptre, a tiny logic programming language for prototyping rulesets that you can run, interact with, and analyze",wdn7mo,2022-08-02 01:29:58,,"Stumbled across this awhile ago too!

I recommend watching the [strangeloop talk](https://thestrangeloop.com/2013/linear-logic-programming.html) from the author

I feel like linear logic programming is underexplored. Would love to play around and see if I could like, generate web apps in a language like this, but it doesn't seem to have expanded beyond a few research languages designed more than a decade ago..."
Goscript -- Golang specs implemented as a VM language in Rust,vtiu68,2022-07-07 21:33:28,"website: [https://goscript.dev/](https://goscript.dev/)

github: [https://github.com/oxfeeefeee/goscript](https://github.com/oxfeeefeee/goscript)",Haha Rust makes Go a scripting language. Show 'em who's boss little Ferris.
"Langception: I wrote a Forth in Charm, which I also wrote",utnhn1,2022-05-20 14:59:44,"I figured it would be a good way to dogfood Charm (it was!  I uncovered several issues), and I've been fascinated by Forth ever since I found out what it was. So [here it is](https://github.com/tim-hardcastle/Forth-in-Charm/blob/main/forth.ch), Forth in 254 sloc of Charm. I have written a concatenative language in a functional language that I wrote in an imperative language.

It may not seem like much to those of you who've done self-hosting compilers, but I'm feeling pretty darn leet right now. I also feel like I should do something with the Forth, maybe make a machine code emulator. And it occurs to me that some alpha geek must have gone for the record on this at some point and seen how deep they can nest things ... do any of you know of such attempts?

\---

ETA : since this is getting a whole lot of views maybe I should say something about Charm itself.

The primary use-case for Charm is that you can use it to hack out a simple backend app real quick. That's the actual point. The basic spec for Charm is ""Better than PHP"".

But because it's a well-designed language, I can *also* use it to implement Forth in under 300 lines of code. And unless someone can do that in PHP (and they can't and would go mad trying) then this too is proof-of-concept.","Now write a Charm compiler in Forth, so you have two mutually self-hosted compilers :)"
"50 Years of Prolog and Beyond. ""This article aims at integrating and applying the main lessons learned in the process of evolution of Prolog."" [abstract + link to PDF, 75pp]",sdo10c,2022-01-27 11:00:00,,"Full abstract:

>Both logic programming in general, and Prolog in particular, have a long and fascinating history, intermingled with that of many disciplines they inherited from or catalyzed. A large body of research has been gathered over the last 50 years, supported by many Prolog implementations. Many implementations are still actively developed, while new ones keep appearing. Often, the features added by different systems were motivated by the interdisciplinary needs of programmers and implementors, yielding systems that, while sharing the ""classic"" core language, and, in particular, the main aspects of the ISO-Prolog standard, also depart from each other in other aspects. This obviously poses challenges for code portability. The field has also inspired many related, but quite different languages that have created their own communities.  
>  
>This article aims at integrating and applying the main lessons learned in the process of evolution of Prolog. It is structured into three major parts. Firstly, we overview the evolution of Prolog systems and the community approximately up to the ISO standard, considering both the main historic developments and the motivations behind several Prolog implementations, as well as other logic programming languages influenced by Prolog. Then, we discuss the Prolog implementations that are most active after the appearance of the standard: their visions, goals, commonalities, and incompatibilities. Finally, we perform a SWOT analysis in order to better identify the potential of Prolog, and propose future directions along which Prolog might continue to add useful features, interfaces, libraries, and tools, while at the same time improving compatibility between implementations."
Write custom Julia compiler passes in library code,rud4xi,2022-01-03 00:26:48,,"Interesting - and rules look simple enough to write, which is a big plus. Need to check Julia again!

One thing what's long been on my wishlist is to compiler/IDE to show what it \*cannot\* optimize due to too little information. Also if it needs to do extra memory reads that programmer might not expect.

For example, I always pre-read for-loop end counter to a local in C/C++ from old habit. Compiler \*might\* optimize it, but due to aliasing might not. Of course, can always check the assembly output, but it's time-consuming."
"Why isn’t D-style, hybrid memory management, with both GC and manual heap more popular?",r4x2do,2021-11-29 22:31:12,"Thinking of memory models I had an epiphany that most of the time a GC is convenient and sufficient except for hot paths that need to be pause free which would have to be managed manually. I looked around if there was such a hybrid system already and I couldn’t find any instances until I stumbled upon a blogpost which mentioned D language’s support for it in passing. Reading more about D, it seems it does have a neat hybrid implementation that does give you the best of both worlds. However, I’ve not seen it discussed or even mentioned much when discussing memory management systems. 
 
Is there a catch that I’m missing here or is that paradigm simply due for a take off? What are your thoughts on a hybrid model that has a customisable GC which can pause before/after critical sections but not in between? What are your thoughts re a programmable GC in general?","D does not give the best of both worlds, if anything it's the demonstration that this idea does not work very well.

Most of the ""nice"" things in the D ecosystem are dependent on the GC. If it were only a matter of implementation that would be one thing, but the reality is that you can't expose the same APIs with and without automatic memory management, unless you make ugly tradeoffs that defeat the point."
"Lux 0.6 is out! Lisp for JVM, JS, Python, Ruby and Lua + static types!",qfg3f4,2021-10-25 20:54:54,,"Excellent work! Statically-typed Lisps are my personal interest, and they're harder to come by than a glacier in a desert. I remember reading about Lux some years ago, but didn't know it's under such active development. Will dig into it when I have time."
Haku: a Japanese programming language,pwxk60,2021-09-28 11:25:53,,"I know that natural language for programming is not meant as a serious use, but this one pleases me. Nice work :)"
What would a principled imperative language look like?,pno38l,2021-09-14 04:48:52,"When we design languages (and other things in computing) we often start with a big idea. Commonly, it can be expressed in the form 'every \_\_\_ is a \_\_\_', but this is not always the case.

Here are some examples.

- In LISP, 'everything is an s-expression'.
- In Haskell, 'every value is <strike>a burrito</strike> lazily evaluated'.
- In Ruby, 'everything is an object'.
- In UNIX, 'everything is a file'.
- In Zig, 'compile time code is first class'.
- Many languages follow Dijkstra's 'structured programming'

I feel like the space of 'big ideas' pertaining to imperative languages is under explored. Yes, some of the ideas I mentioned are most prominently implemented in imperative languages, but this is more of a coincidence than anything else: there is nothing about Zig's `comptime` that is intrinsic to imperative programming, Ruby's big idea could very well be implemented in a functional language (I admit I can't come up with an example, though), and one could argue that most functional languages (all that I know, at least) have structured control flow.

For that reason, I wanted to ask: what do you think a principled imperative programming language would look like? What should the big idea/founding principle be? What features follow from that idea?","FORTH is the ultimate imperative language. In FORTH, anything and everything is state manipulation and structural programming. It is one of the DRY-est languages around, and is little more than an incredibly clever wrapper around assembly code."
Reminder: LangJam starts this Friday,p5qy09,2021-08-17 06:39:36,,When does the theme get announced?
Types versus sets in math and programming languages,oqpynj,2021-07-24 21:23:44,,"By a funny coincidence, I spent the last few weeks contemplating how to address this *exact* issue in my programming language.

I don't know if this will help you, but the following are my thoughts on the topic, as it related to the language I am designing.

In a recorded strangeloop conference talk (which I cannot seem to locate anymore - and I didn't bookmark it :-( ) a speaker said something to the effect of: ""Type value are *constructed* while set members are *collected*."".

To me that was very intriguing, because I am designing a logic programming language where sets are very prominent. For a long time I suspected that sets would be the only ""types"" that I needed.

For instance

    // The set of non-negative integers
    NonNegativeInts = { int x \\ x >= 0 }

The set of `NonNegativeInts` would thus be a subset of int. But it could also be used to like a type when defining sets of records:

    // Persons is the set of records with Name and Age
    Persons = {. string Name, NonNegativeInts age .}

The problem in my planned semantics that the above quote highlighted was, that the `Persons` here is the set of *any* record with a (string) `Name` field and an integer `Age` field - as long as the value of `Age` is non-negative.

That is because an intentional set definition specifies a proposition which must be satisfied by a value for it to be considered a member of the set. But when the proposition is satisfied then the value *is* a member of the set. It is *collected*. That - to me - looked a lot like structural typing.

But for my language I wanted nominal typing. Or so I thought.

Then I realized that *sets are structurally typed* and by introducing *nominal types* into my language, I could have both structural typing and nominal typing within the same type system.

So, how are sets and types related?  My conclusion is that a *type* is **based** on a set, and values of the type can be *constructed* from members of that set. The set defines the *possible* values of the type, but each value must be constructed.

So if I wanted Persons to be a *type* I could write:

    // The Persons type consists of records with Name and Age
    Persons = type {. string Name, NonNegativeInts age .}

Like types are based on sets, sets can be based on types.

    Children = { Persons p \\ p.Age <= 12 }
    Teenagers = { Persons p \\ p.Age >=13 & p.Age <= 19 }
    LegalAdults = { Persons p \\ p.Age >= 18 }

These are *sets* based on the *type* Persons. The are collected. Which means that any Persons value is automatically *collected* by the sets.

Notice how (as opposed to subtypes) the (sub)sets based on Persons can have overlapping conditions (where I live you are legally adult at age 18, so some teenagers 18 and 19 are legally adults)."
A comparison of Futhark and Dex,klsk8y,2020-12-28 23:32:35,,"> I think this program illustrates the main difference in philosophy between Dex and Futhark. While Dex uses dependent types to secure an index-based notation, Futhark instead encourages index-free programming.

I think it's misleading to refer to Dex as having dependent types. It just has HM types, implicit typeclasses for index types, and a lot of polymorphism.

Specifically, there's no way for types to be based on runtime values (something that the ""Help wanted"" section of the paper explicitly calls out). Instead of dependent types, it just has existential types to deal with non-statically known values.

---

I think this is actually important for its design; as far as Dex is concerned, the actual theory needed to implement it is very boring - it's standard Hindley-Milner with almost no actual extensions besides `for i. (...)` being a convenient syntax for what could be expressed as `createArray (\i -> ...)` in any HM language with higher-order functions.

---


> Here the size of the tabulate must be the size of the array returned by filter, which is existential. As far as I can figure based on the paper, Dex wouldn’t allow an expression like the above, as it handles existentials in a conventional explicit manner:

I can't tell for sure, but I *think* that Dex essentially desugars existentials for you, meaning that it's not quite as limiting as you might expect. It would depend on whether you had a ""strict"" zip `zip : n=>a -> n=>b -> n=>(a,b)` or a ""relaxed"" zip `zip : n=>a -> m=>b -> E k. k=>(a,b)` that allows inputs of (statically) different lengths.

Although reading the source, I can't actually tell if existential types have really been implemented in code yet."
"Inko 0.9.0 released, featuring generators, pattern matching, the removal of nullable types, a brand new manual, and much more",kj0rqn,2020-12-24 04:36:26,,"The introduction of Option types and generators is something I'm quite excited about, as it makes writing iterators _so much_ easier.

Fun fact: [not too long ago](https://www.reddit.com/r/ProgrammingLanguages/comments/k8zh8m/why_nullable_types/gf3eac4/) I was still on the fence about Option types. But after finding yet another soundness issue with how Inko implemented nullable types, I got tired of them and replaced them with Option types. This did take about 3 days of fixing hundreds of compiler errors, but in the end I'm satisfied with how it turned out.

For the next release I'll be focusing on a more efficient memory layout and method dispatches. Originally I wanted to include that in 0.9.0, but it's going to be a lot of work; so I pushed 0.9.0 out first."
Esolang based on Chemical Equations: requesting ideas,jkr54y,2020-10-30 13:38:37,"I'm working on [an esolang](https://github.com/bigyihsuan/EsotericFormula) where code is modeled to look like a chemical equation, and I haven't gotten far with it other than its paradigm (stack-based, maybe functional), and that `light` and `heat` do something with I/O.

The farthest I've gotten so far is that `light` and `heat` do different things based on which side of the equation they are on: reagents they're input, products they're output, maybe STDIN/STDOUT.

For a type system, I've decided to keep it simple with an integer type (floats are represented as their IEEE 754 representation interpreted as an integer), and lists (which can be nested indefinitely and are heterogenous). Strings are represented as integer values under UTF-8.

I've also decided upon that each element (H, C, O, U, Fe, etc) corresponds to some instruction/function that gets applied to items popped from the stack, and a subscript (`H_2`) modifying the instruction in some way. A coefficient (`2H`) applies the function that many times. Default precedence is left-to-right, code inside parentheses first.

I feel this is something, but my main snag is the semantics of each equation. I don't want the program to be one massive equation. I have the thought of each equation popping some number of elements off the stack into a new mini-stack for that equation, which gets executed, then anything left on the mini-stack gets pushed back onto the main stack.

Thoughts?","This is such a cool idea!

Here are my thoughts:

1) I think that, like you said each element should be a function, however, I think the subscript should be the number of arguments that function takes. For example, `H_2` means the H function with 2 parameters. Those 2 parameters are the next two molecules of the equation. No subscript means the default value of that function. For example, the addition function with no subscript might return 0 and the multiplication function 1. So if H is addition and O is multiplication then `H_2 + O + O` would be 2. The addition function is applied to the result of the two multiplication functions that have zero subscripts so they return 1. Lists could be assembled with this syntax. Perhaps Lithium could assemble a list with the elements so ex: `L_3 + H + 0 + (H_2 + O + O)` is the list `[0, 1, 2]`! This is really flexible.

2) Perhaps instead of the coefficient repeating the function, which seems like a less used operation, it could instead act as if you had written it out twice. ie `2x == x + x` this reduces our earlier example to `H_2 + 20` which really looks like a chemical equation.

3) Your idea of using light really fights into this well. Here is how to create a list filled with 4 user-inputted numbers: `L_4 + 4light`. It's so expressive!

3) There is currently no way to apply functions to a single value so let's fix that. Molecules will work well for this. I propose that chaining together elements in a molecule applies them one at a time right to left. For a demonstration, let's say N negates a number. `NO`. This equals a negative one. First, the O on the right gets evaluated. Zero arguments so it returns 1. Then the N is applied to give a negative one. Let's create a list of negative numbers. Say for now that N also inverts all the numbers in a list. `NL_4 + H + O + (H_2 + 2O) + light` This creates a list of 0, 1, 2, and a user inputted number and negates all the elements. It looks great! To be clear in a molecule the rightmost function is the only one allowed to have a subscript. All the others operate on the single element returned by that function.

4) Now for the main question you had. What do we do with the equations? So far I have only talked about the left. Now let's talk about the right. Given that the chemical equation already includes an arrow for us it seems sensible that the right is the place the result gets sent. Light is easy. That's stdout. This prints 1: `O -> Light`. Otherwise, I think that the value from the left should just be sent to the memory location indexed by the result of the right. This may be a bit boring for simple use like `H_2 + 2O -> H` but for complex arrays and dynamic memory allocation, this could get interesting. Lastly, for control flow, I think that you should also be able to send the result of the equation to the instruction pointer as in `H -> Heat` This means start execution at equation 0.

This project is so cool! I would love to help!"
An unavoidable performance regression,ikhnzm,2020-09-01 18:14:08,,"Have you thought about making it configurable per project?

Instead of indexing arrays with type `i32`, have a new type `isize` or `iindex` which can be set with the attribute/directive `#![isize = i32]`.

In cases where the concrete size type is not known — for example in a library — and you'd like to compose functions of those different types, you will need to cast to the concrete types (which may fail).

On the contrary, introducing alternatives is often not a good idea in language design, obviously because of increased complexity and worsened maintainability.

^(e: phrasing)"
Pascal interpreter written 100% in Typescript,icxno0,2020-08-20 06:10:32,,"The question is 'why building an interpreter for an actually compiled language in a interpreted language to make it extra slow?' ;)

But so far a pretty good job!"
I made an esolang that uses 3D source code.,i6j28p,2020-08-09 21:09:10,,"I wanted to see actual 3d source code editor, not just a sequence of layers in 2d.

Also, ouch."
A History of Clojure (HOPL-IV),gx7as1,2020-06-06 00:01:33,,"I love this article. It is long and filled with distilled insights that motivated the design choices made for Clojure. It makes for a really enjoyable and thought provoking read. IMHO Clojure is one of the main events in PLD in the last decades. It's impressive how Hickey draws on hard won experience as well as deep philosophy and applied CS, just to change the world so that he can get to continue to work as a software developer using tools he need in order to stay sane and write the code he wants to write. You don't need to agree every point to enjoy it fully. I highly recommend it for any Language Design aficionado!"
Why don't people fork existing languages rather than writing new ones?,fsrdw4,2020-04-01 11:04:23,"There are thousands of languages out there written in every language under the sun covering all paradigms of computing. 

Instead writing yet another one from scratch why don't people just find one that's close enough and then fork it?  It seems to me this would be much better as some contributions could flow back to the origin and sooner or later there would be wider platform support.",And where's the fun?
I've been designing and implementing a programming language for the past couple months. I'm releasing my work so far.,eiun3z,2020-01-02 14:34:25,"I've been working on a programming language that is a bit of a mix of Kotlin and Rust with its own spin. I call it *Beagle* (of course I'm open to name suggestions). There are lots of design plans for it which are very volatile. Since most of my time is taken up by my day job, I'm limited to an average of one to two days per week to work on this project. I'm mostly trying to keep its development limited to weekends but sometimes that's not really possible. So its implementation is pretty slow. I am doing this in Kotlin/Native (for several reasons) and honestly all I have is a working lexer, which is the first pass of the parsing engine. This parsing engine is going to be a pipeline designed similar to Kotlin's new compiler starting 1.4. This allows several backends to be created using the same parsing pipeline. I'm in the process of creating several utility modules that all the passes will be using (such as data dumping, state management, error management, bytecode io management, etc).

Most of the work has been on documentation. This includes code and architecture documentation (UML models and flowcharts), language specification documents, and project task management (trello). Code wise, as previously mentioned, there is only the lexer and some of the utility modules.

Links:

[Language Codebase](https://github.com/AlexCouch/beagle-lang)

[Language Specification (so far)](https://github.com/AlexCouch/beagle-lang-specifications)

[Trello](https://trello.com/b/k0pbe8XT/project-beagle-language-compiler)

[Twitter](https://twitter.com/alexcouchdgc)","I think full language specifications can be a bit too long for people to read, but I see the repository has a summary `PHILOSOPHY.md` which is nice.  

For fellow language implementers I think these questions are generally interesting to read about:

* What's the background around its creation?
* What are unique/interesting features of your language?
* What issues does it try to solve for you?"
How to write a Turing-Complete programming language from scratch in less than 60 minutes (video),d076go,2019-09-06 05:31:55,"Any Turing-Complete language can, in principle, compute anything any other Turing-Complete language can. I use NodeJS and my parsing library CaffeineEight to not only explain what it means for a language to be Turing-Complete but also build the parser and interpreter in less than 60 minutes and 80 lines of code. This is powerful stuff!

Writing a toy programming language doesn't have to be difficult. With the right tools it can be easy and a useful exercise. At a minimum, it can be very educational. Languages define the boundaries of our thinking. Writing your own language, even if it's just for play, helps you think way outside the box. And who knows, your new-found language-writing skills can also come in handy the next time you need a custom DSL or have to parse something more complicated than regular expressions can handle.

[https://www.youtube.com/watch?v=f4\_WOycAx3Q](https://www.youtube.com/watch?v=f4_WOycAx3Q) (edit: fixed link)","Welcome to /r/programminglanguages. Your post sounds absolutely spot on for this sub. Sorry about the automod. :)

Your link starts at 66 seconds in. Is that a mistake?"
My First Fifteen Compilers,cb16xf,2019-07-09 21:55:39,,"Ok...I took the bait. 
So it’s a single compiler that the author worked on for 15 weeks as an undergrad."
Introducing the Odin Programming Language,bvzbn9,2019-06-03 00:42:54,,"Congratulations on all the work you have put into designing and building a new language that offers C like simplicity, but improves on several key capabilities. I hope you get the sort of traction you want. Good luck!"
Global Variables · Crafting Interpreters,al1ibo,2019-01-30 00:09:47,,Author here! Happy to talk about this if you'd like.
Why are you writing a lang?,1183daz,2023-02-21 21:25:22,It's a perfectly reasonable question.,For fun and to learn mostly. A little bit to experiment with different features.
"Charm, now with logging and instrumentation --- nooooo, come back people, I swear this is cool and interesting!",zps2gt,2022-12-19 21:48:43,"Pure functions are very lovely when they work, but when they don't, you want to stick a few temporary `print` statements in. And it wouldn't do that much harm, because functions that only do output are neeearly pure. It doesn't count. Bite me. I've done it. And so I've come up with something kinda cool which works for Charm ([repo, docs, here](https://github.com/tim-hardcastle/Charm)) and its syntax, I don't know about how it would work other languages.

The idea is that like comments, the instrumentation should be off to one side of the code, metaphorically and literally. It's easy to find, put in, turn on and off (easier still with a good IDE). Here's a script with a couple of tiny example functions. The bits with `\\` are logging statements and show up purple in [my VS Code syntax highlighter](https://github.com/tim-hardcastle/Charm/tree/main/vscode-charm-highlighter):

    def
    
    foo(x, y) :                  \\ ""Called with parameters"", x, y
        x % 2 == 0:              \\ ""Testing if x is even.""
            x                    \\ ""x is even. Returning"", x
        else :                   \\ ""Else branch taken""
            3 * y                \\ ""Returning"", 3 * y
    
    zort(x, y) :                 \\ 
        x % 2 == 0 and y > 7:    \\ 
            x                    \\ 
        else :                   \\
            x < y : 42           \\
            else : x + y         \\ 

Run it in the REPL ...

    → hub run examples/logging.ch     
    Starting script 'examples/logging.ch' as service '#0'.
    #0 → foo 1, 2   
    Log at line 6:
        Called with parameters x = 1; y = 2
    
    Log at line 7:
        Testing if x is even. 
    
    Log at line 9:
        Else branch taken 
    
    Log at line 10:
        Returning (3 * y) = 6
    
    6  
    #0 →

But wait, there's more! The sort of things you might want to log at each line could be inferred for you, so if you leave the logging statement empty, as in the function `zort`, Charm will take a stab at doing that:

    #0 → zort 2, 2 
    Log at line 12:
        Function called.
    
    Log at line 13:
        (x % 2) is 0, but y is 2, so the condition fails.
    
    Log at line 15:
        The 'else' branch is taken.
    
    Log at line 16:
        x and y are both 2, so the condition fails.
    
    Log at line 17:
        The 'else' branch is taken. Returning (x + y) = 4.
    
    4  
    #0 →     

The logging can be tweaked by setting service variables:

    #0 → $logTime = true                                                                                                                          
    ok 
    #0 → $logPath = ""./rsc/test.log"" 
    ok
    #0 → zort 3, 5 
    42
    #0 → os cat ./rsc/test.log 
    Log at line 12 @ 2022-12-19 05:02:46.134767 -0800 PST:
        Function called.
    
    Log at line 13 @ 2022-12-19 05:02:46.13737 -0800 PST:
        (x % 2) is 1, so the condition fails.
    
    Log at line 15 @ 2022-12-19 05:02:46.137498 -0800 PST:
        The 'else' branch is taken.
    
    Log at line 16 @ 2022-12-19 05:02:46.137561 -0800 PST:
        x is 3 and y is 5, so the condition is met. Returning 42.
    
    #0 →                                                                                                                                          

There are things I could do (as always, with everything) to make it better, but is this not a nice idea? How would you improve it? This is still a first draft, as always I welcome comments and criticism.

\---

ETA: Thanks to the suggestions of u/CodingFiend over on the Discord I've added conditionals to the logging statements, e.g changing the constants in the script below does what you'd think it would:

```
def

log = true
simonSaysLog = true

classify(n):
    n == 0 :
        ""n is zero""                 \\ log : ""Zero branch""
    n > 0 :
        n < 10 : ""n is small""       \\ log or simonSaysLog : ""Positive branch""
        n > 100 : ""n is large""
        else : ""n is medium""
    else:
        ""n is negative""             \\ log : ""Negative branch""
```
This is for when you want to keep the stuff around long-term and switch it on and off.","In `D` all `debug` statements are executed even if the function is annotated `pure` but the statement is doing impure stuff like I/O

https://dlang.org/spec/function.html#pure-special-cases


```d
pure int foo(int i)
{
    debug writeln(""i = "", i); // ok, impure code allowed in debug statement
    ...
}
```"
Looking for some good programming language papers to read. Any suggestions?,xzvzad,2022-10-10 04:51:26,"One of my favorites is [""Structured Asynchrony with Algebraic Effects""](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/asynceffects-msr-tr-2017-21.pdf) by Daan Leijen. Its easy to follow, well written, and makes a great case for its utility. Looking for something similar.

Thanks!",[Monad for functional programming](https://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf)
Type Theory Forall - #24 The History of Isabelle,xxj24j,2022-10-07 07:04:34,,"I wish so badly that this was on YouTube, so I could manage it with my other lectures, podcasts, and long-form audio content."
"updates to wrench, an embedded-system interpreter that handles weakly-typed c lightning fast with a small memory footprint",xa3rk6,2022-09-10 03:08:19,"big beat: actually using wrench in a project has yielded a lot of real-world optimizations that have sped it up by over 40%

\- In both a computationally-intensive task and a recursive fibonacci number generator wrench out-runs lua by around 50%

\- Reduced even further the bytecode size, wrench produces bytecode about 60% smaller than lua (yes with debug stripped)

\- Still runs on a teensy ram footprint of less than a k, with the interpreter taking up about 40k of text space on an arduino build.

\- can execute bytecode from ROM, or dynamically loaded on-demand from off-chip storage (such as an I2C EEPROM which I am currently doing)

Why compare to lua? Because it's a standard and it's what I've been using for little scripting tasks. Of course lua is far more mature, has more functionality (tables/meta) than wrench of course, so it's not an entirely fair comparison.

The purpose of this interpreter is to execute c-like easy-to-write and easy-to-read code but in a very small space and very VERY fast.  Also to be able to very quickly call back and forth to the native code.  I've posted earlier about this already so no need to repeat I guess.

Anyways it's smaller, faster, and available for you try if you want: [http://northarc.com/wrench-current.zip](http://northarc.com/wrench-current.zip) or github: [https://github.com/jingoro2112/wrench](https://github.com/jingoro2112/wrench)",Would you mind putting this up on a forge somewhere so that people can browse without downloading and expanding a zip?
You can have it all: abstraction and good cache performance,x2oexd,2022-09-01 04:55:40,,"> Safe, High-level, Abstractions for oPtimisation of mEmory cacheS

Great paper but these names are getting out of hand!"
General purpose match/switch/branch?,wmmeu5,2022-08-12 22:17:55,"I quite like how erlang does if statements. It's a bit flawed (not allowing a trailing semicolon makes for a lot of headaches when reordering or adding conditions), but I like the concept behind it.

    if
      guard1 -> expr1;
      guard2 -> expr2;
      true -> default_expr
    end.

Rather than being a collection of forks, it's a scope listing all possible branches. Kind of like a C switch-statement, if it was any good.

I also like Rust's match statements for much the same reasons

    match x {
       Some(n) if n > 100 => expr1,
       Some(n) => expr2,
       None => expr3,
    }

I like it so much that I constantly wish I could use it in place of a regular if-statement, and you kind of can, but it's terrible style and quite hacky


    match () {
      _ if x % 15 == 0 => println!(""FizzBuzz""),
      _ if x % 3 == 0  => println!(""Fizz""),
      _ if x % 5 == 0  => println!(""Fizz""),
      _ => println!(""{x}""),
    }

And I understand the why it is like that, it's not meant for general-purpose branching but for pattern matching.

And this got me thinking, we have `if`, `match`, and, in some languages, `switch`. Is there a need for these to be different constructs? Go and Odin already made their looping statements as general purpose as possible (using `for` as the traditional `for`-loop, for going through iterators, as a `while`-loop, and as an endless loop). So it stands to reason that the same could be done with branching.

So, I came up with the following syntax and I was wondering what your opinion was on it (ignore the specifics of the syntax, such as the use of curlies, arrow notation, and the like):

The idea is that match is a generic branching statement that tests the statements within its scope, executing the one with the first successful guard.

    match {
        guess < number  -> ""Too low""
        guess == number -> ""That's it!""
        guess > number  -> ""Too high""
    }

Additionally, you can add some partial expression next to `match` that will be evaluated against all guard values. So for divisibility, we pass `== 0` as the test and each guard value is some reminder of a division. 

    match == 0 {
        x % 3  -> ""Fizz""
        x % 5  -> ""Buzz""
        x % 15 -> ""FizzBuzz""
    }

We can do a regular `switch`-like statement by comparing the guards against the value of some variable.

    match == x {
        ""100"" -> ""Success""
        ""404"" -> ""Not found""
    }

Or, for pattern-matching, we try to bind the value of some variable as the test and match it against the provided patterns.

    match := x {
        [ hd, ...tl ] -> print(""{hd}, ""), print_list(tl)
        []            -> print("".\n"")
    }

Would it prove interesting for developers to have this freedom when working with branching code? Or are there any glaring reasons why this has not been implemented in any languages (that I know of)?","This is exactly Kotlin's `when` syntax (https://kotlinlang.org/docs/control-flow.html#when-expression)

Interestingly Kotlin also has regular `if` for when you really do just have 1 or 2 branches, and IntelliJ has a lint where it will convert more into a `when` clause."
Wrong by Default - Kevin Cox,uqy1ew,2022-05-16 23:05:20,,"> A few programming languages use a “defer” pattern for resource cleanup. This is a construct such as a defer keyword which schedules cleanup code to run at the end of the enclosing block (or function). This is available in Zig, Go and even in GCC C.

I'd note that this is a trivial higher-order function in any functional language:

    let with start finish run =
      let handle = start() in
      let value = run handle in
      let () = finish handle in
      value

His example:

    fn printFile(path: str) !void {
      var f = try open(path)
      defer f.close()
      for line in f {
        print(try line)
      }
      // File is closed here.
    }

Is simply:

    with open_read close [f →
      for line in f {
        print line
      }]

If you have exceptions in the language then the `with` function will need to handle them with the equivalent of a `try..finally..`, of course."
Generics syntax in different languages,tibrzi,2022-03-20 11:05:25,"Across different languages, we have seen different syntax for generics.

1. Angle bracket syntax (Type<T>). Ex languages: C++, Java, C#, Rust.

2. Square bracket syntax (Type[T]). Ex languages: Scala, Go.

3. SystemVerilog/Verilog Parameters syntax ( Type#(Param) ). 

4. Not sure if this applies, but functional languages' syntax (Though most of these languages have generic types inferred at the call site).

What are some other syntaxes for generics in other languages? Which syntax do you use in your language? What syntax do you prefer? Which syntax do you think is most readable?","Off the top of my head:
- D uses `Type!T` and `Type!(T, U, ...)`
- OCaml (and other MLs?) use `t my_type` and `(t, u, ...) my_type`
- PascalABC uses `Type.&<T>` in expression context I think. At the very least, it's used for function calls
- Nemerle uses `Type[T]` and `Type.[T]` depending on the context
- Julia uses `Type{T}`
- Crystal uses `Type(T)`
- ActionScript uses `Type.<T>`
- Dylan uses the very intuitive `limited(<type>, of: <t>)` /s"
MiniVM: A zero-dependency cross-language runtime on par with LuaJIT and C,rzcfcx,2022-01-09 06:52:17,,"Interesting. So what's the reason behind its speed? So far, I can only see register machine with computed goto."
Whitespaces around operators sets their precedence,q88a4i,2021-10-15 04:38:35,"For example

    2 * a+b // equals 2  * (a + b) instead of the usual (2 * a) + b
    a+b < c  or  a*b > d // eq. ((a+b) < c) or ((a*b) > d)
    2*(a+b) // parentheses reset the number of spaces needed
    a+ b // invalid because number of space and the left and right must match
    2*a+b // invalid because mixing multiple operator with the same number of space
    a+b+c // valid because it's the same operator

An advantage for languages allowing custom operators is that they would no longer need to make their users choose a priority for each operator.

However I'm not really sure how I would parse such a language.

What do you think about this ? From my point of view it seems more readable.

Edit: I'm not suggesting this approach to avoid choosing priorities for custom operators, it was just a side effect.

I should also add that math and basically every PLs do not use this approach and I know it is probably not a good idea in the end. It was more of a thought experiment if you will.","The (unfortunately dead) [Fortress](https://en.wikipedia.org/wiki/Fortress_\(programming_language\)) research language from Sun employed a very similar idea.

Though they made `2 * a+b` an error because no sane person would want that :)"
A mini-Erlang/Elixir -- tell me if/why my idea sucks,q7spi2,2021-10-14 12:52:11,"Hi everyone!

I've been making my way through ""Crafting Interpreters"" and it's really lit a fire under me for programming languages. I love this sub and all the weird discussions that go on about programming language minutiae. I've had this idea for a little while and I want to see what you all think -- whether you think it has any potential or should just remain a plaything.

I started a job this year programming in Elixir. This is my first exposure to functional programming and I have to say, I love it. I also love the concurrency-oriented framework that is Erlang/OTP and the BEAM.

Lua is well-known as a scripting language for games, since it's fast, simple, and easy to embed. However, what if there was a language that is *just as small* and easy to embed as Lua, but even faster, because it allowed you to easily take advantage of multithreading?

Multithreading can be a scary beast; locks, shared memory, communicating between threads, etc. is all expected as part of the price of entry. What I like most about Erlang is that it actually does away with essentially all of that:

* Data structures are immutable, so you never have to worry about another thread messing with your variables
* Erlang ""processes"" (essentially lightweight coroutines) have separate memory spaces and communicate solely by message-passing
* Processes are super lightweight in memory usage and context-switching time, so you can e.g. spin up 100k or more processes and the BEAM won't break a sweat
* Processes in Erlang are the encapsulation of state, similar to objects in object-oriented languages.
* Because of the immutability guarantees & message-passing mechanism, it's easy & natural to split up programming tasks across many different processes
* A central thread scheduler ensures all cores are constantly digging through all available processes and doing any work they need to have happen.

# The idea - a mini-Erlang

Essentially the idea is to take the best parts of Erlang and make a scripting language just around those principles. I don't think it'd be *too* hard to implement the basics as outlined above and bundle them up into a small Lua-sized package.

There's a lot of Erlang that we wouldn't have to worry about using; for example, generating [applications](https://erlang.org/doc/man/application.html), supporting distributed systems, or even having networking be part of the standard library. We could even package up a lot of the goodness and syntactic sugar that comes from Elixir macros as part of the programming language itself, which I think would simplify things (as opposed to supporting macros, that is).

What would be some of the benefits of this programming language over, say, Lua?

* Easy use of all cores the CPU has to offer, so multithreaded code can be moved out of the ""main"" application (C/C++ or whatever the embedding language is) and into the scripts. This is by far the biggest benefit.
* Since processes are garbage collected separately, there's no stop-the-world GC which means a lot less GC latency (important for real-time things like games)
* Code is easier to reason about, for functional programming reasons (pattern matching, immutable data structures, behavior separate from data etc.).

What would be some of the drawbacks compared to Lua?

* Adoption would be harder. Lua is undoubtedly very easy to pick up. Functional programming, while it has its zealots (myself now included), is hard for people to get into, for many reasons but especially it's not ""mainstream.""
* For something like games, it may be harder to guarantee that work split up among various workers finishes at the correct timing. (Erlang is good for ""*soft* real-time systems"" such as web servers, not games.) The scheduler may result in some non-deterministic orders of execution and/or unacceptably inconsistent calculation times. Hopefully the multithreaded nature of it means that you'll just have tons of extra CPU time all around, but when pushing the language to its limits, this will have to be addressed.

# Questions

Well, **what do you think?** I'm open to any and all feedback. Especially looking for suggestions of things I may not have thought of, or other applications/fields this type of thing might be useful for that I could take into account.

Besides feedback, I also have a couple questions I'm trying to figure out the answer to:

* For game development, I understand console development (for PS4/5, Xbox, Switch) is kind of limited to C/C++. For a library like this that I want to be as portable as possible, is it okay to implement it in C++ or should I stick to C? What about modern C++ features and/or the STL?
   * (I know it's unlikely I'll get to the point where that question would matter, but I want to be prepared just in case)
* Do you know of any papers or things I could read about the implementation of Erlang or M:N threading models? Or libraries that do anything similar, that I could check out the source code of? I have ideas of how I'd implement it, but it'd of course be helpful to see how people have done it in the past.","I think this could be an awesome project and I myself have spent a lot of time thinking about many of theses problems.


Two years ago I started working on an Erlang inspired language, that evolved into [lunatic](https://lunatic.solutions/). Lunatic nowadays allows you to take ""any"" language compiled to WebAssembly and expose mechanisms of lightweight processes to it. Originally it started as a lua variant (that's where the name comes from), but today we have the [best support for rust](https://crates.io/crates/lunatic).


Lunatic is not ""minimal"" and a bit more complex in scope as we want to support distributed processes like Erlang does. The VM is fairly stable today and most of my time is actually spent thinking about how to express many of the Erlang messaging patterns safely with a strong type system, to make the developer experience great when writing Erlang-ish code in Rust, C or all of the cool new languages that are WebAssembly first (e.g. [AssemblyScript](https://www.assemblyscript.org/) and [Grain](https://grain-lang.org/)).


Eventually I may write my own language that compiles to Wasm and is a perfect fit for the lunatic VM, but implementing the VM first and being able to test it and make it rock solid before actually thinking about the language proved to be an awesome approach. If I run into bugs I can be fairly sure it's the VM and not the language.


Taking onto such a big project can be scary and overwhelming, so I like to ""cheat"" a bit. Instead of developing a M:N scheduler I picked an already mature and proven one from the Rust ecosystem: [tokio](https://tokio.rs/#tk-lib-runtime). Then I just needed to develop a [virtual stacks solution](https://github.com/lunatic-solutions/async-wormhole) that works well with the scheduler. Instead of inventing my own byte-code I just picked WebAssembly, it's just a small abstraction above machine code and has mature JIT compiler libraries that generate code close to native speed. Then again, I just needed to figure out how to do reduction counting and insert preemption points into WebAssembly code during loading.


As you can see, at every step I try to lean as much as possible on existing libraries. This allows me to get something working fast, and keeps me super motivated. There is no better feeling than getting something on the screen fast, instead of spending months trying to debug weird stack switching assembly bugs and losing motivation, then giving up. That's why I would suggest you to try to pick a green thread (M:N) library and try to work with it and just maybe focus on the language in the beginning. You can come always back and re-implement everything to better suite the problems you are solving. I realise that this advice doesn't answer your questions, but I think it can still be useful as you are going to be working for a long time on this project and technical problems are going to be mostly dwarfed by motivational ones."
Examples of type systems describing things other than data flow?,pzs4rn,2021-10-02 17:41:40,"Almost all type systems describe data flow, but I recently was reading about experimental type systems describing time/space complexity. I guess one could implement a 'type system' for any aspect of code one might want to prove something about.

Which leaves me wondering: are there other kinds of 'exotic' type systems describing other aspects of code?","Some examples:

* units of measurement (eg. F#)
* lifetime / ownership (Rust)
* permissions / capabilities
* memory layout (this is mostly ongoing research; see eg. Cogent, Sixten, Konna)
* protocol correctness (linear types)
* hard realtime guarantees
* ...

Time/space complexity should be possible, but probably hard to use in practice (you have to prove your complexity yourself, as the compiler probably won't be able to do it apart from trivial cases)"
What are current challenging problems in PL design & implementation? (apart from those related to FP and proof systems),p7j9cf,2021-08-20 01:18:58,"I think most of PL research currently focuses on FP, type systems, proofs etc.. and that's pretty cool.

But even outside these, I am pretty sure there are hard problems in PL, compilers and tooling, although probably not as elite as P = NP problem.

some that comes to mind: closure serialization for distributed systems, properly implementing compile time computations, Generics monomorphization vs dynamic dispatch dilema, static linking (pretty sure this is due to cranky designs / implementations in C toolchains, but a problem nevertheless)....


what are some interesting ""practically unsolved"" or really hard problems in this space?","As a general note, much of the community is focused on work that can’t necessarily be characterized as an “unsolved problem”. For instance, a lot of work sounds a bit like “there’s an algorithm for xyz, now I present one that is [faster/more complete/better for some class of inputs]”. 

Attending conferences is generally how one gets a sense of what topics are interesting to the community at any given time. It’s safe to assume that if a talk is accepted at a prestigious conference such as POPL or PLDI, then that talk is gonna be about “current challenging problems in PL design & implementation”. (That’s actually what PLDI stands for)"
Supporting half-precision floats is really annoying,oyj4lp,2021-08-05 22:23:11,,"> But I doubt we’ll add more bits, so the numerics are always going to be dubious.

;)"
Programming Language Design and Implementation (PLDI) 2021: Accepted Papers,nh0q6s,2021-05-20 21:49:31,,"Some highlights (biased by my own interests):

- Alive2: Bounded Translation Validation for LLVM

- Beyond the Elementary Representations of Program Invariants over Algebraic Data Types

- Canary: Practical Static Detection of Inter-thread Value-Flow Bugs

- CompCertO: Compiling Certified Open C Components

- Concise, Type-Safe, and Efficient Structural Diffing

- Filling Typed Holes with Live GUIs (this one is about [hazel](https://hazel.org/), and I'm really excited to see the development)

- Mirror: Making Lock-Free Data Structures Persistent

- Task Parallel Assembly Language for Uncompromising Parallelism"
"If you could re-design Rust from scratch, what would you change?",mymwhz,2021-04-26 09:49:19,,I'd make it slower. I'm not good at program language design.
The language strangeness budget (2015),my1hqc,2021-04-25 13:15:56,,"I think it is more about barrier-to-entry than strangeness. Going from Java to Python is arguably more strange, but less difficult than going from Java to C++. And in particular the author is talking about Rust, which arguably has the highest barrier to entry of any modern language.

I like Rust, but I think simplifying the concepts and syntax would have had more benefit than merely using syntax that was similar to C & C++. Relating Rust's  use of enum to C's use of enum is straight up more confusing IMO than if  Rust had picked a new name."
The Golden Age of Compiler Design in an Era of HW/SW Co-design by Dr. Chris Lattner,mvv24w,2021-04-22 10:10:22,,"This talk references the Hennessy and Patterson Turing Award lecture from a few years ago:

https://old.reddit.com/r/ProgrammingLanguages/comments/9kzkm7/john_hennessy_and_david_patterson_2017_acm_am/

Also this new hardware design tool project under the LLVM umbrella:

https://circt.llvm.org/"
"What is your idea of a ""perfect"" programming language",k0hrpx,2020-11-25 08:51:35,"I'm currently writing a bytecode interpreter for a compiled programming language I am going to make in the near future. I'm looking for some features that are useful to the everyday programmer and that would be interesting to implement.

So please comment with either a feature suggestion or just what kind of ""ideology"" is best for you in a programming language :)","I suspect there is no such thing as a perfect programming language.  It's going to depend on your domain.  

It looks like for general purpose languages (if there is such a thing) are headed towards looking a lot like the ML family of languages.  C# looks more like F# every year.

So algebraic data types, pattern matching, and lambda are probably a good idea.

Allocation strategies and/or alternate memory management strategies seem interesting these days too.  So maybe an ownership system, linear types, regions, etc.

Personally I think row polymorphism and polymorphic variants are neat but I dont think they're that popular in general."
Featherweight Go,gu0jha,2020-05-31 22:58:44,,"> In our prototype, we use Unicode letters that resemble angle brackets and a
dash: Canadian Syllabics Pa (U+1438), Po (U+1433), and Final Short Horizontal Stroke (U+1428).

They actually went ahead and used them!"
Brother and I are developing a declarative DSL for building full-stack web apps - would love to get feedback / advice!,f7r8n9,2020-02-22 19:09:32,"**Background**: Brother and I have finished Master’s in CS, and our main experience with building compilers was during college, where we wrote a compiler for subset of C, in C.

We have spent most of our time last years developing web apps, and we got this idea that it would be great to have a higher-level domain-specific programming language that would abstract away a lot of boilerplate in developing a web app, but of course still allow you to customize it with your business logic.

That is how we started building **Wasp** ([https://wasp-lang.dev](https://wasp-lang.dev/)) and have just finished building (after a year, as side project) the first prototype! It is not yet powerful enough to be used for real development, but it should demonstrate some very basic concepts.  


Language is pretty simple, it is a DSL and is not general-purpose (for now) - currently it is more of a configuration language.  
We have built compiler in Haskell, and it consists currently of parsing step (lexical + syntax analysis in one, no semantic analysis yet), which results with some kind of AST, and then generation step, which generates “file drafts” containing js/htmls/css and similar, and final step (trivial for now) which creates actual files (generated code) based on those file drafts. We see ourselves adding more steps in the future, to parser for semantic analysis and to generator in order to manage complexity.

Wasp is still really early, and we would love any feedback - what do you think about the concept, design, implementation, do you find landing page / github repo clear … .

Some extra questions / food for thought:

* We have not used any specific methodology for designing Wasp, instead we picked a simple app (todomvc.com) and designed minimal language needed to develop it with Wasp. Next step is to pick a bigger app and continue with this approach. So “use-case driven development” (just made that up :D). I would love to hear if you have some suggestions, is there a better approach to this, maybe some methodology that you used or would recommend?
* Is there something we are doing wrong, regarding design or implementation that you see immediately obvious?  


Link to github repo, with all the code: [https://github.com/wasp-lang/wasp](https://github.com/wasp-lang/wasp) .

Thanks a lot, and I am looking forward to participating in this subreddit :)!","I think this is neat. Potentially name space issue?

[https://github.com/wasplang/wasp](https://github.com/wasplang/wasp)"
How I Came to Write D,8omjyo,2018-06-05 08:47:13,,"Inspiring article and great LOTR lesson, thank you!"
I'm working on a new promising FiascoLang,15xgt8b,2023-08-22 03:05:37,FiascoLang – an experimental language that can't even compile to machine code • loves embracing memory leaks • frequently takes coffee breaks (GC parties all the time) • snail-paced • thinks wasting memory is trendy • produces executables so big they've got their own gravitational pull • type confusion galore • dances on the edge of memory safety • worships NULLs • believes in chaos over optional-based control flow • what are tests even? • anti-modularity champion • versioning? More like guessing! • adores its shared mutable state • embraces radical complexity • has a brick wall for interop with C.,This is pretty close to just C++ isn't it?
Mojo 🔥: A programming language for all AI developers,135tfrc,2023-05-03 00:50:37,,"As an AI researcher/engineer this solves none of my problems. No Tensor scheduling, shape inference, auto diff, auto GPU memory management, AST manipulation, multi-host utilities etc

Just looks like Zig and Python mixed in some way without introducing any new capabilities"
Do you prefer writing DSLs or general purpose languages more?,1117t12,2023-02-13 21:07:07,"I've just finished (for now) working on my general purpose language. It was a long journey and I learned a lot about language design, and it was a lot of fun. But it did take up an enormous amount of my time and mental capacity.

I'm now interested in starting an easier, more focused project. Something that I can apply all the lessons I learned building a GP lang. So I decided I'm going to write a DSL. I'm not sure what domain to focus on (I have somewhat of an idea/concept), or how serious I want this project to be, but I think it would be a breath of fresh air to work on something more tightly focused.

Only problem is, I keep finding myself having the urge to turn the DSL concept into a general purpose language because a small part of my brain is convinced that writing a DSL would be a waste of time because of its limited scope.

So, who here is working on a DSL? What domain are you focusing on and are you enjoying it more than writing a general purpose lang?","Yes, the problem with little languages is keeping them little.

""Just one more feature..."""
say hello to wrench: a c-ish interpreter that actually fits and runs on tiny embedded systems,wpj2u9,2022-08-16 11:05:54,"So I needed to embed some user-modifiable scripts into a cortex M0, 256K of ROM 32K of RAM.

I started with lua which fit into the available space, but blew up when I tried to run.. anything..

so then I tried squirrel, same result. wren? non-starter. I'm sure they are all great but  have two fatal flaws: 

1 - when you run a script, even as bytecode, they load it into ram. with 32k ? \*stack collision with heap\* ring a bell? I need to be able to run bytecode directly from ROM. 

2 - even if that wasn't an issue, they all create heaps of tables and virtual structures when they run which also blows the ram cap on my poor little chip. wren allocated 250+K of memory just to load itself.

I know I'm trying to mow the lawn with a threshing machine but after a search I could not find anything that would do my job, so I wrote one. Actually I wrote something like this as a fun project (callisto) some years ago so I sort of started with those routines and a clean slate.

The result is wrench! The language is dynamically-typed c but full featured, everything works. I don't need switch yet but can add it.

* can execute bytecode from ROM, or dynamically load it with a callback (like an I2C EEPROM)
* has a very small RAM footprint when running, on the order of 500 bytes is the base it requires.
*  execute speed is on par with lua for the basic profiling I've done so far
*  interpreter fits into around 32k of program space (currently, project is still in motion)
* compiler+interpreter takes double that, all of 64k
*  supports everything a good interpreter should: if/then/else/do/while/for/functions/operators/etc..
*  bytecode is very compact, endian-neutral, compile anywhere run anywhere else (the command-line tool will generate byte-code headers for #include in the actual project)
*  handles char/int/float/array types
*  can operate directly on native data arrays, no thunking required
*  quickly and easily call c from script and vice-versa
*  memory is garbage-collection model but only used for arrays and very sparingly
*  MIT license, share and enjoy.

Still working on documentation, wrench.h is pretty well commented and lexicon.txt shows what the interpreter can understand (just about everything c does). Examples are included. It runs on win32 (visual studio) linux, arduino and Raspberry Pi, the platforms I have easy access to. I generally valgrind it to catch my stupidity, and so far it's clean.

Still a work in progress but functioning now and driving my board! Thought I'd post it now and get the ball rolling on feedback.

I'm attaching the current snapshot but I'll be iterating on it a lot, latest will always be on: [http://northarc.com/wrench-current.zip](http://northarc.com/wrench-current.zip)

Please forgive my spartan documentation, its hard to be polished when there is no audience, I strongly suspect very few will care about this.. but man I sure wish it already existed.

Here is a complete example ( for arduino, but the principals are all here)

    #include <Arduino.h>
    #include ""wrench.h""
    
    void log( WRState* w, const WRValue* argv, const int argn, WRValue& retVal, void* usr )
    {
    	char buf[512];
    	for( int i=0; i<argn; ++i )
    	{
            	Serial.print( wr_valueToString(argv[i], buf) );
    	}
    }
    
    const char* wrenchCode = 
    
    ""log( \""Hello World!\\n\"" ); ""
    ""for( i=0; i<10; i++ )       ""
    ""{                           ""
    ""    log( i );               ""
    ""}                           "";
     
    
    void setup()
    {
    	Serial.begin( 115200 );
    	delay( 2000 ); // wait for link to come up for sample
    
    	WRState* w = wr_newState(); // create the state
    
    	wr_registerFunction( w, ""log"", log ); // bind a function
    
    	unsigned char* outBytes; // compiled code is alloc'ed
    	int outLen;
    	
    	int err = wr_compile( wrenchCode, strlen(wrenchCode), &outBytes, &outLen );                     
    	if ( err == 0 )
    	{
    		wr_run( w, outBytes, outLen ); // load and run the code!
    		delete[] outBytes; // clean up 
    	}
    
    	wr_destroyState( w );
    }
    
    void loop()
    {}
    

I'd really love for this to be useful to others, I've sort of wanted this for quite a while (executing code from an I2C EEPROM could have been WAY handy on a space-limited chip, less of a problem now)

\-Curt","oh I should mention I took a page from wren's book and the source code wraps itself up into a single .cpp and .h file, makes adding to a project super easy."
Tell me what you think of this type system distinguishing between sets and types,qeyj5j,2021-10-25 02:54:51,"I am still developing my **Ting** logic programming language.

This is a very old itch of mine, which I unfortunately cannot help scratching.

Originally the language was developed from classic logic. Although fascination with Prolog was what made my friend and I start out on this, the language itself is not derived from Prolog nor any other language that we know of, besides mathematical/classic logic.

Here I would like to describe how the language features both *sets* (collected data types) and *types* (constructive data types), in the hope that you will provide me with some constructive feedback, challenges, comments, questions, and/or encouragement.

What do you think?

# Sets

Set members are *collected*: A set contains all objects that satisfy the set condition.

A set can be defined through a *set constructor* or through a *set expression*.

A *set constructor* is a list of expressions enclosed by `{` and `}`.

    OneTwoThree = {1, 2, 3}
    
    Names = { ""Alice"", ""Bob"" }
    
    Disparates = { ""Zaphod"", 42, true }
    
    Empty = {}

The above sets are alle constructed from a list of expression where each expression is simply a constant value.

An expression in a set constructor can also be *non-deterministic*, in which case the set will contain *all* of the possible values of that expression. A set constructor unrolls the nondeterminism of the expressions.

    PositiveInts = { int _ ? > 0 }
    
    Halves = { int _ / 2f }
    
    Points = { [x:float,y:float] }

Sets are first class objects, and can also be defined through *set expressions*.

    Coordinates2D = double*double
    
    Coordinates3D = double^3
    
    NamesOrNumbers = Names | OneTwoThree
    
    NamesAndAges = Names * (int??>=0)

# Types

Type members are *constructed*: An object is of a given type if and onlty if it has been constructed as a member of that type or a subtype of it.

A type is defined based on a *candidate set* through the `type` operator:

    // Customers a type of records, each with a number and a name
    Customers = type { [Number:int, Name:string] }

Like with sets, a type is also the identity function onto itself.

    // declare a customer
    Customers c

*Typed objects* must be explicitly created through the `new` operator:

    Zaphod = Customer new [Number=42, Name=""Zaphod Beeblebrox""]

# Functions

A function is merely a set of relations. The operator `->` defines a relation between two objects.

    Fibonacci = { 0 -> 0, 1 -> 1, int n?>1 -> This(n-1) + This(n-2) }

The domain of this `Fibonacci` function is the set `{0,1,int _?>1}` (i.e. 0, 1 and any integer greater then 1) which can be reduced (normalized) to `int??>=0` (the subset of `int` where each member is greater than or equal to zero).

The codomain of `Fibonacci` is the set of fibonacci numbers *{ 0, 1, 2, 3, 5, ... }*

A short form for defining a function is the familiar *lambda* `=>`:

    Double = float x => x*2

However, this is equivalent to writing

    Double = { float x -> x*2 }

# Partial functions and dependent sets/dependent types

An example of a dependent set is the domain of the *divide* operator `/`. The divide operator maps to a union of functions (some numeric types omitted for brevity):

    (/) = DivideInts || DivideFloats || DivideDoubles || ...
    
    DivideInts = (int left, int right ? != 0) => ...
    
    DivideFloats = (float left, float right ? != 0) => ...
    
    DivideDoubles = (double left, double right ? != 0) => ...

Each of the divide functions excludes `0` (zero) as a denominator (right operand). The domain of the divide functions are *dependent sets*.

    // `g` accepts a `float` number and returns a function that is defined
    g  =  float x => float y!!(y-x!=0) => y/(y-x)
    
    // f is the same as `x ? != 5 => x/(x-5)`
    f  =  g 5","This looks like a really interesting project, I like that it’s motivated by classical logic. I don’t see anything wrong with distinguishing between sets and types, doesn’t seem much different than defining a scalar vs a collection in lots of other languages (int vs int array in a language like C for example). The ability to define an infinite set is a really cool feature. Is there anything specific about this distinction you’re looking for feedback on? Also, how are you implementing infinite sets? Lazy evaluation I assume?"
How do type checkers deal with functions like this?,q6jyx6,2021-10-12 19:23:39,"Consider (pseudocode):

    function foo(x) {
        return foo(array{x})
    }

This function cannot exist in most static type systems - nor can it have its type inferred. But how does a type checker know that? What stops it from getting stuck in an infinitely recursive loop trying to work out the return type of `foo`?","IMO, the best way to understand Hindley Milner type inference is to look at it as a constraint-based system. (This is also how GHC and my compiler deal with it).

When you write `function foo(x) {...}` , the typechecker sets `foo`'s type to a fresh unification variable `a`.

Because `foo` is a function taking 1 argument, two fresh unification variables `b` and `c` are created for the types of the argument and the result, and the typechecker emits a constraint `a ~ b -> c`.

Tn the body of the function, the typechecker discovers `return foo(array{x})`, so it, once again assigns a fresh unification variable `d` to the type of `foo(array{x})` and emits the constraint `c ~ d`.

Now when the body, `foo(array{x})` is typechecked, the argument `array{x}` is assigned a unification variable `e` and, because this is a function call, the constraint `a ~ e -> d` (where `a` ist the type of `foo`)  is emitted.

Finally, `array` is instantiated at type `f -> Array f` and the constraints `f ~ b` and `e ~ Array f` are emitted, because `x` has type `b` and the result is supposed to have type `e`.

The generated constraints are

* `a ~ b -> c`
* `c ~ d`
* `a ~ e -> d`
* `f ~ b`
* `e ~ Array f`

Now the second phase of the typechecker, the constraint solver, kicks in and tries to solve these constraints. This part is pretty boring, so I'm just going to highlight the interesting steps

`a ~ b -> c` | `c ~ d`

\-> `a ~ b -> d` | `a ~ e -> d`

\-> `e -> d ~ b -> d` | `e ~ Array f`

\-> `Array f -> d ~ b -> d` | `f ~ b`

\-> `Array b -> d ~ b -> d`

\-> `Array b ~ b`

We have a really interesting constraint here. we need `b` to be a type, such that `b` is the same as `Array b`. If we just try to solve this naively, we might substitute `b` with `Array b`. But now our constraint becomes `Array b ~ Array (Array b)`. If we substitute again, we get `Array (Array b) ~ Array (Array (Array b))` and so on.

Typecheckers avoid this, by implementing a so-called ""Occurs Check"".

If in a constraint `x ~ y`, `x` *occurs* somewhere in `y` (or vice versa), the typechecker stops and throws an error, because it cannot solve this constraint."
Could you use previous runs of a program to improve optimization for future compilations of the same program.,ouz9qf,2021-07-31 10:42:35,One of the advantages of JIT is that it gives the optimizer knowledge of what the most used methods are and in what way they are used. This can enable better optimization in some cases. I don’t see why we couldn’t do the same think for ahead of time compilation. What if we have something to monitor the program or built in code in the program that generates a report that can be used by the compiler to make better optimizations in the future. This could be used for final release builds. Are there any compilers that do this or any discussion about it?,"Yes, it's called [profile-guided optimization (PGO)](https://en.wikipedia.org/wiki/Profile-guided_optimization)"
"Enso 2.0 is out! (Visual programing in Enso, Java, Python, R, and JavaScript)",mq9xvi,2021-04-14 03:44:02,,"Hi, I'm Wojciech, one of the founders of Enso.

Enso is an award-winning interactive programming language with dual visual and textual representations. It is a tool that spans the entire stack, going from high-level visualization and communication to the nitty-gritty of backend services, all in a single language.

Enso is also a polyglot language - it lets you import any library from Enso, Java, JavaScript, R, or Python, and use functions, callbacks, and data types without any wrappers. The Enso compiler and the underlying GraalVM JIT compiler, compile them to the same instruction set with a unified memory model.

Check out:

* our **website**: [https://enso.org](https://enso.org/)
* our **GitHub** (Enso is Open Source): [https://github.com/enso-org/enso](https://github.com/enso-org/enso)
* the **GraalVM** website (which Enso compiler bases on): [https://www.graalvm.org](https://www.graalvm.org/)"
Dependent Type Systems,jtwyxu,2020-11-14 14:00:30,"Does anyone's language project have a Dependent Type System?

Just curious if anyone has any experience with a DTS and wants to discuss details about it and how it's working out for them.

edit------

So, I've built a type system in my language where the types depend on values. I'm fairly certain that it is not equivalent to the academic type systems of COQ or friends, but it's pretty useful and expressive. I guess I'm wondering just what it is that I've built.

The language is basically an indented JavaScript and I've started with a type expression language at least superficially similar to TypeScript.

For instance

    type StringOrNumber = String | Number
    type FooAndBar = Foo & Bar

My type expressions can actually constrain specific values though using what I call a ""DotExpression"" `.` as a placeholder for the runtime value.

    type Alpha = Number & . >= 0 & . <= 1
    type Byte = Integer & . >= 0 & . <= 255
    type ShortString = String & .length < 100

The AST representation of my ""types"" are just Expressions.

    type Alpha = Number & . >= 0 & . <= 1
    //  is actually shorthand for this AST
    . is Number && . >= 0 && . <= 1

So my type ""analysis"" simply works by comparing two arbitrary expressions. If I have two types:

    type A = String & .length > 10
    type B = String & .length > 5

if I can show that if A expression is true then B expression must be true then I know that any instance of type A is an instance of type B.
(Of course in this example that's not true, although the reverse is true.)

My analysis function is known as isConsequent(a: TypeExpression, b: TypeExpression): Boolean | Null
If it returns true then I know expression b must be true if a is true. If it returns false then I know that b cannot be true if a is true. If it returns null then it means we cannot prove it one way or another.

I only throw semantic errors at compile time if I can prove that a type is wrong. If I cannot prove it's wrong, I let it go, but emit runtime type checks that will throw an error at runtime if the type is wrong.
This is effective for constraining the values of class instances, function parameters and variable types.

So, my question is: What have I built? and how does it relate to the more academic dependent type systems?","I wouldn't dare claim to have ""experience,"" but I am working on-and-off on an idea for a language with refinement types and types as first-class values, which are similar to dependent types. The idea being that this unifies a lot of disparate and individually hard-to-implement things (RTTI, generics/polymorphism, macros, typesafe parsing...) under a single (although still difficult) problem/language feature

My thought is to emit Z3 as part of the type checking phase, and let Z3 do all the hard SMT stuff.

There are a lot of great talks on YT by Edwin Brady, who created Idris, which, to my eye, is the most approachable and exciting implementation of dependent types I've seen. The Little Typer is also a great read, and Dan Christiansen also has a great talk on YouTube that covers much of the same material (IIRC, replete with live demonstrations of Pie, the dependently typed language implemented specifically for the book).

I'm not sure how much I can contribute, as I'm fairly new to the concept myself, but I'm happy to help however I can. Is there a specific aspect of DTS that you're curious about?"
Functional Programming and Reference Counting,i1s8m0,2020-08-01 21:30:58,"It's fairly well known that most functional languages use a garbage collector of some sort. This can sometimes end up being one of the performance bottlenecks.

Recently, I've taken an interest to Lean, and I found [this](https://arxiv.org/abs/1908.05647) paper by Leonardo de Moura and Sebastian Ullrich describing their implementation of a Reference Counting system for Lean 4, the upcoming version of Lean.

The performance numbers in that paper look very promising (faster than GHC, ocamlopt and MLKit in everything, beating MLTon and Swift in most tests).

Is there any other FP+RC languages out there? What could be potential benefits (apart from performance) for using RC over conventional GCs? Are there any issues that might come up?","I think this line from the paper is important: ""We remark that in Lean and λ\_pure, it is not possible to create cyclic data structures. Thus, one of the main criticisms against reference counting does not apply."". Most functional languages have no restrictions on recursion and cyclic data structures, so the algorithm from the paper probably will not work for them."
The Rise of Type Theory,g97pf9,2020-04-28 03:37:17,,"* Russell discovered the paradox in Frege's system. It isn't 'propositions' that leads to the paradox, it's Frege's axiom of unrestricted comprehension
* You gloss over the 'independent work' of Brouwer and Heyting, but it's very important to the entire type theory program
* I would clarify that Gödel proved ""there are *some* statements in this system can neither be proved nor disproved"". This is not true of every statement.
* Combinatory logic was developed by Schönfinkel as early as 1924
* Church was developing his lambda calculus prior to 1936
* It's true that Lisp's ""lambda"" is named after the lambda calculus, but McCarthy misunderstood it (in his own words). The language was initially based more on IPL.
* Algol has call-by-name evaluation but not lambdas.
* I'm not sure what you mean by ""This substructural logic applies contraction and weakening rules to sequent calculus"" in the linear logic section
* Rust is not really built around linear logic"
Message from ACM Regarding Open Access to ACM Digital Library during Coronavirus,frsp1f,2020-03-30 23:20:11,,"It's a shame to make people pay to access publicly-funded research that was contributed by researchers free of charge. ACM should not be getting credit for doing, during three months, what they should be doing all year long."
What are the language features you miss the most?,fcr6dd,2020-03-03 15:33:54,"Piggybacking a bit off the favorite languages post, not necessarily your favorite feature, but what features do you end up missing when you're writing in another language?

I imagine a lot of people will say things like ADTs and pattern matching, but I've had to write a bit of JavaScript recently and one of the things that immediately stuck out to me was the lack of tuples and checks for numbers of arguments passed.","Auto currying, partial application."
Lunar Programming Language by David A. Moon,f1jcdf,2020-02-10 10:11:51,,"I received the link as part of the email from Dave below. As mentioned below, feedback is welcome. 

> I finally proofread and posted the book I write three years ago about 
the ""Lunar"" programming language, in case of the unlikely possibility 
that anyone is interested in why I am right and everyone else is 
wrong (too common a phenomenon on the Internet.)

> http://users.rcn.com/david-moon/Lunar/

> Comments and criticisms are welcome, but I might not be able to 
respond right away.

> You are receiving this email because you once expressed an interest, 
possibly many years in the past.


> --David Moon"
"FOSDEM 2020 - Minimalistic, Experimental and Emerging Languages",eyst6u,2020-02-05 00:42:32,,"4 talks on Nim, eh? Seems to continue gaining a bit of momentum."
Does a language have to be especially unique to be worth making?,ep3itg,2020-01-15 23:14:02,"I have been working on my language, [kima](https://kima.xyz) (website is still WIP) for close to a year, and in the past few months I came across [koka](https://koka-lang.github.io/koka/doc/kokaspec.html), a project from Microsoft Research. As far as I can tell everything about this language is very close to what I have been going for with Kima: from it's main ideas about algebraic effects to even its syntax.

Finding out about this had a big impact on my motivation and made me reconsider whether this project is even worth pursuing. Why bother if a big research center like Microsoft Research has already done the same thing but better?

Lately I have been going back on my ideas about the design of Kima and trying to find/create differences from Koka, trying to see where I can do something different. Do you think this is the correct path to take?  On the one hand it feels like I'm trying to make changes for their own sake which could compromise the overall design, but on the other hand if I don't make these changes I don't really see what the point of the project is.

What do you think? Have you come across anything similar with any of your projects?","You could argue that there is even value in reimplementing something that already exists. As long as you're learning something, it's valid. Whatever knowledge you gain will guide you through future projects as well. So there's really nothing to lose by continuing your work. Plus, the ways your project is different from what already exists are potentially very important."
Algebraic Effects for the Rest of Us,cfzkdl,2019-07-21 22:29:06,,">When we end up in the `catch` block, there’s no way we can continue executing the original code. ... The best we can do is to recover from a failure and maybe somehow retry what we were doing, but we can’t  magically “go back” to where we were, and do something different. **But with algebraic effects,** ***we can.***

I recognize this was just one example, and will address the more general aspects of algebraic effects below, but the author had to start somewhere and so do I with this comment.

Aiui you can do as the author described in any language with suitable infrastructure without algebraic effects.

First, while I'm not sure how they've done it, two other posters in this sub have posted about resumable exception like mechanisms:

* u/codr7 posted about [their g-fu's Typical Restarts](https://www.reddit.com/r/ProgrammingLanguages/comments/bwh99h/typical_restarts/). They noted Common Lisp's restart features.
* u/sagecode posted about their Bee's `trial`/`patch`.

Perhaps more significantly, a key industrial response to the pure functional programming program, including algebraic effects, is coming to Java 9 and any other languages that see the light.

Imo, anyone who even considers algebraic effects needs to at least read the paragraphs about it in [From Imperative to Pure-Functional and Back Again: Monads vs. Scoped Continuations](http://blog.paralleluniverse.co/2015/08/07/scoped-continuations/); then if they still think algebraic effects are important, watch the video that the blog post corresponds to; then read the blog post.

This is the guy behind Project Loon that is bringing scoped continuations to Java this summer aiui (with fibers and tail calls coming later).

My own focus is P6. Aiui Larry assumed scoped continuations in the design since essentially the start in 2000. (They were first mooted in the 1980s I think.) I am increasingly convinced that they are a key factor in why P6's features such as its concurrency/async/parallel processing and exception system are simultaneously simple, flexible, powerful, composable, and able to be pushed out to mere libraries as OP mentions in their article.

As I posted in the threads linked at the start of this comment, P6 has the key primitive `.resume`. So far it's typically only used for interesting control features such as warnings (which throw an exception, which by default is caught by an outermost catch that displays the warning on stderr then just `.resume`s as if nothing had happened). But it's general. If I get time I will try to write a P6 version of the examples in the OP article, but just as they noted ""algebraic effects are much more flexible than try / catch"", the scoped continuations that make `.resume` possible are much more flexible, and again are why P6 doesn't require that function coloring for them to be async.

>You can also impress your friends by calling it a “one-shot delimited continuation.”

P6 assumes underlying platforms provide one-shot delimited continuations and builds its features on that basis.

One of the many great hackers who have worked on P6 over the years, in this case Stefan O'Rear, implemented them in Java for the Rakudo P6 compiler's JVM backend in 2013. They were implemented in C for the MoarVM backend in early 2014 iirc; I'm not sure if Stefan wrote the C version as well though my guess is he did.

>Which means that those pieces can even become librarified

Indeed, as they are in P6. Thus adding Actor semantics, in a manner that composes cleanly with the rest of P6, is a couple dozen lines of simple code in a userland module.

(Which then means one gets to leverage P6's very powerful module system to govern language evolution along anarchist, democratic, republican, or tyrannical lines simultaneously. This is of course a whole other story but I suspect part of the reason it's even realizable may be the underlying assumption of scoped continuations at the heart of the system.)

>Algebraic Effects ... are very powerful, and you can make an argument that they might be *too* powerful for a language like JavaScript.

As [Larry noted on the #perl6 IRC channel about their role in P6](https://colabti.org/irclogger/irclogger_log/perl6?date=2016-09-12#l20):

>we have delimited continuations internally, but we hide continuations from mere mortals

But they make P6 simple for the average user who has no idea, nor any need to have any idea, that one of the reasons the language seems to have such a magical combination of power and simplicity is scoped continuations."
"What is *the* book for linting, code analysis, etc?",c74kbs,2019-06-30 05:02:50,"We have the Dragon Book, and all of the others, that serve as the go-to references for learning about all of the gritty details of parsing theory, and compiler theory, and how to actually write parsers and compilers, and so on. Are there any references that serve the same purpose for linting and static code analysis? 

A little bit of Googling turned up a lot of projects working on the topic, but very little about how to actually do it, or even the theory behind it all. Granted, analyzing code starts with parsing it, so the parsing-related references are certainly relevant, and there are plenty of them. But, I'm not having much luck with resources for going beyond parsing and compiling. 

For example, once you've searched enough (or read enough Wikipedia articles) to know what a parser is, you can quickly find plenty of references along the lines of ""this is how you make a recursive descent parser"", or ""LR parsers are great, but packrat is easier, and here's why"". Trying the same learning strategy for code analysis turns up roughly nothing. Or my GoogleFu is failing me. I'm not sure which. And that leads me to my question: How does one get started in code analysis?

EDIT: Wow. I certainly came to the right place. And apparently I'm terrible at googling. I'll still take it as a win though. Thanks! I have a lot of reading to do now.","Here are some of the resources:

* Program Analysis Reading List - Rolf Rolles - http://www.msreverseengineering.com/program-analysis-reading-list/
* Reading for graduate students in static analysis - http://matt.might.net/articles/books-papers-materials-for-graduate-students/#analysis
* Program Analysis Resources - http://reversing.io/resources/
* Conferences on Software Verification and Analysis
https://github.com/soarlab/conferences

Lectures and courses:

* Foundations of Programming Languages (includes great introductory lectures to static analysis, dynamic analysis, abstract interpretation)
  - M-PS (WS 2014/2015): Concepts of Programming Languages
  - http://sepl.cs.uni-frankfurt.de/2014-ws/m-ps/index.en.html
  - lectures (videos & slides): http://sepl.cs.uni-frankfurt.de/2014-ws/m-ps/sessions.en.html

* Static Program Analysis
  - Anders Møller and Michael I. Schwartzbach
  - https://cs.au.dk/~amoeller/spa/
  - PLISS 2019 - Anders Møller 
     - Static Program Analysis (part 1/2) - https://www.youtube.com/watch?v=Lr4cMmaJHrg
     - Static Program Analysis (part 2/2) - https://www.youtube.com/watch?v=6QQSIIvH-F0

* 25 Years of Program Analysis
        - DEF CON 25 (2017) - Yan Shoshitaishvili (Zardus)
	- https://www.youtube.com/watch?v=XL9kWQ3YpLo
	- https://media.defcon.org/DEF%20CON%2025/DEF%20CON%2025%20presentations/DEFCON-25-Zardus-25-Years-of-Program-Analysis-UPDATED.pdf

* Software Analysis and Testing - [Mayur Naik](http://www.cis.upenn.edu/~mhnaik/)
  - http://rightingcode.org/
  - http://www.cis.upenn.edu/~mhnaik/edu/cis700/
  - https://www.youtube.com/channel/UCvwqRhlkE_Wm2FF9qzvHfJw

* Program analysis for reverse engineers: from T to ⊥
  - BSides Canberra 2018; Adrian Herrera
  - https://www.youtube.com/watch?v=vOmGmjbVff4
  - slides (not displayed in the video, may be a good idea to watch alongside):
https://drive.google.com/file/d/1j9rfMt14pubi6G9PKK3akddyeet5bf0x/view

* CS 252r: Advanced Topics in Programming Languages
   - http://web-static-aws.seas.harvard.edu/courses/cs252/2011sp/
   - https://www.seas.harvard.edu/courses/cs252/2015fa/schedule.html

Additional readings:

* This paper is a *must read* for anyone interested in program analysis in practice (one of my favorite papers in general):
  - A Few Billion Lines of Code Later: Using Static Analysis to Find Bugs (2010)
  - https://cacm.acm.org/magazines/2010/2/69354-a-few-billion-lines-of-code-later/fulltext

* All You Ever Wanted to Know About Dynamic Taint Analysis and Forward Symbolic Execution (but might have been afraid to ask)
	- Security and Privacy (SP), 2010
	- Edward J. Schwartz, Thanassis Avgerinos, David Brumley
	- Paper: https://edmcman.github.io/papers/oakland10.pdf
	- Slides: https://edmcman.github.io/pres/oakland10.pdf

* Analysing the Program Analyser - https://srg.doc.ic.ac.uk/publications/analyse-analyser-icse-v2025.html

* Continuous Reasoning: Scaling the Impact of Formal Methods
  - Logic in Computer Science (LISC) 2018; Peter O'Hearn
  - https://research.fb.com/publications/continuous-reasoning-scaling-the-impact-of-formal-methods/

* From Start-ups to Scale-ups: Opportunities and Open Problems for Static and Dynamic Program Analysis
  - IEEE International Working Conference on Source Code Analysis and Manipulation (SCAM) 2018
  - Mark Harman, Peter O'Hearn
  - https://research.fb.com/publications/from-start-ups-to-scale-ups-opportunities-and-open-problems-for-static-and-dynamic-program-analysis/

* Lessons from Building Static Analysis Tools at Google (2018)
  - https://cacm.acm.org/magazines/2018/4/226371-lessons-from-building-static-analysis-tools-at-google/fulltext

* Righting Software
  - IEEE Software,  May 2004
  - https://www.microsoft.com/en-us/research/publication/righting-software/

* Source Code Analysis: A Road Map
  - Future of Software Engineering (FOSE) 2007
  - David Binkley
  - https://dl.acm.org/citation.cfm?id=1254713

* Static versus dynamic analysis---an illusory distinction?
  - https://www.cs.kent.ac.uk/people/staff/srk21/blog/research/static-and-dynamic-analyses.html

* Background:
  - https://blog.acolyer.org/2018/01/26/a-practitioners-guide-to-reading-programming-languages-papers/
  - https://siek.blogspot.com/2012/07/crash-course-on-notation-in-programming.html
https://siek.blogspot.com/2013/05/type-safety-in-three-easy-lemmas.html
  - What is soundness (in static analysis)? - http://www.pl-enthusiast.net/2017/10/23/what-is-soundness-in-static-analysis/
  - OPLSS (Oregon Programming Languages Summer School) - https://cs.uoregon.edu/research/summerschool/ - free video lectures available, including the introductory ones based on Practical Foundations for Programming Languages: http://www.cs.cmu.edu/~rwh/pfpl/
  - SSA book -- particularly useful in a compilation context: http://ssabook.gforge.inria.fr/latest/"
Type Systems for Memory Safety,1576cdw,2023-07-23 13:47:33,,"I wonder how Daan Leijen's line of research fits in all this, especially the [Perceus ref counting](https://www.microsoft.com/en-us/research/uploads/prod/2020/11/perceus-tr-v4.pdf) stuff and all the new [functional but in place paradigm](https://dl.acm.org/doi/10.1145/3547634) and the [fully fbip](https://www.microsoft.com/en-us/research/uploads/prod/2023/05/fbip.pdf)."
"""What is syntax?""",143po64,2023-06-08 05:31:03,,What is syntax? A miserable little pile of semicolons!
How should Futhark be written?,13j3u3m,2023-05-16 20:29:21,,"1. Search for `flatten n m xs` because typo. (You want `unflatten` here.)
2. That `iota (3*3)` does not equal `iota (9)` is rather surprising. After all, `3*3` is probably `9` most other times. It breaks the substitution model of evaluation (as well as the principle of least astonishment). If it were somehow clear that the `(3*3)` is a shape and not a number, then I'd withdraw this critique.

I'm probably missing a subtlety on the `split` thing, but if the caller must *prove* that 0 <= i < N then your types they are dependent."
Algebraic Effects: Another mistake carried through to perfection?,1378d15,2023-05-04 11:16:32,"[https://kjosib.github.io/Counterpoint/effects](https://kjosib.github.io/Counterpoint/effects)

One of the best ways to get a deeper understanding of something is to write about it. The process forces you to go and learn something first. But on the internet, if what you write is the least bit provocative, you're bound to learn quite a lot more once you post your words.

I have donned my asbestos underwear. Let the next phase of my education commence. Please? Thanks!","This is a very insightful and beautifully written post. You nailed it!

The problem with checked exceptions been [identified and solved](https://www.cs.cornell.edu/andru/papers/exceptions/). The same problem and solution applies to [effect handlers](https://www.cs.cornell.edu/andru/papers/tunnel-eff/). [Effekt](https://effekt-lang.org/) is a language with [lexical](https://maciejpirog.github.io/papers/binders-labels.pdf) effect handlers which doesn't have this problem. Consider the following program in Effekt:

    interface Exception {
      def throw(): Nothing
    }

    interface Service {
      def doStuff(): Int
    }

    def useService {service: Service}: Unit = {
      println(service.doStuff())
    }

    def main() = {
      try {

        def myService = new Service {
          def doStuff() = do throw()
        };

        useService {myService}

      } with Exception {
        def throw() = println(""an exception occured"")
      }
    }

We define an interface `Exception` for throwing exceptions and an interface `Service` for some service that does stuff. Then in `useService` we use the service which is explicitly passed as a parameter. None of these mention any effects. Now in `main` we instantiate `Service` as an object `myService` which will throw an exception in method `doStuff`. We pass this object `myService` to `useService`. Again, no mention of effects. Yet, in Effekt we guarantee _effect safety_ which is to say all effects will be handled. In the special case of exceptions this means all exceptions will be caught. The program prints ""an exception occured"".

> Occasionally someone – typically in the 5-10 years experience range – will complain about the alleged burden of passing all these parameters every which way.

To ~~shut these people up~~ make these people happy we employ [implicit capability passing](https://se.informatik.uni-tuebingen.de/publications/brachthaeuser20effects/). Everything is explicit on the type level and available on hover, but on the term level you don't have to pass capabilities explicitly. Consider the following variation of the example above:

    interface Service {
      def doStuff(): Int
    }

    def useService(): Unit / Service = {
      println(do doStuff())
    }

    def main() = {
      try {

        useService()

      } with Service {
        def doStuff() = resume(1)
      }
    }

The type of `useService` says that it takes no parameters and returns `Unit` _using_ `Service`. At its call site the service has to be provided. The program prints ""1"".

One thing you seem to neglect is that effect handlers give you access to the delimited continuation. This way you can implement generators, asynchronicity, exceptions, threads, coroutines, and stuff like this. In order to keep your sanity in presence of all this non-local control flow I believe a type-and-effect system is absolutely vital."
let's take a census of language creators?,11xjwwk,2023-03-21 23:18:13,"*UPDATE: ok, there is* [*the link*](https://forms.gle/aRSUksDrwEtFxTrk8)

*UPDATE2: Let's start the survey!*

*UPDATE3:* [*the results table*](https://docs.google.com/spreadsheets/d/1lGGZzfoqlkUCvFaO3GyWng6haLWPeYU80_aKL24DCW8/edit?resourcekey#gid=1348035337)

*UPDATE4: If you have not kept the link to edit your response, fill free to submit new version. It's not hard to find and delete duplicates later.*

I love the data and I'm interested in language ideas. I'm reading this subreddit for a while and yet I find new languages from time to time I haven't heard of.

Or sometimes I remember somebody mentioned an interesting concept, but can't find it.

So I propose to take a census of language creators here. I've created a google form for that.

What are your thoughts about it? Are you interested in it? Would you participated?

Share the questions to ask and answer options.

Here is the current questions and possible answers. I'm not publishing it's right away to make everybody answering the same form when the questions and answers are finalized.","Additionally:

- Not every language targets LLVM for its backend. Consider maybe adding options for other backends, such as GCC or libgccjit, or ""integration with <other> compiler backend"", as options?
- consider adding an option for languages that are compiled to an intermediate language (source-to-source compilation) for the ""what stage is completed?"" question
- do you want to add some multiple-choice questions about any standard library, including whether there is passthrough support to another language's stdlib such as C/C++'s, if the language provides its own stdlib, etc..?
- typo in _operatol overloading_
- a question on whether or not `goto` is a feature would be interesting!
- a question on whether code formatting is a part of the language specification or not would be interesting!"
Barn Programming Language!,11pp9hu,2023-03-13 04:42:29,"Hi everyone 👋

I was working on a brand new programming language for like 6 months with a little break, the name of it is Barn. I've started to write Barn like at the beginning of October because I was really bored and didn't have any goal.

But more about Barn, It's an compiled programming language to C++. For now it is in 0.1v BETA version, which means it don't pointers, arrays and also format arguments for now, but I want to do them in future. Now im working on 0.2v which is improving everything that I've done wrong in the first version like ugly if conditions, new auto type for variable and some more stuff. Syntax is very easy and here's the example of it

@import ""std.ba""

fun main() {
   println(""Hello World"")
}

I will really appreciate if you will look at mine project and maybe give a star on my github repository, Thanks y'all 🦧

Here's some important links:
🌎 Website: https://barn-lang.github.io/barn-docs (where u can find link to our discord server where i update the little barn community on some changes)
🗿Github Repository: https://github.com/barn-lang/barn
🦕 Discord server ""Barn Enjoyers 🗿"": You can find the link on barn website

Have a nice day/night! 👺","I took a look at the code and it looks like the compiler does a very literal line-by-line translation from your language to C++

Of course, this makes the compiler really fast, but it also means you're restricting yourself to all the limitations that come with C++, like the limited type inference, having to forward declare stuff, etc

If you first build the AST entirely and do operations on that before you start generating code, you're going to be able to move your language further away from C++"
Why is Strangeloop (the conference) ending?,112bnl6,2023-02-15 01:57:32,Does anyone know the real reason? I can’t find an explanation.,"I don't know how reliable this is, but I heard from a friend it's basically organized by one guy and he didn't want to keep running it"
Macros in 22 languages,10dfzhn,2023-01-16 22:32:00,,How on earth is Common Lisp missing from this list?
"Features you've removed from your lang? Why did you put them in, why did you take them out?",104rug8,2023-01-06 19:26:39,"I removed most of my misfeatures in the planning stage but I put truthiness in and it's for the chop.

I originally thought that although it has potential to perplex people, it wouldn't do so in a language where (I expected) it would be used quite a lot, and everyone would be used to it. Well, dogfooding has persuaded me that it doesn't come up that much. The language is meant to be small and written on YAGNI principles. Farewell truthiness.

You?","In the early drafts of [Letlang](https://letlang.dev), I had the goal to add an equation solver. I got rid of that because:

 1. that's above my pay-grade
 2. that might be a performance killer
 3. it may not be that useful compared to proof assistants like coq

I also removed infinite sets because with my type system, you could easily introduce the Russel Paradox, and that was a big no no.

I talk about this in more details in [this article](https://david-delassus.medium.com/letlang-road-to-v0-1-61c8fd661e05)."
What are you doing about async programming models? Best? Worst? Strengths? Weaknesses?,zfa7h9,2022-12-08 03:02:16,"I've been spending a lot of time professionally with C#'s async systems and its various warts, as well as recreationally with Rust's async innovations, and it has given me food for thought w.r.t. language design.

For instance, cancellation seems awkward. Rust handles it elegantly through its explicit polling and deterministic deconstruction system.

But first-class async programming seems to me really to be a special case of coroutines (isn't everything) and for instance Ruby has introduced a scheduling mechanism for its green threads, which are themselves (to my knowledge) built on top of its well-developed coroutine and block system.

JavaScript to my knowledge goes possibly the worst way with continuation passing.

Erlang and Go use each their own systems of green threads sleeping on channels.

Is there a local maximum in this design landscape? What are you doing with your language? What works, what doesn't?","> But first-class async programming seems to me really to be a special case of coroutines (isn't everything)

[Koka](https://github.com/koka-lang/koka/blob/v1-master/lib/std/async.kk) and other languages implementing Algebraic Effect Systems make everything a user-defined case of coroutines: [`async` is just another effect/Monadic type](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/asynceffects-msr-tr-2017-21.pdf).  
Zig does something similar by having [first class stack frames](https://ziglang.org/documentation/0.10.0/#Async-Functions), making all function calls possibly asynchronous.  
Both approaches allow you to write the async/event/coroutine/greenThread/actor/reactor/whatever runtime in user land, as libraries.

In my own experiments I'm trying my best to have an optimised approach like Koka's: it answers the ""what colour is your function?"" question by giving you a rainbow."
How to JIT - an introduction,z8ma8t,2022-11-30 16:41:09,,"JIT is pretty fascinating, there's a whole lot more to it than this handy little article covers, but getting a block of executable memory is a nice thing for developers to know how to do.

Once you have that, there's a huge area of knowledge that you can tumble through: method-based or tracing-based? How much optimization (and what kind) are you doing up front and what is done incrementally? How do you ""escape"" from JITted code back to managed code? Are you doing callsite caching? It's really a huge and interesting topic."
Why I am switching my programming language to 1-based array indexing.,ye1ffa,2022-10-26 23:51:45,"I am in the process of converting my [beginner programming language](https://easylang.online) from 0-based to 1-based arrays.

I started a discussion some time ago about [exclusive array indices in for loops](https://old.reddit.com/r/ProgrammingLanguages/comments/x8xtou/asking_for_opinions_on_the_best_way_to_specify_an/)

I didn't get a really satisfactory answer. But the discussion made me more open to 1-based indexing.

I used to be convinced that 0-based arrays were ""right"" or at least better.

In the past, all major programming languages were 1-based (Fortran, Algol, PL/I, BASIC, APL, Pascal, Unix shell and tools, ...). With C came the 0-based languages, and ""1-based"" was declared more or less obsolete.

But some current languages (Julia, Lua, Scratch, Apple Script, Wolfram, Matlab, R, ~~Erlang~~, Unix-Shell, Excel, ...) still use 1-based.

So it can't be that fundamentally wrong. The problem with 0-based arrays, especially for beginners, is the iteration of the elements. And the ""1st"" element has index 0, and the 2nd has index 1, ... and the last one is not at the ""array length"" position.

To mitigate this problem in for loops, ranges with exclusive right edges are then used, which are easy to get wrong:

Python:
	range(0, n)

Rust:
	0..n

Kotlin:
	0 until n (0..n is inclusive)

Swift:
	0..< n (0..n is inclusive)

And then how do you do it from last to first? 

For the array indices you could use iterators. However, they are an additional abstraction which is not so easy to understand for beginners.

An example from my programming language with dice roll

0-based worked like this

	len dice[] 5
	for i = 0 to (len dice[] - 1)
	    dice[i] = random 6 + 1
	end
	# 2nd dice
	print dice[1]

These additional offset calculations increase the cognitive load.

It is easier to understand what is happening here when you start with 1

	len dice[] 5
	for i = 1 to len dice[]
	    dice[i] = random 6
	end
	# 2nd dice
	print dice[2]

random 6, is then also inclusive from 1 to 6 and substr also starts at 1.

Cons with 1-based arrays:

You can't write at position 0, which would be helpful sometimes. A 2D grid has the position 0/0. mod and div can also lead to 0 ... 

Dijkstra is often referred to in 0 or 1-based array discussions: [Dijkstra: Why numbering should start at zero](http://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.PDF)

Many algorithms are shown with 0-based arrays.

I have now converted many ""easylang"" examples, including sorting algorithms, to 1-based. My conclusion: although I have been trained to use 0-based arrays for decades, I find the conversion surprisingly easy. Also, the ""cognitive load"" is less for me with ""the first element is arr[1] and the last arr[n]"". How may it be for programming beginners. 

I have a -1 in the interpreter for array access, alternatively I could leave the first element empty. And a -1 in the interpreter, written in C, is by far cheaper than an additional -1 in the interpreted code.","*Should array indices start at 0 or 1?  My compromise of 0.5 was rejected without, I thought, proper consideration.*     —Stan Kelly-Bootle"
The Cost of Safety in Java,y0nms5,2022-10-11 03:27:59,,"> When accessing an object in Java, it first makes sure that the object is an actual object and not null

Afaik, if hotspot detects that the object is usually not null, it will skip this check, instead taking a fault and recovering from it in the event that the object is null.  So I would expect this does not actually add any overhead.

E: apparently the measurements were taken using graal nativeimage, where this optimisation is not performed."
Vinci — a toy ML shading language for Vulkan,xmpb8r,2022-09-24 18:31:22,"Hi all!

For my master's thesis, I was working on [a compiler for a toy functional shading language](https://github.com/swtwsk/vinci-lang) that outputs SPIR-V and thus is available to work with Vulkan. I want to drop it here, hoping it may interest some of you 🙂 and maybe spark some interest in SPIR-V-based language development :D

It's a very proof-of-concepty version, so it's not sufficiently optimized yet and generates an awful lot of unnecessary operations, but I'm still proud that it's working as it should 😃 Oh, for now, Sampler support is broken but it's due to poor optimization of entry point logic that will have to be addressed.
I wanted to check if it is possible to express a shader in a functional manner, and then compile it so it could run on the GPU. Moreover, the language is supporting type inference and polymorphism, which was one of the main design aims.
Feel free to drop some comments, and – even better – contribute, if you'd like to!","This is so cool! I myself am working on a SPIR-V shader language, but the repo isn't public yet. I need to write documentation and finish working on the compiler itself.

If you want to go back to working on this, send me a DM and I might be able to point you in the right direction; though I'm still learning SPIR-V myself."
erg: A Python-compatible statically typed language written in Rust,wo9u8j,2022-08-15 00:05:04,,"I have no criticisms of the language, but, how shall I put this ... your logo is ... *nice*."
A language you feel the most productive with?,vv27ui,2022-07-09 21:54:50,"And (optional), what set of features makes you feel productive/horny when writing something in that language?","> And (optional), what set of features makes you feel productive/**horny** when writing something in that language?

If programming makes you horny, you should probably go touch some grass.

Jokes aside, I feel this is a better place for /r/askprogramming (i.e. there's zero interaction from OP, and the question doesn't really foster a discussion), but it's been 8 hours since this post is up so I'll give it a pass this time."
Uniqueness Types and In-Place Updates,vc2c92,2022-06-14 20:25:37,,"For a language I’m designing, I plan to have some sort of linear/affine/unique type system (undecided for now) as well. Just like you, I'm also struggling with how to go about it in a way that is both ergonomic and doesn't complicate the type signatures of higher-order functions.

At the moment, I've been toying with the idea of either explicitly annotating unique types in type signatures with some sort of sigil/symbol (just like your example with `*a -> b`) or using a system similar to [Idris 2’s multiplicities](https://idris2.readthedocs.io/en/latest/tutorial/multiplicities.html#multiplicities) where you (explicitly or implicitly) annotate the number of ""uses"" a variable is allowed to have. Do you know of any other approaches by other programming languages? You mentioned Clean in your blog, so I guess I could look into that language.

In any case, it's nice to read someone else's thoughts on this topic. Thanks for your insight!"
Any languages doing anything interesting with allocators?,szk64o,2022-02-23 23:31:31,"So far, I've heard of these advances related to allocators:

* Cone's enables the user to make [custom allocators](https://cone.jondgoodwin.com/memory.html) very easily, hopefully leading to a nice ecosystem of allocators to choose from, one day.
* Odin allows us to temporarily switch the current allocator to a specified one, via its [context system](https://odin-lang.org/docs/overview/#allocators).
* Vale's [regions](https://vale.dev/guide/regions#bump-calling) allow us to specify a [region memory strategy](https://verdagon.dev/blog/zero-cost-refs-regions#region-memory-strategy), to temporarily switch the current allocator, while maintaining memory safety.

Any other languages doing something interesting with allocators?",[Zig](https://ziglang.org/) has custom and explicit [allocators](https://ziglang.org/documentation/master/#Choosing-an-Allocator) (explicit means there should be [no hidden allocations](https://ziglang.org/learn/why_zig_rust_d_cpp/#no-hidden-allocations)).
IDEs and Macros,qz3725,2021-11-22 04:06:48,,"Good article, I was once, in the context of Common Lisp, thinking about designing a protocol where programmers would provide additional info libraries to their macros and reader macros libraries so that a basic thing like syntax highlighting would work. So with these in mind:

* It'd naturally be a subclass of macros which would be supported
* Programmers would have to provide additional info (e.g. how to semi-parse reader macros, indentation style for macros, etc.)

I still don't know where to begin with that protocol, it's such a hard problem."
"What are other ""better and cleaner C++"" contenders?",qtp4bq,2021-11-14 20:46:55,"Basically, my ideal goal of my hobby language project is to create a ""better and cleaner"" C++.For that I like to look at other languages with similar goals. Of the ""big languages"", only Rust and Swift come to mind. Then maybe Jai by Jon Blow. Other than that no other programming language project comes to mind, at least that I know of.

There are odin and zig of course, of which I also let myself be inspired, but they are rather a ""better and cleaner C"", though then again the line could be blurry.

Basically, I'm looking for programming language projects where the languages are statically and strongly typed and compile to machine code as a rough description. Think of ""languages which are similar to and/or could theoretically replace languages like C++, Swift, Rust"".

EDIT: I'd like to add that I'm also interested (and even somewhat more interested) in smaller languages like the ones which are posted here from time to time. Because there people tend to also do interesting stuff.",https://dlang.org/
How Statically-Typed Functional Programmers Write Code,qa7r73,2021-10-18 05:07:49,,"One of the authors (Justin Lubin) had a twitter thread lately about the paper:

https://twitter.com/jplubin/status/1449159815058522115"
"Rust implementation of µKanren, a featherweight relational programming language",pmmhwo,2021-09-12 13:01:59,,"So I went and found the minikanren homepage and read it and a hacker news thread from 2018, so that I could at least have some context that this belongs to a family of implementations of similar logic languages. I still have no bloody idea what this feature list is talking about."
Don Syme explains the downsides of type classes and the technical and philosophical reasons for not implementing them in F#,placo6,2021-09-10 08:35:57,,"(copy of my lobste.rs comment)

This is gold, bookmarked!  I have been trying to articulate a bunch of these tradeoffs, and apparently not being very convincing.

Another big one which I don't see mentioned -- from a language design point of view -- is the bifurcation of the language, with the compile time language starting out weaker, and gradually falling down a slippery slope (e.g. what happened to C++).

For example Rust appears to be following in C++'s path -- gradually opening up more of the language to `constexpr`.  It appears that a huge part, or even a majority, of C++ 11, 14, 17, 20 actually relates to increasing the increasing the scope of the compile-time language (`constexpr`), not the runtime language.

Rust is in the earlier stages of this process, i.e. I expect `const` contexts to get more powerful as people run into its limitations.

I think Zig got it right with full compile-time metaprogramming from the beginning -- it's one language, and not two (or three, as Rust as multiple kinds of compile time metaprogramming).

------

I'm not necessarily saying this is bad, but it's certainly more complex.

related threads:

https://news.ycombinator.com/item?id=26378141 and https://lobste.rs/s/lng3c0/announcing_rust_1_50_0 (Rust following C++)

https://lobste.rs/s/9rrxbh/on_types#c_qanywm and https://news.ycombinator.com/item?id=26378696 (Type checking vs. metaprogramming; ML vs. Lisp)

https://news.ycombinator.com/item?id=27026990 (debugging and profiling macros, Andy Kelley chimes in on the ""1000 backward branches"" rule in Zig)

https://news.ycombinator.com/item?id=21310029 (MyPy inhibits metaprogramming; it's basically a different language than Python.  I use it in Oil for predictable speed via translation to C++, not really for correctness)

----

(This issue is related to the compile-time debugging issue: because you have 2 languages, you would need essentially different debuggers for both compile time and runtime.  This isn't theoretical; plenty of programmers avoid C++ template metaprogramming simply because they do not know how to debug it with the only tool they have available: the compiler's error messages.  Well, I think compile time asserts have helped but  I think those didn't even exist when template metaprogramming became a thing -- there was a trick to roll your own!)

Also Syme is making a ""slippery slope"" argument here too ... I think this is an issue that fundamental to computation itself.  Maybe it's a ""Turing tarpit"", although I'm not sure that's the exact concept and/or the best name.  I'll have to think about it."
The problem of effects in Rust,paxzki,2021-08-25 06:33:22,,"I'd like to advertise some work we've done in this area.

[This paper](http://ps.informatik.uni-tuebingen.de/publications/brachthaeuser20effects/) formally explores what OP proposes as the second solution: To use a control effect you need a capability and closures close over these capabilities.

[This unpublished draft](http://ps.informatik.uni-tuebingen.de/publications/schuster21stack/) combines control effects with region-based resource management. The key idea is that both effects and regions are ""all about that stack"" and we can use the same type-level machinery to keep track of both.

EDIT:

Concretely, in [Effekt](https://effekt-lang.org/), consider the following program:

    effect Throw[A](message: String): A

    def filter[A](list: List[A]) { pred: (A) => Boolean }: List[A] = {
      list match {
        case Nil() => Nil()
        case Cons(x, xs) => if(pred(x)) {
          Cons(x, filter(xs) { (y) => pred(y) })
        } else {
          filter(xs) { (y) => pred(y) }
        } 
      }
    }

    def main() = {
      try {
        filter([4, -5, 9]) { (x) =>
          if(x < 0) {
            do Throw[Boolean](""below zero"")
          } else {
            mod(x, 2) == 0
          }
        };
        ""success""
      } with Throw { (message) => message }
    }

It filters a list for even numbers, but throws an exception when a number is below zero. The closure uses `Throw` which will always throw to the handler at the _definition site_."
What do you think of variable shadowing?,o35kvk,2021-06-19 09:53:09,"In some languages (e.g., Rust) variable shadowing is [considered idiomatic](https://doc.rust-lang.org/rust-by-example/variable_bindings/scope.html); in others (e.g., C++) it's [frowned upon](https://rules.sonarsource.com/cpp/RSPEC-1117) and may throw a warning; in at least a few (CoffeeScript?), I believe it's banned outright.

This subreddit [discussed the issue](https://www.reddit.com/r/ProgrammingLanguages/comments/a5trqe/pro_and_cons_of_variable_shadowing/) in 2018, but I thought it might be worth talking through again.  From that thread and other reading, it seems that the primary argument against variable shadowing is that it can sometimes lead to bugs if code refers to the wrong variable; the primary arguments in favor of variable shadowing are that it means you can name local variables without worrying about name clashes with the outer scope and that it makes working with immutable variables easier (because you can use a new ""version"" of the variable without needing to come up with a new name).  And that can encourage more programmers to use immutable variables.

Any other key points?  Any horror stories about how the presence or absence of variable shadowing made a big difference to a project?  Any other tradeoffs to consider?","Its usefulness to me isn't much about reusing names per se, but about ""access control"" from inner to outer scopes —to keep inner code accidentally referencing to variables from outside the block.

You could imagine other, more explicit mechanisms to limit lexical scoping with access control. JavaScript's `with` comes to mind."
The Siren Song of Declarative Programming,n7iudh,2021-05-08 14:12:09,,"I think pitting functional vs declarative/relational is a bit of a false dichotomy. miniKanren falls into both categories, in the sense that it's sort of like Prolog, but forbids any impure/extralogical operations like Prolog's cut, giving you the best (and sometimes the worst, depending on how you like to program) of both worlds.

Declarative is a sliding scale that is relative to the problem domain, and isn't a paradigm in its own right imo. In a sense, Haskell's State monad, when combined with do notation, is like a declarative language for writing **imperative** programs. The problem space is naturally imperative, and a declarative language for that problem is one that expresses imperative operations cleanly whilst still remaining composable and easy to reason about."
Safe dead code removal in a pure functional language,l98q4p,2021-01-31 17:07:40,,How often does dead code happen in other types of projects?
Pyxell 0.10 – a programming language that combines Python's elegance with C++'s speed,jlnqf4,2020-11-01 02:45:03,"[https://github.com/adamsol/Pyxell](https://github.com/adamsol/Pyxell)

Pyxell is statically typed, compiled to machine code (via C++), has a simple syntax similar to Python's, and provides many features found in various popular programming languages. Let me know what you think!

Documentation and playground (online compiler): [https://www.pyxell.org/docs/manual.html](https://www.pyxell.org/docs/manual.html)","I kinda expected examples in the github readme and even on the website I had to click ""Get started"" to get to examples.

I think some 10+ lines of code, maybe even presenting some advanced features would really help sell it more.

And you'll have to do some job on the marketing in general to position your language, as the similar sounding ""pyxel"" is also a python project.

I think, I'll toy around with it some more. I always toyed with the idea of a subset of python getting compiled."
Incomplete List of Mistakes in the Design of CSS,ilba09,2020-09-03 01:29:40,,"CSS is so deeply flawed, and the entire HTML / JS / CSS / SVG stack so fundamentally flawed, it should be completely replaced. Instead of giving you a nice basic set of primitives, and a combining operator, the CSS model tries to do all things for all people with hundreds of options and keywords.  A very complex system to produce simple results."
Is Futhark getting faster or slower?,hjbcwh,2020-07-01 22:34:06,,"For [Flix](https://flix.dev/) we run a build script several times a day and [collect the results in a Grafana view](https://arewefast.flix.dev/d/nFYOg5FZk/flix-compiler-overview?orgId=1&from=now-1y&to=now). I find that Grafana makes it easy and fun to interactively explore changes to compiler performance (and the performance of the compiled programs).

What is missing from our setup is the ability to measure the performance impact of each commit and pull request. The latter would be especially nice when merging in a PR. Right now, we merge code in, and then only retroactively discover if we have ""broken performance"".

I wonder if there is already some open source tooling for monitoring how the performance of compilers (and their compiled programs) evolve over time?"
What do you dislike about class inheritance?,he2wmh,2020-06-23 07:02:30,"Over the last decade or so it has become more widely accepted that inheritance, while in theory it might seem brilliant, is in fact a very faulty concept. People have found many reasons for it being a bad idea, however what I'm curious about is what is the most widely accepted issue. Normally I'd have made a poll, but I feel as though it's very easy to miss a reason in this situation, so I'm simply asking you what you find the biggest issue is.

For me personally it's two things:

 * Not being able to make an existing type implement an interface. Swift does let you do this as a part of its protocol-oriented style, and Scala's implicits can, to a very verbose point, simulate this behavior, but the majority of OO languages do not have a simple way of implementing a sort of extended polymorphism for existing, potentially final, classes. 

 * Encapsulation of data. While encapsulation of methods can be annoying, encapsulation of data can result in large memory buildup that you have no control over, yet have to live with if you want your class to be a subclass of said bloated class.

I do however believe that I'm in a minority here, which is probably a result of me not having used a lot of class inheritance in the first place.

Now, there definitely are languages like Dart that put their own spin on things, however this discussion is more about the more ""stereotypically OO languages"", like C++, C#, Java, Python, that were made while inheritance was a popular idea.","When class hierarchies get more than 2 classes deep, accessing inherited methods starts feeling like spooky action at a distance. My preferred alternative to inheritance is a getter interface to access a ""parent"" variable. Doing it that way is just as powerful and more explicit, though a bit more verbose."
PureScript to Python Backend: Writing Haskell and running in Python,f4qmu6,2020-02-16 21:27:30,,"I'm now working on

- testing the compiler and, 

- porting some purescript libraries using JavaScript FFI to Python.

Certainly I wonder if any one want to contribute or collaborate to work together, even tidying up how to get started is a very good task."
A question for you all: What's your least favourite thing about your favourite programming language?,at7eso,2019-02-22 03:39:01,,"C++: lots of existing legacy and pretty slow rate of change in cleaning it up. The coroutines ts uses `co_await` rather than `await`, etc, to avoid naming conflicts with existing code; identifiers like `_ThisOne` and `this__one` are reserved (your code is bad if it uses them), which afaik is only a requirement to avoid conflicts between standard library internals and user macros; modules haven't yet been accepted into the language after years and years of work because there is pushback against it enforcing any particular structure and pushback because the current proposed compromise is fairly limited as a result.

Oh, and compile times. Including `<iostream>` adds like 100ms to the time it takes to compile hello world versus forward declaring `write()` and calling that instead directly."
Futhark 0.9.1 released - now with CUDA backend,aoicht,2019-02-09 00:42:14,,"> Hence, we can perform an in-place update on the result transpose if and only if we can perform an in-place update on the argument to transpose.

If the argument array is shared, then won't you still generate a fresh (unique) result array which can be updated in-place?"
Garbage collection with zero-cost at non-GC time,15o1wxa,2023-08-11 15:24:57,,"> I think it's a good idea to separate GC and non-GC concerns as much as possible, in order to make them separately as fast as possible

But if the mutator can do a small amount of extra work in order to make the collector much faster, isn't that a worthwhile trade?"
Beyond functional programming: a taste of Verse. Simon Peyton Jones & Tim Sweeney,14k9c5x,2023-06-27 17:59:22,,"Has anyone looked at verse in depth? As more of a distributed systems/systems person than a pl person, I’m struggling to figure out how one would efficiently map verse onto our current C-asic processors. Especially once you need to run it as a dynamically reloadable set of programs in a large distributed system like a metaverse."
What are some strategies for ensuring correctness and fewer errors in dynamically typed languages?,13p3hbq,2023-05-23 05:11:09,"I've been spoiled by functional languages with good type systems as it helps weed out so many errors during compile time. The type system is also a great way to model a particular domain, which helps build large programs incrementally. 

Are there equivalent strategies employed by dynamically typed languages that help towards the same goals?","One interesting strategy is available in Prolog, which is a dynamically typed language: In Prolog, you can *ask* the system whether certain cases can arise *at all*, by posting *queries* on the toplevel, to which the Prolog system will respond in the form of *answers*.

This is a bit similar to what other languages (also functional languages, and also statically typed functional languages such as Haskell) call a ""REPL"", a *read-eval-print-loop*. However, in Prolog, the ability to post queries goes far beyond the ""eval"" of functional languages, because in Prolog, you can *generalize* parts of the query by using logic variables instead of concrete values. Using logic variables asks for solutions *in general*, and thus covers an *infinite* number of concrete cases.

Hence, if you ask for example ""Can a case of this form arise *at all*?"", and the Prolog systems answers ""No"", then you can rely on *all* more specific queries to also fail. This reasoning works as long as you program in the pure core of Prolog, where such logical reasoning is admissible.

For illustration, here is an example: Suppose we have defined a Prolog predicate called `length/2`, which is supposed to relate a list to its length. We can write a **test case** for the predicate, in the form of a query. For example, we know that the list `[a,b,c]`, which we can write equivalently as `""abc""`, is supposed to have length 3. So, we ask the system: Is that actually the case, does `length/3` yield the correct answer in this case?

    ?- length(""abc"", 3).
       true.

Yes. And now, we can *generalize* this, by asking: What about **all** lists of length 3?

    ?- length([_,_,_], 3).
       true.

And now we can generalize this further:

    ?- length([_,_,_], L).
       L = 3.

From this, we see that the predicate behaves as intended for *all* lists of length 3, in that it correctly yields the length. With this single query, we have covered *infinitely many concrete cases*. The ability to express such extremely general test cases is a major attraction of logic programming languages like Prolog.

Note that this is conceptually different from ""QuickCheck"", which uses a number of random and *concrete* tests. A finite number of tests, no matter how large, cannot cover *all* cases that can arise, because there are infinitely many concrete terms that can be used as elements. Only the ability to reason about *all* cases that can arise gives us a useful correctness guarantee."
The Blissful Elegance of Typing Joy,13g54so,2023-05-13 10:49:53,,I think it's a perfectly cromulent article.
A Brief Interview with Common Lisp creator Dr. Scott Fahlman,ysmfym,2022-11-12 04:41:01,,">Hassam: What would be your advice to young people today who want to get into the field of designing programming languages?  
>  
>Dr. Fahlman: Don't! Unless you like to do this as a hobby. What I came to understand, after years of work on Common Lisp and the death of Dylan, the ongoing popularity of the hideous C++, and the rise of Java, is that programming languages don't become mainstream based on their elegance or their deep utility."
Books to better understand memory allocation.,yohzr7,2022-11-07 17:42:47,"I'm looking to understand how data gets stored in memory. I'm looking to understand memory allocation in a detailed fashion, in fact. Could y'all suggest names of books/ YT videos / research papers to help me get started? Thanks in advance.","The Garbage Collection Handbook is the most canonical book on this.

https://gchandbook.org/"
Are ideas presented here implemented anywhere?,ycejhy,2022-10-24 23:42:15,,"Dr. Sussman has a book called ""[Software Design for Flexibility](https://mitpress.mit.edu/9780262045490/software-design-for-flexibility/)"" which goes into more depth. There's also the paper [""the Art of the propagator""](https://www.researchgate.net/publication/38004425_The_Art_of_the_Propagator) which covers them.

Edit: updated the link"
Introducing the Esolang Motorway,xq0nfv,2022-09-28 10:50:08,"Hello all. I have just completed my first esoteric programming language (and first full language implementation), which is called Motorway.

You can view its [esolang wiki page](https://esolangs.org/wiki/Motorway) or the [Python interpreter on GitHub](https://github.com/Ninesquared81/motorway-lang). I'll also give a brief introduction below:

Motorway is a stack-based esoteric programming language based around the British motorway network. A program describes a route along the motorway network, with some motorways effecting the data stack, which are listed below:

&#x200B;

|M1|Increment top of stack|
|:-|:-|
|M4|Pop top of stack and print as ASCII character|
|M5|Pop top of stack and discard|
|M6|Push a new element to the stack, initialized to zero|
|M20|Read a single character and push it to stack|
|M25|Pop top of stack and jump to matching `M26` if zero|
|M26|Jump back to matching `M25`|
|M40|Duplicate top of stack|
|M42|Swap top two stack elements|
|M48|Add top two stack elements|
|M49|Subtract top of stack from next element|
|M60|Rotate top three stack elements like so: `... c, b, a` \-> `... b, a, c`.|

&#x200B;

There is more information at the links given above.","> A program is valid only if the route it describes is physically possible

LMAO i love it"
Build a WebAssembly Language for Fun and Profit Part 2: Parsing,wxqyej,2022-08-26 06:04:18,,"The vast majority of tutorials on creating your own programming language spend way too many pages on lexing and parsing and then almost no time on code generation. In that spirit, let's start a pool. Taking bets on the following:

1. There won't be another post.
2. There will be another post, but it will only cover a trivial, naive implementation that doesn't even generate the full language that was parsed so far.
3. There will be another post, but it will be too dense and assume a bunch of prior knowledge, even though very little prior knowledge was assumed in the previous two sections.
4. There will be the 3 or 4 more posts necessary to cover code generation to the same simplicity and depth as lexing and parsing were covered."
I wrote a simple stackless lisp,wcom1h,2022-07-31 21:21:39,"Always wanted to understand how stackless language implementations like Chez Scheme work, so [I wrote my own](https://github.com/divs1210/simple-stackless-lisp)!

It's a simple Clojure-like lisp implemented in Clojure.

It has infinite recursion, continuations, and first-class macros.

Sharing it because it might help others understand how `call/cc` and infinite recursion is implemented in Scheme.",What does stack less means in this context
How to create fundamental libraries for my language?,w59hdf,2022-07-22 20:33:07,"There are many fundamental libraries required for a language that directly interacts with the operating system. 

For eg, taking C as an example, when I want to print something, I use `printf`. That is internally implemented using `puts`(?) with some additional features. But `puts` again can't be implemented using anything that is already present within the language. It somehow has to communicate with the OS to printout the buffer.

But I'm not getting how to do this. If I take Ubuntu as the OS, does it provide some apis, so that I can call them from my version of `puts` to print the buffer? Where can I find these apis and their documentation?

I thought of using `syscall` instruction with appropriate number directly. But when I saw assembly generated by gcc, for `puts` it is doing an actual function call instead of just emitting `syscall`.","Assuming Unix/Linux the syscall you are looking for is 'write'. 'puts' writes it's output to 'stdout' and the file descriptor for 'stdout' is 1.  


Digging through glibc to try and track all this down is a nightmare of #defines, macros and scary bad dreams. I'd look at a simpler to follow libc implementation like musl  


https://git.musl-libc.org/cgit/musl  


Don't be put off, all that locking and is to make it threadsafe and the calls to fwrite are so the output is buffered, Have fun, good luck.  


If you aren't already aware of this book [https://en.wikipedia.org/wiki/Advanced\_Programming\_in\_the\_Unix\_Environment](https://en.wikipedia.org/wiki/Advanced_Programming_in_the_Unix_Environment)  
find a copy and read it, its old buts there is A LOT of useful stuff in there."
The Future of Programming Languages,vqiumr,2022-07-03 22:29:43,,Nice video. I like the calm and methodical explanations.
SNOWBOL -- Need your opinion on a future project I'm planning on building. Would you use a language which is basically just COBOL but with simplified syntax?,uaa044,2022-04-24 01:08:37,"Let's use this [factorial example](https://www.ibm.com/docs/en/cobol-zos/4.2?topic=storage-example-sections) from IBM, in this ""new"" language the syntax for that program would look something like this:

    // SNOWBOL line comment
               
    IDENTIFICATION.
      program-id. factorial.
    ENVIRONMENT.
    DATA.
      working-storage.
        numb pic number value 5.
        fact pic number value 0.
      local-storage.
        num pic number.
    PROCEDURE.
      move numb to num.
    
      if numb = 0
        move 1 to fact
      else
        subtract 1 from numb
        call 'factorial'
        multiply num by fact
      end-if.
    
      display num '! = ' fact.
    end.

This language could easily be transpiled to COBOL -- since the syntax is so close to it -- to take advantage of current compilers, or it could serve as a simplified ""replacement"" for COBOL.

I've been reading about the issues of replacing COBOL, one part of it is because no other language behaves the same way COBOL does, so the only realistic way to ""replace"" it is to either extend and modernize COBOL itself, or create a new language that behaves the same way.

A new language that has very similar syntax, semantics and behavior could be used to replace a COBOL codebase without having to rewrite all the complex behavior to a completely different language like Java or C#, while also allowing to more easily improve the ecosystem and to leave some of the legacy stuff behind.

This would be like what Typescript is doing to Javascript, JS has some weird quirks and limitations due to the way it was designed and Typescript is trying to fix that while still being able to run in the same places by transpiling to Javascript.

What are your opinions on this, I'm planning on building this language soon (after I finish the code editor I was building), would you change or improve anything in the syntax above?

What things would be required in the language for you or your company to use a ""simplified COBOL""?

P.S.: I don't see COBOL being replaced by any language available currently, in my opinion it can only be replaced by a language similar to itself, so this language could end up being a good thing long term, I hope I don't end up getting downvoted or making someone angry, it sounds weird saying this, but I like COBOL and want to help improve the ecosystem, I hope that's something I can achieve.","Just so you're aware, there has already been a [SNOBOL](https://en.wikipedia.org/wiki/SNOBOL), which may make talking about your language somewhat confusing."
Speed up the OCaml GC by prefetching during marking (GitHub PR),orjeto,2021-07-26 04:35:24,,"When I worked at Xamarin, we got a decent speedup doing this in Mono’s GC (SGen). On Intel chips, VTune was really handy for these low-level optimisations, to get a fine-grained look at caching & pipelining.

Improving cache use is a great source of “systemic optimisations” for a tracing GC. Caching can be the kind of issue where nothing shows up as a hotspot in a profile, because the cost is distributed everywhere, but when you make an improvement, sometimes the speedups are ridiculous. A lot of it comes down to heap layout, though, which can be hard to change unless you plan ahead and decouple things very carefully.

I’m still waiting for a language with a type system that makes it easier to write a GC correctly. Get on it, folks! lol"
Why Least Fixpoints,mzjq8n,2021-04-27 16:25:42,,"Of course we care about greatest fixpoints! Best example is for giving semantics to infinite computations (think reactive programs) or infinite data structures (like streams instead of lists).

Another example is bisimulation, a behavioral equivalence that can be defined as a greatest fixpoints and that is commonly used in process algebra, but also in lambda calculus"
Release of Oil 0.8.8,m92q3y,2021-03-20 16:22:27,,"There is some ""behind the scenes"" info about the Python -> C++ translation process here.

I'm interested in any parallel efforts.  In fact one person said they made great process on translating Oil to Nim, and another person wanted to rewrite in Rust.  That is all welcome.

I think the current strategy will work but it's not easy.  Rewriting isn't easy either!  But I won't be at all surprised if there's someone out there that can do a better job :)

*How to Rewrite Oil in Nim, C++, D, or Rust*: http://www.oilshell.org/blog/2020/07/blog-roadmap.html#how-to-rewrite-oil-in-nim-c-d-or-rust-or-c"
Are there any language where reactive programming are first class constructs?,kqxuek,2021-01-05 20:46:54,"Reactive programming as defined at https://en.m.wikipedia.org/wiki/Reactive_programming

TLDR; reactive programming is a declarative programming paradigm concerned with data streams and the propagation of change","Swift (Apple's programming language) has something like that. They have this notion of wrapped properties, where a member of a type can be wrapped in a helper type that enables additional functionality. Syntactic sugar is then used to distinguish between the wrapped value and the wrapper value, and their reactive framework (Combine) is built around this mechanism."
At What Point Does A Design Pattern Become A Language Feature?,j6a904,2020-10-07 02:13:15,"While contemplating how useful compile-time dependency injection would be as a built-in language feature, I came to the realization that a lot of design patterns have ended up becoming fully-supported language features. Amongst the most notable ones are iterators, such that now most modern languages have for each loops, decorators, which exist in an annotation form in JavaScript, Python and other, mostly dynamic, programming languages, factories, from which we have Dart's factory constructors, singletons, from which we have Kotlin's object types, etc. With that said, what do you think is the criteria for a design pattern to become a language feature? If it's a matter of how much boilerplate is removed, let's not forget that C++, Java and C# merely removed 2 extra statements when using iterators by adding for each loops. Additionally, do you think there currently are any design patterns which we could expect to see as language features in the near future?","I would argue that they're related but there's a strict line.

A language feature can enable, or limit, a pattern.

At the same time a language feature can exist with a specific pattern in mind, or expose itself through an interface following a pattern.

For example. Iterators are a pattern. You create a ""thing"" that represents a pointer going through something. It allows you to do external iteration (pulling things out of its container) while still keeping some functional hygiene (iterators are lenses to the contents of the collection, so you don't have to move them in and out). Now in languages were for loops take anything that is iterable, and behind the scenes uses an iterator to get individual elements, then iterators are critical to a language feature, and therefore whatever the language defines as an iterator.

Another example. Returning a sum type, a discriminated union, to handle a function that may return an error, is a pattern, enabled by a language feature (discriminated unions, though they can also be implemented as a pattern). But having a language feature, like Rust's `?` makes the pattern easier to use."
Diagrams for Composing Compilers,gnvdtb,2020-05-21 19:47:15,,"Diagrams. 

Composing.

Sounds like a job for category theory."
"Solutions to the ""turbofish"" problem?",fe3yg9,2020-03-06 07:08:28,"Anyone familiar with Rust code of reasonable complexity has encountered the turbofish `::<>`. This devilish little sequence exists to deal with ambiguity in the parser: without knowing the kind of the symbol precedes the left angle bracket, we have no way of determining if the angle brackets denote a list of generic arguments, or simply two separate comparison operators.

The [canonical example](https://github.com/rust-lang/rust/blob/master/src/test/ui/bastion-of-the-turbofish.rs), found in Rust's own test code is

    let (oh, woe, is, me) = (""the"", ""Turbofish"", ""remains"", ""undefeated"");
    let _: (bool, bool) = (oh<woe, is>(me));

In the preceding source, there is no way to know whether the angle brackets span a generic argument list without knowing whether `oh` is a generic type or not.

In order to deal with cases of ambiguity, rust requires the use of the scope operator `::` preceding a generic argument list (i.e. `Generic::<T, U>(foo)`). This is has been deemed one of Rusts ""ugliest"" bits of syntax.

My question is: without introducing a symbol table, which has its own host of issues, is it possible to remove this ambiguity?

Options that I've considered:

1. Use `[]` for argument lists. This is ambiguous with an array's index operator.  e.g.`type[index]`
2. Use `()` for argument lists. This is ambiguous with a function call, and furthermore looks really confusing. e.g `type(index)`
3. Use some other pair of delimiters. I've tried pretty much every other common key on a US keyboard, and none look good at all. `/` or `|` are alright, but have the same problem as `<>`.
4. Add a character before the list to disambiguate. This is what Rust (`Generic::<args>`) and D (`Generic!(args)`) did, but it looks noisy visually to me. Particularly, the overloading of the scope operator for generic arguments is egregious from a semantic point. Using `!` could conflict with macros, depending on your choice of postfix (if any).
5. Some kind of disambiguation between a type and a variable. I considered a few options:
  - Require that every type starts with a capital. Either this makes for some gross standard builtin types -- `I64`? bleh. `Int64` is alright, but I prefer the abbreviated form considering how often it gets typed -- or you special case built-in types, which is annoying. And limiting a user's choice to simplify parser complexity doesn't seem like a good trade-off.
  - Add a character when in contexts that require disambiguation. I've considered `!` and `'`. Rust uses the latter for generic lifetime parameters consistently, partially to disambiguate between generic type parameters in the same list (e.g. `<'a, T>`.) If I were to ever want to add some kind of lifetime analysis, it would be nice to mirror the usage of `'`. That leaves `!`, or perhaps another character like `~` or `#`, but nothing jumps out at me as the obvious winner.

I hope I've covered all the relevant solutions/cases. If you think of any, please share them in the comments. I'm curious to see how much thought people have given to this particular problem.","The `[]` and `()` options are ambiguous with array indexing and function calls, but only in the same way as normal operator overloading. That is, if you can ensure that you can parse a type using your expression grammar, then you can decide whether something is a type or a term in a later phase, at the same time as you would decide which overload to call.

This is also nice if you want to pass values around at the type level, or any other dependent-types-like syntax. Notably, because the turbofish approach lets the parser to expect types in a type parameter list, Rust's const generics then need a second disambiguator to force `<` to be treated as a comparison operator again- surrounding the expression with `{}`.

Your parser will catch fewer errors this way (e.g. `let x: 3 = ...` might now parse) but that may not matter so much either. The same errors will be detected at a higher level of abstraction, so you may even be able to provide more helpful diagnostics."
Rust as a gateway drug to Haskell,d9c235,2019-09-26 08:31:10,,"The title seemed clickbaity and dogmatic, but the write-up was a useful compare and contrast of semantics and features. Very nice."
Strings · Crafting Interpreters,9iiw71,2018-09-24 23:29:41,,"> If Lox was your language, what would you have it do when a user tries to use + with one string operand and the other some other type? Justify your choice. What do other languages do?

In my language, even though the language is untyped and even though there are tags stored with the object representations (to distinguish between string, integer, tuple, etc), I've decided that those tags are an implementation detail that should only be used to ensure memory safety and I have provided no way to access them using the language semantics.

There is no `is_string` function and there is no way to implement it.

I think that this is a very unusual design choice for an untyped language and it has the effect that you are forced to program in a style that often feels more like Haskell or OCaml than Scheme or Clojure or Erlang.

I've also decided not to support function or operator overloading.

So the `+` operator only adds two integers and the `STRING.append` function only appends two strings. If you try `+` with strings (or with a string and an integer), that's a programmer error and the program will halt at runtime."
Is LLVM a good backend for Functional languages?,8ggx2n,2018-05-02 20:12:58,"I want to start on a small toy language which borrows a lot from Elm ( purely function, strong typing) but is compiled. I was wondering if I should use LLVM as the backend for it? I read that functional language compilers are based on CPS instead of SSA. AFAIk, LLVM doesnt have CPS support. Should I go with LLVM? Or are there other options which fit my use case? For me the ease of use and getting started are the most important bits.","Oh wow, I _just_ went down the rabbit hole of CPS, SSA, and ANF while developing my [compiler](https://github.com/jdreaver/amy) for a strict Haskell-like functional programming language.

I read the [outstanding book by Appel](https://www.amazon.com/Compiling-Continuations-Andrew-W-Appel/dp/052103311X) on compiling using CPS, and was all ready to go to refactor my pre-LLVM IR to be CPS. Then I did more research and realized that while a number of optimizations are very natural in CPS, compiling CPS to machine code is not as simple. It felt like a really daunting project, and after wrestling with my CPS transformations for about a week I filed a CPS IR away in the ""research again someday"" bucket.

The best intermediate representation for a functional language I've found is A-Normal Form (ANF). Here is the [original paper](https://slang.soe.ucsc.edu/cormac/papers/pldi93.pdf) on the subject. The argument goes that ANF is much more compact and easier to understand than CPS, and still enables almost all of the same optimizations. Some recent work with [join points in GHC](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/join-points-pldi17.pdf) and a few other papers/theses I read (linked below) convinced me that ANF was going to be my choice of IR.

I highly recommend sticking with LLVM. It is a very mature ecosystem and it gives you so much ""for free"". I think it's neat that my optimization pipeline will look like:

1. Core -> Core optimizations
2. Some small ANF optimizations
3. Compilation to LLVM where I can have LLVM do some optimizations as well before spitting out machine code

Even now, I only have some very rudimentary optimizations implemented for ANF, but turning on `-O3` when compiling to LLVM makes my toy programs just as fast as equivalent programs I wrote in C. I feel like using LLVM gives you the best of both worlds between ANF and SSA; you hand-write your ANF transformations in your compiler, and let LLVM do the neat things that can be done with SSA optimizations. Note: I am no compiler expert. Maybe I'm being naive in thinking the LLVM optimizations after ANF optimizations give me that much. I'd be happy for someone else to chime in here :)

Lastly, you mention ease of use and the ability to get started as important criteria. In that case something like ANF to LLVM is the obvious choice.

Good luck!

---

If anyone is interested, I gathered a lot of resources while researching CPS/ANF/SSA. I'll just dump them here:

Andrew Appel wrote a book called Compiling with Continuations
(https://www.amazon.com/Compiling-Continuations-Andrew-W-Appel/dp/052103311X),
where he explains how continuations can be used as the back end of a compiler.
Lots of stuff since then has been written on how using continuations makes lots
of optimizations a lot simpler, and how it is pretty much equivalent to SSA.

More stuff:

* SSA is Functional Programming:
  https://www.cs.indiana.edu/~achauhan/Teaching/B629/2006-Fall/CourseMaterial/1998-notices-appel-ssa_fnprog.pdf
* How to compile with continuations
  https://news.ycombinator.com/item?id=7150095
* Paper by Appel in 1988: Continuation Passing, Closure Passing Style
  ftp://ftp.cs.princeton.edu/techreports/1988/183.pdf
* Compiling with Continuations and LLVM:
  http://manticore.cs.uchicago.edu/papers/ml16-cwc-llvm.pdf
* Compiling without continuations:
  https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/compiling-without-continuations.pdf
* Compiling with Continuations, Continued:
  https://www.microsoft.com/en-us/research/wp-content/uploads/2007/10/compilingwithcontinuationscontinued.pdf
* Slides implementing Compiling with Continuations, Continued:
  http://code.ouroborus.net/fp-syd/past/2014/2014-08-Sloane-CPS.pdf
* Compiling with CPS (nice blog post using Haskell):
  https://jozefg.bitbucket.io/posts/2015-04-30-cps.html

ANF and SSA resources:

* Original ANF paper: The Essence of Compiling with Continuations
  https://slang.soe.ucsc.edu/cormac/papers/pldi93.pdf
* Apparently the MLTon compiler used to use CPS but now uses SSA
  http://mlton.org/pipermail/mlton/2003-January/023054.html
* Comparing CPS, ANF, and SSA for compiling functional languages
  https://www.jantar.org/talks/zadarnowski03languages.pdf
* A Functional Perspective on SSA Optimisation Algorithms
  https://www.jantar.org/papers/chakravarty03perspective.pdf
* The CPS/ANF saga
  http://www.ccs.neu.edu/home/matthias/369-s10/Transcript/anf-vs-cps.pdf
* Thesis on optimizations with ANF: David Tarditi. Design and Implementation of
  Code Optimiziations for a Type-Directed Compiler for Standard ML. PhD thesis,
  School of Computer Science, Carnegie Mellon University, December 1996
  http://www.dtic.mil/dtic/tr/fulltext/u2/a326493.pdf
  - This is a great thesis.
  - It emphasizes a ""pay as you go"" compilation strategy where simple,
    monorphic code should not be slower simply because the language supports
    higher-level code.
  - Also, monomorphic functions should get specialized to machine-type
    functions.
  - C compilers focus on loops, so functional compilers should focus on
    recursive functions.
  - Explains lots of optimizations with a lambda calculus IR
* Paper summarizing the David Tarditi thesis
  https://pdfs.semanticscholar.org/55bc/5ceee768223f4b233de568a7181297eb2c4d.pdf
* A Correspondence between Continuation-Passing Style and Static Single
  Assignment. (IR'95, published as ACM SIGPLAN Notices, 3(30), March 1995)
  http://mumble.net/~kelsey/papers/cps-ssa.ps.gz"
Efficient Parallel Functional Programming with Effects,147x4xh,2023-06-13 04:43:13,,"It's only a matter of time before every term from Quantum Mechanics becomes a term in Comp Sci. (Entanglement, in this case.)"
Lazy Let: A Cheap Way and Easy Way to Add Lazyness,13rztfp,2023-05-26 09:42:57,"Adding a lazy let expression to a strict language is a small feature that a lot of expressive power. They let you add lazyness to a language while not changing the type system^((may cause soundness issuse in exotic type systems though)) and only mildly changing the runtime implementation.

**Example Syntax**

Lets imagine a Javascript-Like language argumented with a lazy let construct.

    // computation is delayed
    lazy x = e();
    
    // `x` is computed here
    console.log(x);
    // `x` is already computed so it's result is reused
    console.log(x);

Here a lazy variable binding delays computing it's value until actually needed. As is usual with lazy languages, this delay extends though conditions and lambda.

    lazy x = e();
    if(conditional) {
      // `x` is only computed if `conditional` returns true
      console.log(x);
    }

&#x200B;

    lazy x = e();
    function f() {
      // `x` is only computed when `f` is called
      // repeated calls don't recompute `x`
      return x;
    }

**Emulating Call by Need**

Consider this Javascript function:

    function need(condition, expensive) {
        if(condition) {
            return f(expensive(), expensive());
        } else {
            return g();
        }
    }

Here the intention is that `expensive` is a call by need argument (while conditional is call by value). It's a zero argument function where it's result is expected to be memoized.

If we imagine Javascript argument with a lazy let, then we can use it to build the thunk for us.

    lazy expensive = e();
    var result = need(condition, () => expensive);

Alternatively, if we assume that the argument is not memorized then we can use lazy let in the body to have memoization.

    function need(condition, expensive_0) {
        let expensive = expensive_0();
        if(condition) {
            return f(expensive, expensive);
        } else {
            return g();
        }
    }

**Solution to Global Variable initialization**

Lazy let gives a nice solution to the problem of global variable initiation. All globals can be implicitly lazy and now they can allow any computation without requiring initialization. There can even be an optimization pass to convert constants into strict values.

    // global_a here does computation
    lazy global_a = global_b + e();
    
    // global_b is a constant
    // so an optimization pass can make it strict
    lazy global_b = 10;

**Recursive Let**

In functional fashion, I'd imagine lazy let could be recursive too. This would allow creation of lazy lists that appear in functional languages like Haskell.

    lazy ones = cons(1, () => ones);

**Interaction with Side Effects**

Lazy let is not without it's problems though. When side effects aren't tracked they could lead to the same problems that occur side effects is lazy languages. Where it becames every hard to predict where the side effect actually happens.

However, here lazy bindings are a separate isolated feature rather then all pervasive as they are in lazy languages. So I'd imagine it wouldn't be as big of an issue.

**Runtime Implemenation**

Lets assume that `delay` and `force` are language constructs that create and evaluate a lazy value respectively.

To implement this at runtime, you would need to desugar `lazy x = e;` into `lazy x = delay e` and all uses of `x` would need to desugar into `force x`.

**Inspiration**

I got this idea after a short conversion with /u/Inconstant_Moo on the discord.","> Lazy let gives a nice solution to the problem of global variable initiation. All globals can be implicitly lazy and now they can allow any computation without requiring initialization.

Dart does this. All top level variables and static class fields are lazy initialized on first access. It works well.

Dart also has `late` as a modifier for variable declarations, which does more or less what you describe here. It's a nice feature, but it is not without its subtleties and complexities.

### Recursive references

When you allow recursive references between lazy variables, you have to worry about cyclic initialization errors:

    late x = y + 1;
    late y = x + 1;

Here, in the middle of running one of the variable's initializers, we recursively encounter another read of that same variable. You have to decide how the implementation handles that and think about the performance implications if the answer involves some level of runtime checking. (In Dart, we check this at runtime and throw an exception if a cyclic initialization is encountered.) If the answer is to throw an exception, you have to decide what happens if the exception is caught and then a user tries to access the lazy variable *again*.

### Async/await

Dart, like several other languages, also has async/await. That gets compiled to something like a continuation-passing style. That transformation does not interact with lazy variables well. Consider:

    Function someAsyncFunction() async {
      late x = await Future.delayed(Duration(second: 1));

      syncClosure() {
        print(x);
      }

      return syncClosure;
    }

    test() {
      var f = someAsyncFunction();
      f();
    }

Here you have a synchronous closure that's accessing a lazy variable. When it does, it should cause the initializer to run. That initializer then does an asynchronous wait. The closure itself isn't async, and it's invoked from a function that isn't async. How does this work?

The answer for Dart is that it doesn't: You simply can't use await inside the initializer for a late variable. We could potentially have allowed that but then say that any late variable whose initializer uses await can only be accessed inside asynchronous functions, but that would be a weird, probably confusing restriction.

The end result of this is that late variables aren't quite as compositional as I'd like. But I blame async/await for that. Async/await just isn't really compositional.

### Detecting initialization

Just of state has deferred or lazy initialization and you might think lazy variables would be a great fit for all of them. It works well in cases where there is simple lazy initialization that unconditionally evaluates a single expression. But in more complex use cases, you often want to be able to *tell* if that lazy initialization has happened yet.

If all you have is lazy variables, there's no way to do that. As soon as you look at it, you run the initialization to completion.

To cover more complex use cases, you really need some other syntax to check the hidden ""is initialized"" bit on the lazy variable without forcing the initializer to run. We don't have that in Dart and in practice it means that sometimes you end up having to use nullable non-lazy variables and implement the lazy initialization yourself."
Kickstarter! Programming Languages Festival!,13igk7z,2023-05-16 02:45:31,,I'll be in Phoenix around that time. How do I reserve a spot?
Is your language solving a real world problem?,12ehsjf,2023-04-07 19:16:28,"If so, what problem or problems are you attempting to solve in your language? And how did you go about identifying these problems?","Yes, reduces my depression. Fun!"
Olive interpreted language,1274jaw,2023-03-31 08:32:19,"Programming languages are insane. There's so many interacting pieces and so much room for bugs. For the past four months I've been working on my programming language Olive and it's probably been the best past four months of my life. I'm sleep deprived, sure. Exhausted as hell, sure. But these months have been a crazy learning experience that I won't trade for anything. I'm a much better programmer as a result.

This, Olive, is my first language and Crafting Interpreters was very helpful with figuring out the important bases of building compilers, parsers, scanners etc. I've gone on to study other texts and they've been largely comprehensible, thanks Robert!

So yeah, this is a language announcement post, guys. Olive is my language and it's the first of many versions. I intend to continue working on it for as long as possible, modifying and adding pieces as my knowledge at the time sees fit. Lastly, thanks to this community for being helpful when I was stumped with questions. That is all.

Here's [Olive](https://github.com/wldfngrs/Olive). Constructive criticism as always is welcomed.

https://github.com/wldfngrs/Olive",A good start but C-style `switch`es are heinous. They make a vanishingly uncommon case into the default behavior.
Challenges writing a compiler frontend targeting both LLVM and GCC?,11kxwql,2023-03-07 20:22:22,"I know that given that I haven't written any compiler frontends yet, I should start off by picking just one of them, as it's a complicated enough task in of itself, and that's what I plan to start off with.

Just thinking ahead, what difficulties might I face in writing a compiler frontend for a language of my own, that is able to target either LLVM IR or GCC's GIMPLE for middle/backend processing?

I'm not asking so much about programming complexity on the frontend itself (I know the design of it will require some kind of AST parser which can then generate either LLVM IR or equivalent GIMPLE for GCC), I'm asking more about integration issues on the binary side with programs produced using either approach —i.e. is there anything I have to take particular care with to ensure that one of my programs compiled with GCC will be able to link with one of my libraries compiled with LLVM? I'm thinking of things like different calling conventions and such. If I'm not mistaken, calling conventions mainly differ on a per-OS basis? But I have heard that GCC's calling conventions differ to MSVC's on Windows...","That sounds really hard.  Writing a compiler frontend is hard enough.  Writing one which interacts correctly and efficiently with two very different middles seems just masochistic.

What do you hope to gain by this and is it worth it?

Have you considered just writing an independent source-to-source transformer the output of which can be fed automatically into the regular GCC/LLVM frontend?"
AG unification is the solution for type inference with scientific units,112n46o,2023-02-15 10:24:14,"So recently I've been learning a bit about E unification I've had the minor realization abelian group (AG) unification is perfect for scientific units / dimensional analysis.

Syntactic unification is the lifeblood of Hindley Milner, which, as we all know, is the gold standard for type inference. Hindley Milner boils down to generating equations which then syntactic unification must solve. For example something it could generate something like `map(array(int()), x) = map (y, double)`.

E-unification is unification with some additional notation of equality. For example if `f` is communative then `f(a(), b()) = f(b(), x)` , which would normally be rejected by syntactic unification, could be satisfied. AG unification is E-unification where you have the laws of abelian groups.

Specifically, you have 2 functions `f`,`i` and a constant `e`, where:

|Name|Law|
|:-|:-|
|Associativity|`f(x,f(y,z)) = f(f(x,y),z)`|
|Identity|`f(e, x) = x`|
|Inverse|`f(i(x), x) = e`|
|Commutativity|`f(x,y) = f(y,x)`|

Here if `f` is multiplication, `i` is inverse and `e` is 1, alongside a set of constants then this fits perfectly with scientific units, where each constant is an SI unit.

What's more is that AG unification with constants, as opposed to many other types of E-unification, still has a most general unifier(see [unification thoery](https://www.cs.bu.edu/fac/snyder/publications/UnifChapter.pdf), section 3.4). Due to this, you could just slot AG unification right into Hindley Milner, where you start off using normal syntactic unification and switch to AG unification when you encounter type level multiplication or inverse (assuming you have a kind system to prevent mixing unifications).

Here some rather obvious type rules I have in mind:

    e :: terms
    n ::= rational numbers
    π ::= scientific units
    ====================
    
    
    --------------
    Γ ⊢ n : int(π)
    
    
    Γ ⊢ e1 : int(π)    Γ ⊢ e2 : int(π)
    ---------------------------------
    Γ ⊢ e1 + e2 : int(π)
    
    
    Γ ⊢ e : int(π)
    ---------------
    Γ ⊢ -e : int(π)
    
    
    Γ ⊢ e1 : int(π1)    Γ ⊢ e2 : int(π2)
    ------------------------------------
    Γ ⊢ e1 * e2 : int(f(π1, π2))
    
    
    Γ ⊢ e : int(π)
    ---------------------
    Γ ⊢ 1 / e : int(i(π))
    
    
    --------------------
    Γ ⊢ meter : int(m)
    
    
    --------------------
    Γ ⊢ second : int(s)
    
    ... and standard HM rules

Here something like `(5 * meter) / (2 * second)` would have type `int(f(m, i(s))`.

It's a tough read at certain points, but [unification theory](https://www.cs.bu.edu/fac/snyder/publications/UnifChapter.pdf) (that I mentioned earlier) covers AG unification in chapter 5 and you can probably find more sources by following the cited works. I haven't implemented it, but I believe it boils down to solving diophantine equations.","You're not wrong: this concept is mathematically sound. Since dimensional units work like an [Abelian Group](https://en.wikipedia.org/wiki/Abelian_group) in real life, it stands to reason that you might structure a dimensional analyzer along those lines.

What are your thoughts on the level of formality here, though? Can/should a list have dimensional units? Or would the units be restricted to the numeric tower? Should a quaternion have one set of units, or four?"
Logic Programming as a Library,10zom72,2023-02-11 22:15:36,,"A great read, but a little hard to follow at some places.  `letlazy` in particular seems kind of hard to conceptualize, but I understand the motivation for it at least.  Thanks for sharing!"
A declarative DSL for calendar events and scheduling,10xwzhw,2023-02-09 23:05:51,"Github: [https://github.com/JettChenT/timeblok](https://github.com/JettChenT/timeblok)

Hi! Over the past few weeks, I've been working on a DSL for calendar event creation. I'm looking for feedback regarding the usefulness of this language and its potential use cases in the future. 

On a high level, the compiler takes in a text file written in the DSL and compiles it to a digital calendar file format (.ics file), which could then be opened by a user on any calendar application.

**Main features:**

* Easy creation of a calendar event at a given time in a given day
* Ability to add notes and metadata regarding an event.
* Dynamic resolving of dates based on inheritance and overriding. 
* Complex and dynamic filtering of dates and events to represent repetition and more.

**Why Timeblok**

* Sometimes you don't want to click around all the time when using calendars
* The ability to represent complex repeat rules in GUI calendar applications is limited
* Text files allow for much more expressiveness and freedom in how one organizes one's content, creating a more streamlined experience for planning
* The format for digital calendars, .ics files, is barely human comprehendible, let alone writable
* Due to the extensiveness nature of this language, it's easy to create plugins and extend the language's functionality
* Current NLP task creation features in calendar apps are not standardized and only allows for creation of one event at a time, while this provides a standardized text interface for calendars, and could potentially be integrated with LLMs to provide a better natural language task creation experience.

**Examples:**

A simple day plan

    2023-1-1
    7:30am wake up & eat breakfast
    8am~11:30 work on TimeBlok
    - Write Technical Documentation
    2pm~6pm Study for exams
    8pm~10pm Reading
    - Finish an entire book

A more complex monthly plan

    2023-1-                         // Locks in the following events to Janurary 2023
    {--1~--10 and workday}          // selects all workdays from jan 1 to jan 10
    7:30am wake up to a new day
    10am ~ 11am work on EvilCorp
    
    {sun}
    4pm weekly review               // weekly review every sunday
    
    --11
    8am~10am Resign from EvilCorp
    - Make sure you still have access to the servers
    
    -2-                       // This overrides the month information from line 1.
    --1
    3pm~4pm Initiate operation ""Hack the planet""

The results shown in a calendar and the language specs (still working on writing this) are all in the [Github readme](https://github.com/JettChenT/timeblok).

**My plans on developing this further:**

* Support a plugin system and the ability to interact with external calendar subscriptions/files
* First-class support for date calculations
* Support for more .ics features such as tasks and availability
* Add syntax highlighting support & port to WASM?
* Syncing feature for online calendars
* Explore how this could work in combination with LLMs for natural language events creation

Feel free to leave a comment below, any feedback / constructive criticism would be greatly appreciated!","I dig this – could be really nice to have pure text interface to my calendar.

Some early feedback:

* really nice that you DIDN'T take inspiration from awful CRON format
* as European, having 24-hour format would be nice
* support for tasks would be great

This approach would also work for recording past activities, right? For example, I export all my running data into this format and I would be able to see when I ran in my calendar, right?"
What was the dumbest thing you implemented/prototyped?,10tnu0m,2023-02-05 02:44:11,"Many years ago people kept talking about how serializing types/functions/whatever into a binary format would make compilers faster (specifically C++). I think this was right before webassembly so I wonder if they were affected by those ideas

I implemented it

It was a disaster.

The first problem is you're creating a lot of nodes, malloc is your first bottleneck. You'll need a memory pool or arena.  

Then random access becomes a bottleneck. Most of your structs and function parameters might be int's and strings but you're going to have a fair amount of structs that uses non native data types

Next problem is the branch predictor hates you. It has no idea if the next type it sees will be an int, bool, string, struct from the standard library, user implemented struct, a pointer etc. If they accept optional data like a non default value the predictor will miss more often (ex:  `struct A{int a=1;}` or `int myfn(int a=123)`) 

After all of the above it turned out that parsing the text gave me the same speed since it was all those random lookups that was the bottleneck. That was the first and only prototype with a text (for source) and binary parser (for libraries)","Parser combinator library before I knew what parser combinators were

Well the *idea* for a parser combinator library wasn’t dumb at all. What was dumb was my implementation, which took minutes to parse even a simple script with no more than 3 lines and probably 20 tokens nodes (it was nondeterministic). And then it parsed wrong anyways. Also the types were way over-engineered with unnecessary extensibility, so it was a verbose complicated mess to write a parser, which kind of defeats the purpose.

Somehow this project took a couple months before I abandoned it. But it was written when I was still in high school, and I learned a lot so it wasn’t really a waste of time. Nowadays I prefer hand-rolled parsers."
Is there a Build Yourself a Smalltalk?,y1w4zh,2022-10-12 14:21:35,"I’ve loved going through tutorials that show you how to build a small programming language from a scratch, such as the Make a Lisp, Write Yourself a Scheme in 48 hours, and Write You a Haskell. However, I’ve never seen such an article for Smalltalk. 

Are you aware of anything tutorials to build simple Smalltalk like languages? And if they don’t exist, why do you think that is?",[A Little Smalltalk](https://rmod-files.lille.inria.fr/FreeBooks/LittleSmalltalk/ALittleSmalltalk.pdf) perhaps.
"Research on performance-oriented, high-level languages?",xx0qav,2022-10-06 17:14:45,"Is there any research on truly high-level programming languages that don't lose focus on performance?

By ""high-level"", I mean languages where the compiler is free to
* select a concrete data structure based on an abstract interface (for example, when the source uses a map, it gets implemented using a hashtable or a B-tree, depending on static analysis or profiling data)
* parallelize an algorithm using various strategies (e.g., use a different approach when sorting 1M arrays with 4 elements vs sorting 1 array with 4M elements; maybe even include both paths and choose at run-time if unknown at compile-time)
* choose between stack-, arena-, malloc-, RC-, and GC-based allocation on a case-by-case basis, determined by static analysis

By ""focus on performance"", I mean that languages which are just very high-level, but skip the *optimization* part, don't count. Neither do mainstream functional languages, which purport to be declarative, but where the algorithm, memory management, and execution strategy are actually pretty strictly determined by the source code and the language semantics.

So kind of like SQL, but general purpose, not domain specific.","Even without performance, are there any languages that do all of the above high level stuff you want?"
jlox in Rust. Bonus content: Lox in your browser!,xt7eqm,2022-10-02 05:57:50,"(Didn't find a more specific subreddit, feel free to redirect me if it does exist)

I guess Lox needs no introduction. I just finished working through the first part of the Crafting Interpreters book (tree-walk interpreter in Java), but writing Rust instead of Java. Rust compiles to WebAssembly, so naturally there's a web-based version you can poke at that you may find fun: [https://abesto.github.io/jlox-rs/](https://abesto.github.io/jlox-rs/)

There's a ton of fun little details I captured, behind the ""What am I looking at?"" button on the website. It's about 600 words, so won't paste it here to keep the post short and sweet.

Flaired ""blog post"" because this is technically actually almost a blog post :)",I love it.
"Pretty printing, which paper?",vzp7td,2022-07-15 21:50:03,"I'd like to write a pretty printer. I've found 2 papers and I'm curious if anyone has read any of them, and could give me some guidance on which to read and implement?

The papers:

1. [*Derek C. Oppen, ""Pretty Printing"" (1979)*](http://i.stanford.edu/pub/cstr/reports/cs/tr/79/770/CS-TR-79-770.pdf) *—* According to [this](https://github.com/dtolnay/prettyplease#algorithm-notes) it is method used by the Rust compiler which caught my eye
2. [*Philip Wadler, ""A prettier printer"" (1997)*](https://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf) *—* I really like the concept of algebraic documents

Are there any considerations about performance, implementation complexity, etc. between them?","Can't speak directly to performance, but it'a worth mentioning Prettierjs, probably the most popular pretty printer in the wild, is based on the wadler paper, so as a practical matter I wouldn't discount probably either based on performance.


I haven't read the Oppen paper, but I do recommend the Wadler paper, on the grounds of the combinator approach being flexible/extensible. I had to implement a pretty printer that output not only text, but text interspersed with HTML elements, and (after a few reads of the paper and a week to digest) was able to implement the printer and extend it in the way I needed in a single work day."
"Programming with union, intersection, and negation types",vx1u2v,2022-07-12 11:58:21,,"The paper mostly uses difference types rather than negation types. As it points out, *technically* they are equivalent:

* A \ B === A & ~B
* ~A === Any \ A

But some of us are not convinced that `Any` should be a thing. In this case, I don't think it makes sense to support negation either - only difference."
Why have an AST?,vgekk2,2022-06-20 14:11:44,"I know this is likely going to be a stupid question, but I hope you all can point me in the right direction.

I am currently working on the ast for my language and just realized that I can just skip it and use the parser itself do the codegen and some semantic analysis.

I am likely missing something here. Do languages really need an AST? Can you kind folk help me understand what are the benefits of having an AST in a prog lang?","Omitting the AST is a perfectly valid approach when the goal is nothing more than a compiler that just works, e.g. when coding the initial implementation of a high level language in assembler. However, the quality of generated code will be limited, since complex program transformations aren't feasible without an AST.

If you pack everything into the parser, maintainability will take a nose dive as more sophisticated analyses and transformations are implemented, and it will eventually become a nightmare to work with..."
Introducing stack graphs,rcto9s,2021-12-10 06:59:16,,what draw tool do you use?
"Skybison, Instagram's experimental performance oriented greenfield implementation of Python",pew88s,2021-08-31 09:49:12,,Hi all! I was on the Skybison team and am happy to answer any questions. I've been working on this for a couple of years and am very excited to finally open it up.
What programming languages can emit code?,p45lca,2021-08-14 18:25:44,"Some language have built-in or libraries that  allow code emitting during runtime.

For example in C# you can dynamically create a method that add two numbers and run it.

Some Javascript runtime will also compile the code passed in an eval statement.

Do you know of any lower level languages without garbage collection that allow code emitting?",LISP called and wants its eval back
Remaking C?,oiqa22,2021-07-12 20:30:04,"Hello everyone I'm just a beginner programmer, have that in mind. I'm wondering why don't people remake old languages like C, to have better memory safety, better build system, or a package manager? I'm saying this because I love C and it's simplicity and power, but it gets very repetitive to always setup makefiles, download libraries(especially on windows), every time I start a new project. That's the reason I started learning Rust, because I love how cargo makes everything less annoying for project setup.","When people remake an old language, that typically just results in a new language all together. Most of the curly bracket languages could probably be seen as an attempt to remake C—at least the older ones, the newer ones are an attempt to remake a remake of C."
I developed a toy visual programming language inspired by Unreal engine blueprint function graphs,oc8lh7,2021-07-02 20:00:01,"Hello, I've been working with unreal engine for the better part of the last year. Blueprints allow users to draw node graphs that are executed during the render loop of the engine. 

For fun, I decided to implement a text-based (esoteric?) visual programming language based on node graphs. 

GitHub: https://github.com/p-ranav/box

Ideas on implementing specific (currently unsupported) language features e.g., importing across files, etc., would be appreciated :)

The language is implemented in Python and transpiles to Python.","There's a fascinating thread going on over on HN right now which may interest you (either just to read, or to submit your project to a like-minded audience):

> Folk wisdom on visual programming

https://news.ycombinator.com/item?id=27705631"
"Announcing Slash, a modern shell scripting language",nlbzrp,2021-05-26 16:35:28,"Slash

A few weeks ago, I had to implement a super simple shell script, executing a few commands and branching depending on the result. I used the awesome bash. Getting the script to work took far too long. That got me thinking. Why was that? One reason is that the bash syntax is quite different from the languages I normally work in, and the way variables and structures works is also different. The idea for me at that point was to see what a modern approach to shell scripting would look like. It needed to have the same awesome abstraction about spawning subprocesses as bash, but it also needed to have a familiar feel with higher level abstractions, such as first order functions.

So, I got to work. And the result is [Slash](https://slashlang.org).

The language is implemented in Rust and the License is GPLv2. The code is hosted on [slash github](https://github.com/mikkeldamsgaard/slash)",cf [Alternative-Shells](https://github.com/oilshell/oil/wiki/Alternative-Shells).
"Is my language ""boring""?",n229an,2021-05-01 04:08:34,"I feel like my lang is just too much like every other language out there:

\- compiles to c++

\- strongly, statically typed

\- object oriented. you can have static (class-wide) members and ""methods""

\- some magic methods: \`\_\_eq\_\_, \_\_ne\_\_, \_\_str\_\_\` (these three defined by default but can be overloaded), and other operators

\- first class functions, you can pass them around, return them from other functions, partial application. except you can't create a function (define its actual code) inside another function

\- enums

\- Tagged Union type:

    var x : Union[Int, Str] = 8
    match x {
    	i: Int -> print(""int"")
    	s: Str -> print(""str"")
    }

\- garbage collected

\- no pointers, pass by reference

\- Maybe/Option type, so null safety

\- no inheritance/typeclasses/traits/etc. (I don't know if I'll add any of these in the future), but:

\- generics: parametric polymorphism

    fun first(x: a, y: b)->a {
    	return x
    }

&#x200B;

    class Wrapper[t] {
    	v: t
    }
    var w = Wrapper(""Hello"")
    print(w.t)

\- Tuples

    var x = #(4, ""Hello"")
    print(x.2) // Hello

\- just like in python you can do things like:

    m = Class.method
    m(obj)

or

    m = object.method
    m()

And that's basically it, I haven't yet given much thought to concurrency, libraries, etc., but I feel there's nothing too interesting about the syntax & semantics. On the other hand, I don't think there's a language already out there that has these ""features"" with this particular syntax. Opinions?",Is it a problem if your language is like other ones?
Anyone working on a package manager for their language?,mdz6wh,2021-03-27 05:42:12,"I'm currently working on a package manager for YASL, in YASL. (I hope to write a post about it here once it's done.)

I've found a few small bugs in YASL so far, which is great.

I'm wondering if anyone else has managed to write a package manager for their language, and if so, how did you choose to handle stuff like multiple versions of a package, for example, and just general design of your package manager.","I’ve been toying with making one. Dunno if it’ll pan out. [So you want to write a package manager](https://medium.com/@sdboyer/so-you-want-to-write-a-package-manager-4ae9c17d9527) is a good overview of the many challenges involved.

My advice to others is that if you can reuse an existing package management tool, then you should probably do so, whether it’s a language-level tool like NPM, or an OS-level tool like Apt/Yum. If your language is meant to integrate with an existing ecosystem, this will make it *way* smoother for users, and someone else has done a lot of the work for you already.

If you want to make a tool as a learning experience, or you have a specific gripe you want to solve, then go for it! But I think a packaging tool, like a build system or editor, is a task on the same scale as making a language itself.

More importantly, I think a lot of package management tools are kinda trying to solve the wrong problem, or at least, trying to implement a technical solution to a largely social problem. Version bounds are *a convention for human communication*, a statement of *intent* by the author of a package based on what they know, *not* a precise description of compatibility.

So what I’d actually like to see in a packaging tool (and what I’ve been idly designing) is something that makes the process of package management easier for *humans* to do, by showing information needed to make good decisions about upgrades and backward compatibility, such as sharing anonymous data about what packages have been tested together."
The Gleam Programming Language and its Creator Louis Pilfold (strongly typed BEAM language),m7qr1s,2021-03-18 21:25:28,,Really interesting and amusing developer who is certainly targeting the right VM.
Looking at Generic Type Variance,m6e2al,2021-03-17 01:13:37,,"Personally, I favor the MLSub approach to invariance. Rather than supporting invariant types directly, you instead simulate them by having two type parameters, one covariant and one contravariant."
Passerine — extensible functional scripting langauge — 0.9.0 Released!,lofdva,2021-02-21 04:03:21,"Well, it's been a couple of months since I [first announced the Passerine Programming Language on the r/ProgrammingLanguages subreddit](https://www.reddit.com/r/ProgrammingLanguages/comments/k97g8d/passerine_extensible_functional_scripting/?utm_source=share&utm_medium=web2x&context=3). Quite a bit of work has been done (+1831 -630 LOC), so I just thought I'd update y'all on what's been done, and what's new in 0.9.0!

For the uninitiated, Passerine has roots in Scheme and ML-flavored languages — it's the  culmination of everything I expect from a programming language,  including the desire to keep everything as minimalistic yet concise as  possible. At its core, Passerine is lambda-calculus with  pattern-matching, structural types, fiber-based concurrency, and  syntactic extension. There's more information in the [Overview](https://github.com/vrtbl/passerine#an-overview) or the [FAQ](https://github.com/vrtbl/passerine#faq) sections of the [README](https://github.com/vrtbl/passerine/blob/master/README.md). There's also a [website](https://www.passerine.io/) with [installation instructions](https://www.passerine.io/#install) for those so inclined ;).

The previous release — 0.8.0 — introduced a powerful hygienic macro system. **This release — 0.9.0 — introduces a foreign functional interface (FFI) between Passerine and Rust**, as well as tuples, along with a number of small quality-of-life improvements. You can find a [more complete list in the PR here](https://github.com/vrtbl/passerine/pull/16).

As per tradition, here's a quick syntactic sample of Passerine (WIP):

    -- comment
    
    fizzbuzz = n -> {
        test = d s x 
            -> if n % d == 0 {
                _ -> s + x """"
            } else { x }
    
        fizz = test 3 ""Fizz""
        buzz = test 5 ""Buzz""
        ""{n}"" . fizz (buzz (i -> i))
    }
    
    1..100 . fizzbuzz . print

Of course, Passerine is still very much so a work in progress — not everything is bug-free or guaranteed to work yet. We've done a lot, but there's still a so much more to do! If you'd like to contribute, reach out on our [Discord Server](https://discord.com/invite/yMhUyhw).  
Feedback I've incorporated from Reddit since the last post:

* u/open_source_guava — Had a few suggestions surrounding ambiguity and error reporting. I've made sure most of these are addressed, and worked out the proper semantics for calling macros from other macros. Also: 'Do post again when you have more' — I have more ;)
* u/__fmease__ and u/superstar64 — Suggested strong static typing through Hindley-Milner. Currently, Passerine is dynamically¹ typed (technically *structurally typed* with planned runtime d*ependent typing*). This is more out of current architectural necessity than language-design preference. **The next release — 0.10.0 — will add support for Hindley-Milner Type Inference, with a few extensions allowing for row and let polymorphism, and hopefully algebraic effects.**
* u/superstar64 — Suggested using `b.a` in place of `b |> a` for function application. I agree that this syntax looks nicer, and will be using it. Additionally, `::` will be the indexing operator instead of `.`.
* u/zem — Mentioned sum types. Passerine plans to have sum types (enums), but I'm planning to work out the exact syntax as I work through the HM type system.

**I've also incorporated feedback from people on Discord (Rishi 💯), and would like to thank everyone who has contributed thus far!**

Again, thanks for the support and feedback! Feel free to leave further comments, feedback, and suggestions. I'm all ears, and I'll try to get back to everyone.","looks awesome! if you're still working on the syntax, look to [pyret](https://www.pyret.org/) for inspiration, so far it's my favourite scripting language syntax"
Pycopy 3.5.0 - a minimalist and memory-efficient Python dialect,koa499,2021-01-01 19:52:48,,"In 3.5, lightweight import hooks are implemented, allowing to easily plug into the import system and support loading of custom module formats, or perform custom processing of the Python source. Some usage examples:

* Load LLVM bitcode: https://github.com/pfalcon/ubitey/ (prototype/WIP)
* Plug pure-Python bytecode (or JIT) compiler: https://github.com/pfalcon/pycopy-lib/tree/master/ucompiler (prototype/WIP)

As usual, this lightweight import hook API is backported to CPython: https://github.com/pfalcon/pycopy-lib/blob/master/cpython-pycopy/pycopy_imphook.py

And on top of it, and slightly more convenient API is built as a small Python module, again portable between Pycopy and CPython: https://github.com/pfalcon/python-imphook"
Rebuilding the Racket Compiler with Chez Scheme,k1vjh2,2020-11-27 13:59:11,,"Matthew also gave a talk about this move with some benchmarks at ICFP '19, for anybody interested in this specific topic."
Understanding static single assignment forms,jgoa2r,2020-10-23 22:35:54,,Thanks I'm a new hire on the field compiler backends and this is a great thing to have at hand
"Inko 0.8.1 has been released, featuring a new approach for running Inko bytecode, fewer dependencies, and a simplified build and installation process.",j5kskh,2020-10-05 22:52:50,,"This release may be a bit ""light"" as it's mostly installation/dependencies plumbing, but past posts seem to gather quite some interest.

For those interested in learning how we dropped the libclang dependency: this is because of [this pull request](https://github.com/tov/libffi-sys-rs/pull/37). It's still under review, but works really well so far."
explicit nullability vs optionals,hy6emx,2020-07-26 20:51:08,"2 approaches to fix the billion dollar mistake, which do you prefer and why ?

###Explicit nullability
* Language example: Kotlin
* Advantages I know: uses operators and smart casts to minimize verbosity

###Optionals
* Language example: Rust
* Advantages I know: optionals can be nested

*There's probably a lot more, but I'm not that experienced yet*","The obvious problem with explicit nullability is that you can't stack null types. For example in kotlin, getting a value from a map is defined as

    fun <K, V> Map<K, V>.get(key: K): V?

But what if we have a map that looks like 

    val m: Map<Int, Bool?> = 
        mapOf(-1 to false, 0 to null, 1 to true)

But the `get` method still only returns null if we pass in a missing key so there's no difference between a `null` value and a missing value. 

That is `V?? ~ V?`. This is why optionals (explicitly the sum type optional, not Java's half assed approach) are much stronger.

The main disadvantage is verbosity. If I have a function like

    // If stop is null, iterate forever
    fun range(start: Int, stop: Int?): Sequence<Int>

it's cleaner to write 

    range(0, 10)
    range(0, null)

than, e.g. in rust

    range(0, Some(10))
    range(0, None)

Still, I'd say the verbosity is worth the power of optionals."
A Thread For Your Favorite Syntax in Programming - Will be published as an article!,hsthj1,2020-07-17 18:41:44,"# EDIT: [Article Published!](https://medium.com/better-programming/youre-creating-a-new-programming-language-what-will-the-syntax-look-like-35199d2a44e9?source=friends_link&sk=12fc9f6d7301928463a833550a72e3db)

&#x200B;

https://preview.redd.it/xnhx44ulbeb51.png?width=1306&format=png&auto=webp&s=3c11255a55f5578a880423400913c09bee737e9b

Hey everyone, just made this separate thread to put the idea from the comment above to work (posted [here](https://www.reddit.com/r/ProgrammingLanguages/comments/hs8lzl/for_fun_my_favorite_pieces_of_syntax_in_8/)), since people seemed to like it. Also, thanks for the award!

So, post below your favorite syntax, the language (of course), your name (optional - for credits), and some link (optional) if you'd want me to share that along.

Also, if possible, please add a little explanation, as I'll surely not be familiar with all examples that come up.

EDIT: Added link to the original post & last paragraph

EDIT 2: Boy this is gonna be a long one

EDIT 3: Wow this is insane! Also very informative. I'm very busy with work this coming week and will try to compile this when I have some time. Also, since we have 120 comments (even though not all are ""submissions""), I might have to resort to pick up the ones with the most upvotes. Let's see...","Several things in Python.

As someone already mentioned, generator expressions (including list comprehensions!), e.g.

    evens = [i for i in range(100) if i % 2 == 0]
    squares = [i**2 for i in evens]
    pairs = [(x,y) for x in range(100) for y in range(100)]
    squarepairs = [(i, i**2) for i in range(100) if i % 2 == 0]

So damn useful!!

Also variable unpacking and multiple assignment:

    x, y, z = (4, 5, 6)
    x, *xs = [1, 2, 3, 4]
    x, y = 4, 5

And also boolean expressions are much nicer:

    A and B
    A or B
    not B
    not A or B and C

And many more things :)"
MewMew - Program is Cat's Language 🙀 - My First Esoteric Language,hp3i7z,2020-07-11 11:56:11,,"A good start, but I think you need to incorporate more cat emoji as operators.  Or something:  

(ΦᆺΦ)   \^.\_.\^   /ᐠ｡‸｡ᐟ\\   ₍⸍⸌̣ʷ̣̫⸍̣⸌₎    \[\^.\_.\^\]ﾉ彡 

[https://cutekaomoji.com/animals/cats/](https://cutekaomoji.com/animals/cats/)"
"Let's build a high quality, comprehensive benchmarks site",fwl4me,2020-04-07 22:08:46,"Hi all – I've been thinking about putting together the mother of all PL benchmark sites, something I think is sorely needed. I'd like to know if anyone is interested in helping.

The status quo seems pretty bad. The most popular benchmark site appears to be the Benchmarks Game. The proprietor there is somewhat capricious about which languages and code submissions he allows, his hardware is too old to be valid, he only runs Ubuntu, and too many languages are absent. For example he removed D at one point and refuses to restore it. He rejected Jon Harrop's F# submissions, which were several times faster than the extant ones. The code for a lot of languages is not idiomatic (I think we might want to have both idiomatic and non-idiomatic solutions on the new site). The benchmarks are generally not ecologically useful, too micro (we should probably have both micro and fuller benchmarks).

The single machine he uses features an Intel Core 2 Quad Q6600 CPU. This model was introduced in 2007, and lacks every SIMD and other modern instructions I can think of: POPCNT, the SSE 4.2 string processing instructions, AVX, AVX2, BMI, F16C, RDRAND, AES-NI, carryless multiplication, etc. Data compression is one area that interests me, and is crucial for a lot of what we do with computers, like the web, search, etc. All the best libraries for classic formats like DEFLATE and gzip, as well as newer ones, leverage modern CPU instructions like CLMUL and AVX(2). It's not possible to get a sense of PL or application performance on a lot of modern jobs and scenarios with a Q6600 CPU. (A lot of PLs facilitate SIMD and/or assembly, even Go for example.)

Moreover, I think cloud computing is fairly pervasive circa 2020, which presents some interesting opportunities. A commodity ""CPU"" these days might well be an *instance type* on a major cloud platform, e.g. an AWS M5 instance. That might be more meaningful to many people than a CPU model number, and cloud instances generally expose CPU models and instruction sets, so we don't have to choose – we could report both sets of info as our environment.

My vision so far:

1. Have virtually every language under the sun; certainly the likes of D would be included.
2. Have both micro benchmarks and more complex application components spanning a lot of scenarios and domains (not just web server or math problems).
3. Have idiomatic and non-idiomatic to hell with it submissions (probably have qualified language teams approve submissions as idiomatic).
4. Use cloud instances running on modern CPUs with at least AVX2 instructions (so Haswell and up).
5. Have several OSes, including one or more Linuxes and a modern Windows Server version.
6. Be welcoming and civil, no capriciousness; really, I see no basis for excluding *any* languages, even ones I've never heard of – what's the harm of allowing all?
7. Look for sponsors, particularly the cloud providers (AWS, Azure, Google Cloud Platform, Joyent, Oracle, IBM, etc.).

I feel like benchmarking, while hard, needs to be a solved problem at this point in history, at least at the level I'm talking about. It's surprising that there aren't a bunch of high quality PL benchmarking sites circa 2020. So... any interest in helping? I'm happy to buy the domain and get the ball rolling. What I would need the most help with is building the site and ultimately finding sponsors to donate instances. (We could probably find some free, or fleetingly free, low-end instances at first if need be, but I'd like to get some donated mid-tier instances somewhere eventually.)

Let me know if you're in. Thanks.

p.s. I'm aware of the TechEmpower benchmark. It's better than the Benchmarks Game, but is only focused on web server stuff, does things that invalidate their results, only runs on an old Ubuntu version, no Windows, etc.","I think that you are under-estimating the amount of work that it takes to gather a good benchmark suite and maintain it on the long run. Isaac Gouy, the caretaker of the Shootout/Benchmarks Game benchmarks, is evidently pouring a *lot* of time in it. I'm pretty sure that many of the things you consider ""capriciousness"" are simply decisions he had to take to be able to keep maintaining the benchmark suite -- it is *probably not* that he is trying to make it worse for no reason.

Maybe the infrastructure that the Shootout is using is outdated, and you could reduce the workload by improving it. Maybe the maintainer is not good at delegating work to other people, and you could find a few motivated people to help you. But then you are also trying to cover a *lot* more ground than the Shootout is, so I don't see how those gains could protect you from suffering from the same problems, or worse."
Feedback on a new shell programming language,ft82w3,2020-04-02 04:36:10,"I've written Crush, a shell/programming language aimed at replacing shells like bash or fish. Unlike those languages, Crush aims to be a reasonably modern programming language with types, closures, etc. But it still tries to be useful for both interactive and batch based shell tasks.

The code (and a longer description) is available [here](https://github.com/liljencrantz/crush/), and I'd be interested in feedback and opinions. The shell is fully functional and should at least work out of the box on modern Linux systems, but it's still pretty rough around the edges.","This is amazing. I love everything about it.

I’d  to see a convergence between shell scripting languages and general programming languages, where every “general” language has a shell mode of sorts. After all, a shell 🐚 is essentially a really advanced REPL."
Languages that make using sets syntactically frictionless,fjwaxu,2020-03-17 09:08:55,"In my opinion, programmers have an unfortunate tendency to use lists or arrays in places where sets are better suited to model the problem. Part of the reason is that often languages consign sets to some library, rather than having them available easily by default.

Python is a good example of a language which makes creating sets completely straightforward, with its `{}` syntax and `in` keyword.

By contrast, in, for example, Haskell, you have to `import Data.Set` and use `fromList`. This isn't too onerous, but it does make programmers slightly less likely to use them.

Are there any other examples of languages which make the use of sets very easy?","Clojure. #{x y z} is a hash set literal. Certain libraries for Java/Scala/Kotlin as well let you do something like Set(x, y, z)"
Whatever happened to the language Ada?,eyeonv,2020-02-04 05:50:31,"Back in my undergraduate days, lots of buzz about how Ada was the first modern  language carefully designed and was to be the end all be all especially for government... Whatever happened to it?","I learned it about 5 years ago for fun and did a few smaller personal projects in it. I'd say it's one of the best languages available, even though it has its quirks and a long learning curve. The reasons I decided not to use it for anything serious:

1. very small community, dominated by a few guys in total and consisting of a strange mixture of old aviation/military/embedded industry professionals and a bunch of clueless beginners; there is just not enough of a community.
2. almost complete vendor lock-in, AdaCore makes the only usable modern compiler and the FSF version based on it is always way behind
3. no optional gc - memory pools are not convenient enough for my needs (as some Ada fanatics will emphasize, nothing in the specs prohibit a gc, it's just that no Ada implementation comes with one...)
4. strange licensing conditions and future uncertainty about licensing: AdaCore's version requires you to publish the source code of your program under GPL, FSF version with mGPL allows you to develop proprietary software, but many essential libraries are not mGPL
5. Ada's plus is one of its perceived disadvantages: Many libraries are old and unmaintained. The reason is that Ada is so good, once the library runs it does not need to be ""maintained"". But it still feels bad to use something from 2005...

Anyway, don't get me wrong. It's a great language, in my opinion the best ""static"" language available. I prefer it over C++ and Rust any time. It's extremely readable, extremely type safe, extremely fast, and has all the constructs you need for system programming.

Personally, I believe 4 killed Ada. AdaCore's business model is basically to support Airbus and a few other companies who use it for safety critical systems. The schizophrenia of trying to maintain an open source language for everyone and at the same time catering to large companies with gigantic budgets and lots of legacy code doesn't work. If I start a project in a programming language, I don't want to have a nagging feeling that in 10 years from now the licensing model might change or the grey-bearded FSF maintainers are going extinct (hope not, of course), and I have to rewrite it or buy a licence for 3000 dollar."
Futhark 0.14.1 released,etakli,2020-01-24 21:43:11,,"> The exception is that tuples in Futhark are now 0-indexed rather than 1-indexed.

Congratulations.

I cannot say whether this is the right decision or not -- although I much prefer 0-index tuples myself. Regardless, I wish to salute your bravery in not shirking from change.

I think that too often, language designers shirk from making a change in fear of upsetting their budding users community. I understand the dilemma, yet I strongly believe that a short-term pain is a much better alternative to long-term paper-cuts (or worse)."
Self Hosting a Million-Lines-Per-Second Parser,cbvysz,2019-07-11 21:59:38,,"It would be interesting to see single-core parsing performance, with the other ""tricks"" applied.

For the little compiler I'm toying with, my plan was to parse on a single core and use the others for more heavyweight tasks: type-inference/check, code generation, etc..."
Lambdas are Codatatypes,c89gd8,2019-07-02 21:50:07,,this is one of my favorite minifacts in PLtheory
A Lox interpreter implemented in ... Lox,9ocz2x,2018-10-15 22:07:05,,"I can't possibly tell you how happy this makes me.

Now you just need to implement a Wren interpreter in Lox. :)"
Brian Kernighan on successful language design,15pfiig,2023-08-13 05:02:06,,"Oldie but goodie.

Another popular one: ""Growing a Language, by Guy Steele"" https://www.youtube.com/watch?v=_ahvzDzKdB0"
Designing a Language without a Parser,14qn9of,2023-07-05 02:40:21,,Thank you for the Part 1 writeup on bidi type checking - that will be useful to me very soon :)
Oxidizing OCaml: Rust-Style Ownership,14fg79e,2023-06-22 03:00:24,,"I've always assumed that uniqueness typing without lifetimes would be so restrictive as to be completely useless. It will be interesting to see if you can make it work at Jane Street.

Also, if you have lifetimes, then you don't need a distinction between ""owned"" and ""exclusive"". ""owned"" is just equivalent to ""exclusive"" with an infinite lifetime. It's probably worth including it anyway to make the type system more intuitive and readable.

Also, I consider borrowing a value to be a ""strong update"", since it changes the type of the value you're borrowing from, it's just a type change that gets automatically undone at the end of the scope and doesn't change any runtime state, making it safe to use on exclusive references even in the presence of exceptions. 

https://blog.polybdenum.com/2023/03/05/fixing-the-next-10-000-aliasing-bugs.html"
The best language tutorials that you have seen?,11yayy5,2023-03-22 15:35:48,"I would like to learn from the best. So when learning a new language which language tutorials stand out in your mind?

I am especially interested in tutorials that succeeded in explaining new concepts and taught you more than just the language.","CS50

Structure and Interpretation of Computer Programs

Concepts, Techniques, and Models of Computer Programming

Haskell Programming from First Principles"
Why have output variables fallen out of favor?,11fg0q7,2023-03-02 04:04:14,"It's been a while since I've used them so I apologize if I'm using the wrong terminology, but I'll give an example. I used to write C# professionally, and I could write a function like this:

    public void changeValue(out int x) {
      x = x + 1;
    }

and then call it like this:

    int y = 1;
    changeValue(out y);

and the value of `y` would be `2`. The thing that was ""cool"" about this was you could return something ***and*** change a value in a function. However, I have not seen such a construct since leaving C# back in like 2017. I've worked with Java, Python, and Ruby since leaving that job and as far as I know, none of these languages have this construct. I read somewhere--I can't remember where--that using out parameters is bad practice and people don't really do it anymore. Is that true? If so, why?","I believe people just decided that passing all input as params and all output as return values was clearer than mixing the two.

At least that's what I see at work. You can still modify parameters in most mainstream languages (i.e. add elements to a list, or update a field), e.g. Python or Java. But it's avoided.

It's just clearer at the call side if you can immediately see what may change and what doesn't, without having to open the declaration or worse, the implementation."
Goal: an array programming language written in Go,zycdaq,2022-12-30 02:26:41,"Hi everyone !

About a year ago or so, I wrote here about an obscure markup language called Frundis. Now, I'm back with an array programming language, unoriginally called “Goal” :-)

https://codeberg.org/anaseto/goal

It's a scripting array programming language, in the same vein as APL, J, K or BQN, so you'll find your typical vectorized array primitives. One unusual thing about Goal is that strings are considered like atoms, and text-handling primitives and regexps are integrated into the builtins. I'm not going to rephrase what's in the README, but given this sub's theme, I'm going to point out the [implementation notes](https://codeberg.org/anaseto/goal/src/branch/master/docs/Implementation.md) I wrote, which might be of some interest.

Have a nice day!","I like the name as well, and in fact Naughty Dog named a language GOAL to create the Jak and Daxter, and Crash Bandicoot games!

https://en.wikipedia.org/wiki/Game_Oriented_Assembly_Lisp"
compile time memory management models,y3rq8w,2022-10-14 19:46:58,"Hey folks

I am trying to read up on compile time memory management approaches. For examples, Rust's ownership and borrow checking is a famous way to implement compile time guarantees on memory safety.

Can you share some other names/approaches I can read up on?
I want to better understand all the ways I can catch memory issues in the code at compile time itself.","[Pony](https://www.ponylang.io/)

[Vale](https://vale.dev/)

[Lobster](https://aardappel.github.io/lobster/memory_management.html)"
Functional Futures: Dependent Types with David Christiansen,xez704,2022-09-15 23:07:55,,A very long but interesting read. Now I know exactly which properties of dependent types make them undesirable for me.
How does control flow happen in declarative programming languages?,x4r429,2022-09-03 18:22:55,"Is pattern matching declarative? What about the clips rules system? Is it declarative? What are the different ways in which declarative languages have implemented control flow?
I want to know how does If/Else/Switch happens in such languages.","As I see it, the main attraction of declarative programming languages is that you can interpret them with **different execution strategies**, and in this way get better performance, or other desirable properties such as completeness of the search, while keeping the program the same. You can also run the same program with different execution strategies that are applied in parallel.

For example, in Prolog, the default execution strategy is *depth-first search with chronological backtracking*, also called SLDNF resolution. However, as long as you keep to the declarative core of Prolog, you can run Prolog programs with *any* execution strategy you want, and in fact an increasing number of Prolog implementations support various other execution strategies. An inreasingly popular different execution strategy is SLG resolution, also called *tabling*. Another good alternative strategy is *iterative deepening*, often easily obtainable by restricting the depth of terms that are tried, so that you can ""coerce"" the default execution strategy to automatically perform iterative deepening.

The same holds for other declarative languages such as Datalog: You can interpret Datalog programs with various execution strategies, with different performance characteristics and declarative properties."
What are some interesting kinds of IRs?,w9u8qn,2022-07-28 08:01:47,"I'm familiar with TAC, SSA, bytecode, continuation passing, AST, and source-to-source translators. What are some more exotic and unusual kinds of IRs that you've seen before?","I think there are a lot of questions you might be asking here with those same words, so please excuse me if I'm not giving the kind of answer you're interested in.  Here are two ideas (at vastly different levels of abstraction!) that I don't see directly represented in your list, though there's certainly overlap with things in your list:

The very general notion of a ""core language"" distinct from the surface language is interesting.  The goal here is to define a language that isn't too far removed from the surface language (i.e., the language you're compiling) semantically, but that dramatically reduces the number of language constructs.  Haskell's compiler is an excellent example: although the language has evolved dramatically over the past 20 years, and there are probably over a hundred different language extensions implemented in the compiler, the core language has changed only very rarely.  The process of translating from the surface language into the core language is ""desugaring"", and it's one of the early steps of the compiler, coming before any kind of optimization passes at all.  This makes it cheap to offer convenient syntax to programmers without having to pay the cost in every high-level optimization pass.  Much of the time, the semantics of the surface language are even *specified* by giving a desugaring that eliminates the surface syntax, though honestly I think Haskell does a bit too much of that and ends up making some poor semantics decisions just because they accidentally arise from accidental characteristics of the desugaring that was originally defined.

I've also worked recently at my job with MLIR (""multi-level intermediate representation""), which is a single unifying structure for an intermediate representation that's supposed to scale pretty well all the way from very high-level representations close to the AST all the way down to low-level representations adjacent where you actually emit code.  The bulk of the compiler, then, is involved in ""lowering"" transformations that replace higher-level nodes in the graph with lower-level nodes until you've lowered everything enough that you're ready to produce a result.  Where in a traditional compiler you might introduce several layers (say, source file -> AST -> high level IR -> low-level IR -> bytecode -> binary), here you ideally shift everything between source file and binary into the same data structure, just with different choices of nodes.  MLIR also defines a few dialects of reusable nodes, which make it easy to interoperate with different front ends or back ends simply by going via a standardized dialect of nodes in the MLIR graph."
Bootstrapping a compiler from nothing.,u5g1ou,2022-04-17 13:14:04,,"Thanks, I wanted something like this. This is going into my bookmarks. :)"
The final problem,u1uqxw,2022-04-12 17:12:55,,"Very much like games storing compiled shaders, if I understand right? If so, neat! I’m currently fiddling with Arraymancer in Nim, I’ve been meaning to check out Futhark"
The Case for Pattern Matching: A tutorial on two pattern match compilation algorithms,u0cwxp,2022-04-10 15:52:41,,"The `case` for pattern matching, I see what you did there Mr Hu"
"Implementing Languages for Fun and Profit - Nicholas Matsakis, PLMW 2022",sjgfu9,2022-02-03 19:01:03,,"this is a really cool talk and Nico seems to be a really awesome human being.

I especially love how he models the brain of a developer as a program with hot loop paths as our common habits which bear most value for optimizations (adjustments)."
"Type Theory Forall Podcast #13 - C/C++, Emacs, Haskell, and Coq. The Journey (John Wiegley)",rn6omd,2021-12-24 06:08:55,,Love listening to this podcast while my workday is closing out! Thank you!!
Can you make a pure functional language typed well enough to never throw runtime errors?,rhh7lg,2021-12-16 11:23:04,Is this doable? Is this practical? Does any language already do this?,"Yes, there are plenty of languages that do not have any notion of ""runtime errors"". As an example, [Brainfuck](https://esolangs.org/wiki/Brainfuck) is Turing-complete. It only has 8 operators, and none of them are capable of throwing a runtime error. There are also plenty of other functional esoteric languages like [""()""](https://esolangs.org/wiki/(\)) that are based on Lambda calculus and do not have any runtime errors. If you expand your notion of what a programming language is, there are also domain-specific languages like `sed` that don't really have any notion of runtime errors, only syntax errors.

In general, it's not hard to find languages without runtime errors, you just have to look for very simple languages. As soon as you start adding language features like division or I/O operations, you'll be faced with the question of what to do when dividing by zero, or when an I/O operation fails. You can handle these cases without runtime errors (e.g. by requiring the programmer to check for a nonzero denominator before dividing), but oftentimes it's much more convenient for the programmer if you just have runtime errors in your language.

*^(If you're being pedantic, you might argue that all Turing-complete programming languages running on finite hardware can run out of memory. However, I don't think that that should disqualify languages from your criteria, since a ""language"" can exist without any hardware to run it.)"
My theory about cognitive effort of for loop vs map/filter,nwhy7o,2021-06-10 15:46:11,"Related to my [List comprehension / map+filter / for loop?](https://www.reddit.com/r/ProgrammingLanguages/comments/nvbmgs/list_comprehension_mapfilter_for_loop/) post. The responses ""confirmed"" my preference towards map/filter.

I would like to share my theory about why I think for loop is less readable. Let me know what you think.

I think for loop requires more effort to be recognized as either `map` or `filter`... (or `each`? or both? is this `result` even related to the for below - you don't know when you start reading?) The pattern is just big:

    result = []
    for ... {
        ...
        result.append(...)
        ...
    }

Depending on the amount of code inside the `for`, it can become harder to recognize. Even more effort if `append` is inside an `if`. It looks like using `map` or `filter` exposes the intent much quicker.

Edit 2021-06-11: Thanks! Once again, most of the answers are thoughtful!","In my opinion, all mentioned methods are equally illegible, unless you use named functions.

A `map` with a giant anonymous function as an argument is just as unreadable as a `for` with a huge body. A `for` that applies a single function is just as wonderful as a `map` that does the same.

The big difference is that `map` forces you to use a function, so it's often misunderstood to be inherently more clear, but that's not true. I review a lot of student code and see unspeakable horrors in `map`s that would change anyone's mind instantly.

The same goes for list comprehensions: they're good if you use named functions, terrible otherwise.

I would conclude that any real improvement to legibility comes from referential transparency and abstraction. Abstract away big blocks of code into functions and give them names to refer to them in terms of _what_, not _how_ (declarative vs. imperative), and leave side effects to the very outermost parts of your program so that loops will not trip up the reader."
GADTs for dummies,nd2qju,2021-05-16 00:30:45,,"I feel like less people would be clicking links if the acronym was spelled out.

Generalised Algebraic Datatype"
Modelling mutable states in pure functional language,mib8fr,2021-04-02 10:45:45,,"This is from '95  and is the basis of the ST monad currently implementated in haskell.

Meta: can we start adding years to old links?"
Re-Imagining the “Programming Paradigms” Course,l7t2u9,2021-01-29 21:55:06,,"Undergraduate here, literally have only been exposed to oop, and this ""Programming Paradigms"" class as it's described isn't even offered. Did anyone else take a class like that?

Thinking that I might need to look for some online class that covers this..."
Type System for Lifting Terms into Type,kc2t2f,2020-12-13 10:55:18,,"Hmm. Currently the language is total and does not allow recursive functions. Is it a design choice to actively not include it or is it just the not included in the figures. 

With recursion, I think something could go wrong. For example, consider `lambda x : int[static (e 1)].x` where `e : int -> int` is `e x = (e x) + 1`, a diverging function. And it defeats the purpose of being “statically” known? Sure it type checks, but what are you going to do when you try to type check it? Consider:

Let’s say `f = lambda x: int[static 10].x`. How do you type check `f e` where `e : int [static (g 1)]` where `(g 1)` could be arbitrarily complex. I think type checking is basically undecidable at this point. 

I think you can’t just simply lift any term into types for this to work. You could try to forgo recursion in lifted terms, Otherwise you either need to give up on decidable type checking, or recursion even in the “dynamic language’. I think C++ chooses the former (well technically they got around it by saying “we only typecheck things within certain term size limit)."
"My concatenative language made for use in chats via bots, calc=",hn233i,2020-07-08 04:11:03,"So essentially calc= ([GitHub](https://github.com/Camto/calc/)) was (it's now being rewritten in bad c89) my attempt to make a quick to write, not unreadable, one line based language. The easiest way to check it out is at [the web console](https://camto.github.io/calc/console/). The web console has up arrow and down arrow keys repurposed for scrolling through command history and the link changes after every command so you can share a link to it.

To learn it, use [`calc= adv_tut`](https://camto.github.io/calc/console/?calc=adv_tut), any feedback will be appreciated. There's also [`calc= tut`](https://camto.github.io/calc/console/?calc=tut) for people who don't know programming, but I'm not sure if it's any good, so feedback on that would be good too. For documentation, use [`calc= page`](https://camto.github.io/calc/console/?calc=page), which should have plenty.

It has an online Discord bot ([link to add](https://discord.com/api/oauth2/authorize?client_id=429763238071238668&permissions=67584&scope=bot), it only has read and send messages permissions). There's also an IRC bot for it, but it's offline.","Language looks cool! I'm always interested in concatenative stack languages, line-oriented calculators, and languages intended for chat use, so this is right up my alley!

One minor non-language-related criticism, though: The use of ligatures in the web console hampers reading and learning. The `->` ligature is fairly straightforward, but _e.g._ I had to figure out myself through trial and error that the inequal operator was `!=` as opposed to `/=`, `=/=`, `≠`, or `<>`. Admittedly it was the first one I tried, but that's only because I mainly live in C-syntax land, and someone who is not already familiar with programming would have an extremely hard time discovering that on their own."
Oil 0.8.pre6 - Pure Bash and C++,hbj3um,2020-06-19 01:29:21,,"Oil can now run a Lisp in bash :)

https://github.com/kanaka/mal/pull/518

For the PL crowd here, kanaka/mal is a pretty cool project with Lisps in 50+ languages.  Going through its process could be a good test of your own programming language.  It has nice tests and even perf tests, which may be interesting for comparing your language vs. others."
Speeding up the Sixty compiler,g8f2j5,2020-04-26 22:18:35,,"Superb write up of your work, thanks for sharing."
Naming Functional and Destructive Operations,fzu00x,2020-04-12 17:51:42,,"> As a side-note: Scheme has used ! to indicate destructive operations for a long-time.

As another side note: German and related natural languages use ! to indicate an imperative sentence."
MLIR: A Compiler Infrastructure for the End of Moore's Law,f9sz8i,2020-02-26 21:10:18,,This seems to be mainly about adding higher level layers on top of LLVM to support things like SIMD parallelism.    I suppose this isn't going to solve the problems people often have supporting things like laziness or custom garbage collection schemes on top of LLVM?
Favorite syntax for lambdas/blocks?,f50t85,2020-02-17 09:26:39,"A lot of different programming languages these days support lambdas and blocks, but they're  remarkably diverse in syntax. Off the top of my head there's:

ML
`fn x => e`

Haskell
`\x -> e`

Scala
`{ x => e}`
`{ case None => e}`

Java
`x -> e`

Ruby
`{ |x| e }` `{ e }`
`do |x| e end`

Rust
`|x| e`

I've always been incredibly fond of the Scala syntax because of how wonderfully it scales into pattern matching. I find having the arguments inside of the block feels a bit more nicely contained as well.


    list.map {
      case Some(x) => x
      case None => 0
    }

Anyone else have some cool syntax/features that I missed here? I'm sure there's a ton more that I haven't covered.",JavaScript `(x) => x + x`
Whats your favorite implementation of Exception handling?,drr3ri,2019-11-05 08:30:29,"This is a design and functionality choice I have easy and not-so-easy justifications to make about placing in my language.

What is the most appealing exception handling implementation you've used or seen? (Or if you need me to phrase: Whats the least worst?)

&#x200B;

EDIT: Forgot to justify my first sentence,

I come from the viewpoint that exceptions are nifty, but... I dont like memory safety implications","By far, hands down, nothing even comes close: Condition systems, also known as Signals, Conditions, and Restarts; ala common lisp:

* [Chapter of a Commnon Lisp book that explains this (not much lisp knowledge is needed).](http://www.gigamonkeys.com/book/beyond-exception-handling-conditions-and-restarts.html)
* [A great straightforward blog post about it, and how it can work syntax wise in classic curly brace languages.](http://axisofeval.blogspot.com/2011/04/whats-condition-system-and-why-do-you.html)

The exception system in most languages is actually a limited subset of a condition system. The key insight of a condition system is that it does not throw away the stack immediately, which allows it to recover by using the wisdom of the callers to choose an intelligent local solution. Even if that caller is a dozen libraries up the stack - every library can add it's own locally relevant solutions to the possible list of options - and if no one has a strategy to solve the problem it can dump out to the developer with a choice about how to resolve the issue.

But using functional programming style Either (AKA Rust's Result) as convention, and only having fatal errors isn't a terrible choice. But it's a far cry from a condition system in my opinion."
"Lead Kotlin developer, Andrey Breslav, tells the story of how he began working with JetBrains during his PhD to create the new programming language from scratch",cju0m2,2019-07-31 00:16:41,,I don't think it's at all related to scratch
Ownership and Borrowing in D,cdkd1n,2019-07-16 01:21:36,,"i'm curious what drove you to apply ownership/borrowing rules to pointers, rather than using distinct types for borrow-checked references and raw pointers the way Rust does. it seems like an obvious distinction which would simplify/alleviate many of the challenges described in your blog post, wrt. converting legacy code and also `unsafe`/implementation-details-ey programming."
"Python closures are capturing by reference, should this be normal in a language?",bxxx7x,2019-06-08 02:18:04,,"Languages with closures always capture by reference. The ""by reference"" isn't the problem. The problem is with the extent of the variable being captured. Python doesn't have block-scoped variables, so when you have a loop like `for d in data:`, there is only *one* `d` variable and it gets re-assigned a new value each iteration of the loop.

Any closures that capture that value will end up seeing the last value assigned to it because there is only that one variable.

C# used to work this way, but was fixed in 5.0. Dart has always done this correctly: each iteration of a loop creates a *new* loop variable."
Dreaming of a Parser Generator for Language Design,axtnra,2019-03-06 10:37:17,,"This post is excellent. It should be required, sobering reading for anyone contemplating the use of parser generators.

In my experience, lexing and parsing is the easiest, quickest part of a compiler to write using a simple recursive-descent parser. Furthermore, the parsing code is dominated not by token matching logic, but by the AST-building and error handling logic, which I believe is not so easily formalized. My lexer is similarly hard to formalize, as its produced token semantics are fully normalized and IR-ready: number literals are converted, string escape sequences processed, and all names not just hashed, but completely interned in the global name table.

Given my preference for a language syntax that requires no backtracking, I struggle to know when to recommend use of a parser generator. Based on what I know, it seems it would only slow my productivity, worsen compiler performance, and degrade the programmer's experience."
What's your ideal language feature?,9um9nw,2018-11-06 15:11:39,"Aside from [Graydon Hoare's blogpost](https://graydon2.dreamwidth.org/253769.html), what's your ideal language feature? It might be a better module system that exist in 1ML , or something that hasn't existed at all. If it's already exist on a language it'd be nice to specify it.","Honestly, pattern-matching and destructuring are *so* cool to me. These are common features in functional languages (Haskell, OCaml, Scala, etc) and I really really wish they existed in imperative languages (like Python, which I use most often).

For less-supported features, I think linear types are really interesting. For every allocation of a resource of a specific type, you must also un-allocate (or use or destroy) it. Really neat idea, though I don't know how useful it is in practice beyond things like files, sockets, etc."
How good is Go for writing a compiler?,15gz8rb,2023-08-03 17:28:26,"I'm considering creating a compiler for a high-level programming language and wonder if Go would be a good fit for writing it.   


I have looked around similar posts on this subreddit and saw various languages mentioned (e.g., OCaml, Haskell, Rust, C/C++, etc.). Go wasn't that frequently mentioned, although, from what I know, if the language is compiled rather than interpreted, the choice of a programming language shouldn't matter as long as the compiler's target is native code. I know Go is self-hosted and, therefore, written in itself.  


My language would have a GC: Would Go already having a GC complicate development? If so, would Rust be a good option? In addition, I know Rust, for example, has tools for building languages, such as `inkwell` (LLVM bindings) and `cranelift`. To my knowledge, Go has no such tools.","I dont recommend it. Ive built interpreters in Go before, i didnt realize how unergonomic it was until I built the same interpreter in OCaml which has algebraic data types and pattern matching. I would look into a language like that I.e rust, oCaml, haskell, etc etc"
Your PL is topologically boring: computing the cohomology of some topoi used in PL,14kzoiz,2023-06-28 12:34:52,,"While this post has very strong ""I smoked a bit too much"" vibes, based on me not understanding a single word I'll just assume I'm too stupid to understand the post and we'll leave it up :)"
Why are sizes signed?,11wiqhz,2023-03-20 21:21:40,,"In the lead to C++17, Herb Sutter argued in favor of using signed types for indexing (and length). In fact, the first implementations of `gsl::span` used signed indexes and returns a signed length.

In fact, he was pleased to report that for a number of programs the port to `gsl::span` has unveiled index arithmetic bugs: much more easily identifiable with negative indexes.

While this worked great for `span` in isolation, the incompatibility with the rest of the ecosystem -- unsigned everywhere -- led to `std::span` (and `gsl::span`, retroactively) using unsigned types.

---

I must admit finding the usecases for unsigned types fairly limited in general.

The one advantage of unsigned types is ""doubling"" the range of possible values for non-negative numbers. This is quite useful for `u8`, and fairly useful still for `u16`, but not so useful for `u32` and quite questionable for `u64` or `u128`.

Scalar arithmetic on hardware also tend to be defined in wider registers anyway, so that a `u8` is inflated to an `int`, arithmetic is performed, and then the result is truncated back to `u8`... This seems to relegate `u8` and `u16` to _storage_ types, though vector arithmetic can actually work with vectors of such types again.

The index/length case, however, is not _that_ general. Unless one is working on a language for exotic targets, index and length are going to be at least 32-bits, and allocations of > 50% of the address space are unlikely to ever succeed, meaning that the ""range"" advantage of unsigned types is null and void.

There is an argument for making invalid values non-representable, but... I find it specious. Due to the allocation issue noted above, over 50% of the values of indexes are invalid anyway, hence bounds-checks will be necessary from a ""functional"" point of view.

This leaves, as best as I can see, an argument for performance. Whether an index is valid can be checked with a single comparison for unsigned indexes, whereas two are required for signed indexes. I am none too convinced by the argument, though:

 - If the compiler can elide the bounds-check, then the point is moot.
 - If the compiler cannot elide the bounds-check, then the presence of _any_ bounds-check, which will prevent further optimizations, is likely the main performance issue -- and what actually it consists of matters little.

I would be happy to entertain any further argument for unsigned types for indexes and lengths. For now, I remain convinced that signed types are better suited to the job.

_Note: Rust does not support indexes over `isize::MAX`, or pointer arithmetic with offsets over `isize::MAX`, despite using `usize`. I am not quite sure why, but it does reduce the usefulness of using `usize`..._"
Formal systems outside of lambda calculus and its variants for use as a foundation for programming languages.,114yp4t,2023-02-18 05:52:21,"Ive been a lurker in this subreddit for a while now and am curious if anyone knows of any? the only other Im aware of is phi-calculus, what eolang is based on, which was posted about in this sub a while ago.","Well there's the pi-calculus, which phi-calculus is based on. In general, wherever you have a model of computation, you have a foundation for programming languages. For example, there exists a programming language (programs in which are unordered sets of 8-tuples) that directly describes a turing machine. There are register-based languages, stack languages, and others. All of these have formal systems at their cores (for example, for a look at the formalism of register languages, read about abacus computability). ""Real"" programming languages based on these systems arise by trying to use some formal system for programming, hating it, noticing common patterns in your code, and then introducing special syntax for them, over and over until you no longer feel the need."
A Wishlist of Zero-Cost Abstractions,10yexgm,2023-02-10 11:22:59,,"Types are proofs of the behavior of the code that the compiler can check. 

Ideally, there should be as many proofs as possible and all the proofs should be erased by the compilation. 

Also, there should multiple levels of proofs that would allow you write as little proofs as possible. From code written without type annotation where everything is inferred by the compiler all the way down to linear types for resource management. 

I should be able to start writing code without proofs, add a few proofs to make the code clearer and document some of the constrains and then drop even lower if I need to optimize some aspect of the behavior of the code (e.g. add some more proofs that would allow the compiler to reshape the output in a more performant way)."
"Toy v0.6.0 - Language Is Stable, Game Engine In Development",y5e0wm,2022-10-16 18:59:22,"[https://toylang.com/](https://toylang.com/)

&#x200B;

* I've re-arranged the branches in the repo, so that it's less cluttered.
* Stopped using a static spec file
* Stated using proper a tagging system
* Proof-of-concept for the game engine is [partially working](https://github.com/Ratstail91/airport)

Toy is a simple language, intended for pure ""game logic"". The engine for airport (an idle clicker game) uses a node-based structure - every node has a Toy script attached, and are arranged in a tree-like structure. If you define a function with a specific name (i.e. `onInit()`, etc.) those functions will be called at the appropriate times.

While the lang is stable (with some bugs remaining), the engine is still in intense development.

The link at the top will take you to the lang's reference website - I plan to continue adding to it and adapting it as I go.

&#x200B;

As a whole, it's not \*efficient\*, but it works. I'm happy to accept any help people are willing to offer.",Good design
What is the philosophical reasons to not differentiate between `::` and `.` in namespace resolution?,w4iqdz,2022-07-21 22:55:44,"As the question above asks. 

For example in Rust and C++, `::` is used for namespace/module resolution, while `.` for member access. Quite many other languages, including Swift, C#, Java, Typescript, python, etc simply use `.` for both?


Is there some sort of philosophical reason? Because personally I find `::` better for understanding the code, as otherwise one can mix up whether something is a variable or not, especially if one does not have syntax highlighting.

Is it maybe because one can regard variables, functions and types of a module/namespace as their ""members""? Or are there other reasons which may go deeper?

What are the pro and cons?

PS: Of course, I'm talking about the concept in general. So also please include languages which use other ways to express those things.","There are two options for this:

a) The namespace-seperation operator and the member access operator simply share the same symbol. The rationality is that after semantic analysis, both uses can be easily seperated, so using a different symbol is seen as unnecessary, as it requires users to know which one to choose in each context.

b) Modules and classes are treated as objects with multiple members. This view exists e.g. in Python or Zig. Therefore accessing an item inside the module is the same as accessing any other object member and should use the same operator."
A functional shading langauge,t6s14z,2022-03-05 04:24:02,"This is an idea I’ve had for a while - A higher level language that is built specifically with the goal of compiling to GLSL/HLSL. I think there is a lot to gain from being functional and pure for a language like that

One of the biggest restrictions for that is probably the memory model - could you have a functional language compile to one that doesn’t support recursion?

Has something like this been done before? Do you have any insight or thoughts about this?",You could also consider [SPIR-V](https://www.khronos.org/api/spir) as a compilation target
Function parameter as a tuple,rcctxy,2021-12-09 15:51:07,"A function with multiple parameters is sometimes cumbersome when you need to chain/pipe it in a functional style. The obvious choice to solve this today would be function currying, but I have another interesting idea to consider.

The idea is that all functions can only take one single parameter behind the scene; multiple parameters functions are just a syntactic sugar of a function that accepts a tuple as the argument.

This reflects very nicely in languages with \`foo(1, 2)\` as its function call syntax since it already looked like a function name followed by a tuple. And it addressed chaining/piping as well since now function can return a tuple to be passed onto the following function easily.

What are your thoughts on this?","There are some languages that do this, but very few in the mainstream space. Haskell has currying and tuples, and no multiple argument functions, for instance.

The last time this topic came up here, some people mentioned that certain features can get troublesome or ugly. The ones I can remember are:

- optional arguments
- nice type errors (e.g. on call to f, expected argument 3 to have type int, but expression `10.0` has type float)
- overloading

These are all solvable with some special treatment of function calls, but at that point you're just not passing tuples anymore"
"Why Javascript devs hate ""var"" function scoped variable declarations, when python devs are using function scoped variables without any problem?",r6jn30,2021-12-02 01:10:02,"I am learning javascript now, and have a reasonable understanding of python. 

I found it difficult to understand the reason why Javascript community hated function scoped declarations, while it is accepted in python community? 

So, please share your thoughts!","As a Python developer, I hate it too. I'd rate it as one of the biggest warts in the language ([here's a bunch of other warts](https://github.com/satwikkansal/wtfpython)).

It bites me the most often when creating lambdas inside a loop:

    fns = []

    for i in range(5):
        fns.append(lambda: i)

    for fn in fns:
        print(fn())

    # Prints ""5"" 5 times.

The ""fix"" is to use `lambda i=i: i`, so that it binds the value to a new locally scoped variable with the same name.

The current scoping rules do have their advantages:

    for i in range(100): pass

    # Print the last loop value:
    print(i)

But I'd still rather have block-scoped names."
Where's more discussion of the designs of effect systems?,qvw6ky,2021-11-17 18:13:26,"Alexis King has lost her motivation to implement GHC's ""Delimited continuation primops"" proposal, which has been approved for a while, to boost performance of effect systems atop Haskell:

https://github.com/ghc-proposals/ghc-proposals/pull/313#issuecomment-970719771

> I’ve been wanting to find the energy to properly articulate my thoughts on that last point in writing for some time now, but it’s an extremely thorny and subtle set of issues, so I have unfortunately struggled to do so. But the relevant summary is that I think the bar for what constitutes an acceptable effect system is unusually high, since it’s such a foundational technology that requires pretty significant buy-in to use in a project, and I am completely unsatisfied with the way all existing approaches I am aware of handle scoping operations, such as `catch` and `listen`. I used to believe that the approach I was taking in `eff` would make it possible to eventually develop a more palatable way of handling those types of operations, but I realized about a year ago that there is a somewhat fundamental contention that makes my idea unworkable, which leaves `eff` in a spot that I personally find unsatisfactory.

She has talked about the issues at some length [on twitch recently](https://www.twitch.tv/videos/1163853841), but I can't quite follow (partly due to my English hearing skills).

While her [eff](https://github.com/hasura/eff) is still under construction, there are relative more mature pieces like `Koka` etc. I'd very much like to understand how her concerns map to those other effect systems, but can't find more write-ups about the design space of effect systems.

Pointers and/or your thoughts?","Languages like `Koka` only support algebraic effects, not scoping operations such as `catch` and `listen`. The [Effect Handlers in Scope](https://www.cs.ox.ac.uk/people/nicolas.wu/papers/Scope.pdf) paper introduces scoping operations, which lead to the Haskell libraries [`fused-effects`](https://github.com/fused-effects/fused-effects) and [`polysemy`](https://github.com/polysemy-research/polysemy), but they turned out to have some [weird semantics](https://github.com/polysemy-research/polysemy/issues/246). [`eff`](https://github.com/hasura/eff) is her effort to have scoping operations with the right semantics."
Friendship ended with the garbage collector,pbbbbg,2021-08-25 21:33:51,,"It's been a while since the last update about [Inko](https://inko-lang.org/), so I wrote a bit about what has been going on. It's not the most in-depth, mostly because that would result in a very long blog post :)"
What are some design patterns or things to think about when designing a (hobby or not) programming language?,myf21r,2021-04-26 03:04:24,"So I like to think and design a lot of programming languages, most of them don't see daylight but I still like  toy with ideas..  
So in one of my toying I put together this little list of common considerations for when designing a language.  
I'd like to share it and would love more concepts to consider! 

^(please note this list wasn't made with the thought I share it so do expect some lack of logical structure)

&#x200B;

**Considerations:**

* Comments
* Variables
   * declaration
* Common types
   * Number (integer or floating point)
   * Char/byte
   * String
   * Boolean
   * Array
   * Enum
   * null
   * any more complex data structure
* Types / Structuring data
   * Generic types (if statically typed)
   * Static vs Dynamic Typing
* Type casting & Polymorphism
* Procedures / functions
   * Arguments 
      * Type , Procedure / Closure as argument
   * Return
   * Methods, Functions 
   * Interfaces
* Operations (& Precedence)
* Exception handling
* Memory management 
   * Garbage collector
   * manual
* Control flow
   * If
   * While / loop / for
      * Break
      * Continue
   * Switch 
* Concurrency
   * Threads
   * processes
   * Async
* Linking external libraries 
   * VM and bytecode
   * native libraries
* Language Reflection
* beyond the language
   * Package management
   * modules and imports","""What problem am I solving?"" That's the first question to consider. The answer could be ""No real problem, I just want to learn X"" or ""I'm hoping it will make it easier for somebody to do Y.""

The answer to that should guide what is important. Everything else you have listed here should follow from that, they are tools that help you accomplish your goals. That way, you avoid the trap where you have a hammer, so everything looks like a nail."
My nameless compiler for the CiC,ku0m4f,2021-01-10 06:08:18,"This something I've been working on for a while now. It's an incomplete hot mess however I thought it would be neat to get some eyeballs on it. The premise is a compiler for the calculus of inductive constructions, built on a pure type system with additional computational sorts. It's inspired by the older theory backing coq and similar systems, however this language is about writing programs annotated with types. I included some of the prior work that I heavily relied on at the end of the readme. 

Part of the fun has been writing the compiler entirely from scratch in SML, starting with only Basis. I was going to use pure Standard ML, but after I switched from PolyML to MLTon (PolyML couldn't type check my type checker) I gave in a bit. The architecture is a monad transformer stack, with a base compiler monad and phases stacked on top of it. The LL(1) parser is handwritten as well as its library. The type checker uses an interpreter monad, as it has to reduce expressions for convertibility. 

Amusingly, if you look way back in the commit log, you can see an older version of the language where I tried implementing a type and effect system and just gave up. And even further back there is another pure type system which was essentially just a poor imitation of Henk. 

Anyway, [here is the code](https://github.com/ueliem/crispy-winner), I'd love to hear your thoughts!","Awesome work. Looks quite terse and pretty well-organized

> I was going to use pure Standard ML, but after I switched from PolyML to MLTon (PolyML couldn't type check my type checker) I gave in a bit. The architecture is a monad transformer stack, with a base compiler monad and phases stacked on top of it. The LL(1) parser is handwritten as well as its library. The type checker uses an interpreter monad, as it has to reduce expressions for convertibility. 

You know using monads doesn't make it not SML :) You're allowed to use new design patterns"
Does it make sense to take an existing language and make radical changes to it's syntax?,kevg4a,2020-12-17 19:01:30,"I see new programming languages pop up here every day and it's fun thinking about maybe writing one myself but it seems like it would be more productive to take an existing language and reshape it to my liking.

Things like garbage collection, concurrency, support for multiple platforms, large standard libraries etc take a lot of effort and advanced knowledge so why not start with a language that has all that and then start chipping away at it.

Just a thought.","This actually happens a lot, and it's a viable way of kickstarting a language.

Elixir runs on top of Erlang infrastructure.

Clojure, Scala, and a whole zoo of other languages, run on top of Java infrastructure.

F#, and a slightly smaller zoo of other languages, run on top of .NET, itself designed from the ground up to support both VB.NET and C# (and, to a lesser degree, ""managed C++"", which is kind of ugly IMO, but still counts).

There are downsides to it, such as design decisions of the original language leaking into yours, but the upsides you mention are real, and they're whopping."
A Case for Properties: Programming Language Design,juol2e,2020-11-16 00:30:29,,"I'm not losing sleep over it, but I don't like properties because they're an example of an anti-pattern of half-way encapsulation/data-hiding. In my perfect world you would have two cases:

1) You know everything about the pre/post conditions to maintain and take full responsibility for it.

2) You know nothing about any of that, and interact with the object through some interface/trait/aspect/concept/etc. (depending on the exact terminology and affordances of the language you're using.)

Properties live in some kind of muddy middle ground between these where you can treat some fields one way and other fields another way. It points to a possible God Object you need to break up."
Introducing Ungrammar,jhop80,2020-10-25 14:17:48,,"Can anyone explain to me how this compares to parser combinators (e.g. parsec in Haskell)? To me it seems like parser combinators can already achieve what ungrammar does, for example renaming nodes"
Was async fn a mistake?,170j01c,2023-10-05 22:10:35,,"In part for the philosophy of Rust I think it was not a mistake, but the execution was, let’s say, a rather poor effort.

Having asynchronous code directly supported by the language was supposed to DO get the language traction. Foundation probably wanted to get their heels off the rails first and then deep dive better into details, planning to polish the rough edges they have with a new edition the next year.

However a couple things they didn’t foresee beforehand now keeps them at bay, with loads of question marks in their heads to how to resolve the current status quo. A couple things they can’t yet fix:

- tokio has a monopoly in its hand. Many people started using it at the beginning because it was good. Now many more people, including the organizations that support the foundation AND many people with packages dependent on it on crates.io, use it on a regular basis. I bet that they will eventually merge tokio as the “standard” and plugging in a custom runtime will be optional moving onwards. This will significantly decrease platform divergence but will come at the cost of having another dependency for the foundation to maintain.

- They also didn’t consider it carefully. Are the tasks going to run on a single thread, or the executor is going to be multithreaded? How will the memory be shared then? How do we expose this in an interface to runtime implementations? It all seems incomplete to me at least. A quite compelling evidence I’d point out to is the async traits not having launched out to stable on day one, and only being stabilized as we speak. A less convincing argument would be the incompleteness of the Future API. How do I combine multiple features and poll them each to completion in parallel? This is often the case where one has to use tokio::spawn, further increasing the tokio monopoly.

- I think tho to give credit where its due, the current stuff we have for async is about 80% right. So not everything is terrible. I like that the compiler can infer the most efficient space per async task and I believe this is a huge accomplishment of the language team. And I also like how the executor was modeled as a queue that advances little state machines. One possibility would be us dumb humans being incapable of using rust properly of course, but I can’t prove that.

In summary, they didn’t get it right first time, now they have to try again. Only now, shit is messier.

So, until they figure out the rough edges before the next edition and manage to find a way so that they can MAKE the community accept those breaking changes somehow without throwing a tantrum, I do foresee a rather grand software drama ahead of us gentleman.

But I may as well be dumb. I do not know."
